---
layout: post
title: "LLaMA: Open and Efficient Foundation Language Models"
date: 2023-02-27 17:11:15
author: "Meta AI"
categories: "Foundation-Models"
tags: ["Open-and-Efficient-Foundation-Language-Models", "Rotary-Positional-Embeddings", "SwiGLU-Activation-Function", "Scaling-Laws-for-Large-Language-Models", "Responsible-AI-Evaluation"]
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
대규모 언어 모델(LLMs)은 텍스트 기반 지시사항이나 소수의 예시만으로도 새로운 과제를 수행할 수 있는 뛰어난 능력을 보여왔습니다. 그러나 기존의 최고 성능 모델들은 대부분 비공개 데이터셋을 사용하여 학습되었고, 모델의 규모가 매우 커서 실용적 활용이 제한적이었습니다. Meta AI 연구진은 이러한 한계를 극복하고자, 공개적으로 접근 가능한 데이터만을 사용하면서도 최고 수준의 성능을 달성할 수 있는 효율적인 언어 모델을 개발하고자 했습니다. 특히 Hoffmann 연구진의 발견에 따르면, 주어진 컴퓨팅 예산 내에서 최고의 성능은 가장 큰 모델이 아닌, 더 많은 데이터로 훈련된 상대적으로 작은 모델에서 달성된다는 점에 주목했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
LLaMA는 Chinchilla 스케일링 법칙을 기반으로, 모델 크기와 학습 데이터 크기 간의 최적 균형을 찾아 효율적인 학습을 달성하는 방법을 제시했습니다. 특히 공개적으로 이용 가능한 고품질 데이터셋만을 선별적으로 사용하여 학습을 진행했으며, 로터리 포지셔널 임베딩(RoPE), RMSNorm, SwiGLU 활성화 함수 등 최신 아키텍처 개선사항들을 통합했습니다. 또한 모델 병렬화와 데이터 병렬화를 결합한 하이브리드 방식을 도입하여 대규모 학습의 효율성을 높였습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
LLaMA는 7B부터 65B 파라미터에 이르는 다양한 규모의 모델을 개발했으며, 각 모델은 약 1.4조 개의 토큰으로 구성된 공개 데이터셋으로 학습되었습니다. 학습 데이터는 영어 CommonCrawl(67%), C4(15%), GitHub(4.5%), Wikipedia(4.5%) 등 다양한 소스에서 수집되었으며, 엄격한 품질 필터링 과정을 거쳤습니다. 학습 과정에서는 AdamW 옵티마이저와 코사인 스케줄링을 사용했으며, 2,048개의 A100-80GB GPU를 활용하여 약 5개월간 학습을 진행했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
LLaMA의 가장 큰 성과는 13B 파라미터 모델이 175B 파라미터의 GPT-3보다 우수한 성능을 달성했다는 점입니다. 또한 65B 모델은 Chinchilla-70B나 PaLM-540B와 같은 최신 대규모 모델들과 대등한 성능을 보여주었습니다. 이는 공개 데이터만으로도 최고 수준의 언어 모델을 개발할 수 있다는 것을 입증하며, 모델의 효율성 향상이 실용적 활용 가능성을 크게 높일 수 있음을 보여줍니다. 다만 모델이 여전히 편향성과 유해성의 위험을 내포하고 있어, 이를 완화하기 위한 추가 연구의 필요성도 제기되었습니다. 연구진은 이 모델을 연구 커뮤니티에 공개함으로써 대규모 언어 모델의 발전과 문제점 해결을 위한 공동의 노력을 촉진할 수 있을 것으로 기대하고 있습니다.
- - -
## LLaMA: 개방형 고효율 기초 언어 모델

Meta AI의 연구진은 7B부터 65B 파라미터에 이르는 기초 언어 모델 시리즈인 LLaMA를 소개했습니다. 이 연구의 가장 주목할 만한 특징은 공개적으로 접근 가능한 데이터셋만을 사용하여 최첨단 성능의 언어 모델을 훈련시킬 수 있다는 것을 입증했다는 점입니다.

대규모 언어 모델(Large Language Models, LLMs)은 텍스트 기반 지시사항이나 소수의 예시만으로도 새로운 과제를 수행할 수 있는 능력을 보여왔습니다. Brown 저자와 연구진이 발견한 이러한 퓨 샷 학습 능력은 모델의 규모가 충분히 커졌을 때 처음 나타났습니다. 이는 더 큰 모델을 만드는 것이 더 나은 성능으로 이어질 것이라는 가정 하에 많은 후속 연구들이 진행되어 왔습니다.

하지만 Hoffmann 저자와 연구진의 최근 연구에 따르면, 주어진 컴퓨팅 예산 내에서 최고의 성능은 가장 큰 모델이 아닌, 더 많은 데이터로 훈련된 상대적으로 작은 모델에서 달성된다는 것이 밝혀졌습니다. 특히 추론(inference) 비용을 고려할 때, 목표 성능 수준에 도달하기 위해 대규모 모델을 훈련시키는 것이 더 빠를 수 있지만, 장기적으로는 더 오래 훈련된 소규모 모델이 추론 단계에서 더 경제적일 수 있습니다.

LLaMA의 연구진은 이러한 통찰을 바탕으로, 다양한 추론 예산 상황에서 최적의 성능을 달성할 수 있는 언어 모델 시리즈를 개발했습니다. 그 결과, 13B 파라미터를 가진 LLaMA 모델이 175B 파라미터의 GPT-3보다 대부분의 벤치마크에서 더 우수한 성능을 보여주었으며, 이는 모델이 10배나 작음에도 불구하고 달성한 놀라운 성과입니다. 더 나아가 65B 파라미터 규모의 LLaMA 모델은 Chinchilla나 PaLM-540B와 같은 최고 수준의 대규모 언어 모델들과 견줄 만한 성능을 보여주었습니다.

특히 주목할 만한 점은 LLaMA가 GPT-3, PaLM, Chinchilla와 달리 공개적으로 이용 가능한 데이터만을 사용했다는 것입니다. 이는 OPT, GPT-NeoX, BLOOM, GLM과 같은 다른 개방형 언어 모델들과 마찬가지로 연구 커뮤니티에 공개될 수 있다는 장점이 있습니다. 하지만 기존의 개방형 모델들과 달리 LLaMA는 PaLM-62B나 Chinchilla와 경쟁할 수 있는 수준의 성능을 달성했다는 점에서 큰 의의가 있습니다.

### 모델 학습 방법론

LLaMA의 학습 방법론은 Brown 저자와 연구진이 GPT-3에서 제시한 방법과 Chowdhery 저자와 연구진이 PaLM에서 사용한 접근법을 기반으로 하고 있습니다. 특히 Hoffmann 저자와 연구진이 제안한 Chinchilla 스케일링 법칙에서 큰 영감을 받아 설계되었습니다.

Chinchilla 스케일링 법칙은 언어 모델의 성능을 최적화하기 위한 모델 크기와 학습 데이터 크기 간의 관계를 수학적으로 정립한 중요한 연구입니다. 이 법칙에 따르면, 주어진 컴퓨팅 예산 내에서 최적의 성능을 달성하기 위해서는 모델 크기와 학습 데이터의 크기를 균형있게 조절해야 합니다. 구체적으로, 모델 크기를 2배로 늘릴 때마다 학습 데이터의 크기도 2배로 늘려야 한다는 것이 핵심 원리입니다.

이러한 스케일링 법칙은 다음과 같은 수학적 모델을 통해 표현됩니다.

$$ \hat{L}(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta} $$

여기서 \\(N\\)은 모델의 파라미터 수, \\(D\\)는 학습 데이터의 토큰 수를 나타내며, \\(E, A, B, \alpha, \beta\\)는 실험적으로 결정되는 파라미터입니다. 이 수식을 통해 주어진 컴퓨팅 예산 \\(C\\)에 대해 최적의 모델 크기 \\(N_{opt}(C)\\)와 학습 데이터 크기 \\(D_{opt}(C)\\)를 도출할 수 있습니다.

LLaMA는 이러한 이론적 기반을 바탕으로 표준적인 트랜스포머 아키텍처와 최적화 기법을 활용하여 학습되었습니다. 구체적으로, 모델은 다음과 같은 핵심 요소들로 구성됩니다.

1. 트랜스포머 기반 아키텍처: GPT 계열의 모델들과 마찬가지로 디코더 전용 트랜스포머를 사용하며, 셀프 어텐션 메커니즘을 통해 입력 시퀀스의 문맥을 포착합니다.

2. 최적화된 학습 과정: Chinchilla 스케일링 법칙에 따라 모델 크기와 학습 데이터 크기를 균형있게 조절하여, 주어진 컴퓨팅 자원을 최대한 효율적으로 활용합니다.

3. 분산 학습 시스템: 대규모 모델의 효율적인 학습을 위해 모델 병렬화(Model Parallelism)와 데이터 병렬화(Data Parallelism)를 결합한 하이브리드 방식을 채택했습니다.
LLaMA의 학습 과정에서는 모델의 구조적 최적화뿐만 아니라 학습 과정의 효율성을 높이기 위한 다양한 기술적 접근이 적용되었습니다. 특히 학습 과정에서 사용된 최적화 기법들은 대규모 언어 모델의 효율적인 학습을 위해 세심하게 설계되었습니다.

학습 과정의 핵심 요소 중 하나는 어텐션 메커니즘의 최적화입니다. 코드를 살펴보면, LLaMA는 로터리 포지셔널 임베딩(Rotary Positional Embedding)을 사용하여 위치 정보를 효과적으로 인코딩합니다.

```python
def apply_rotary_emb(xq, xk, freqs_cis):
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xk)
```

이러한 구현은 Chinchilla 스케일링 법칙의 실제적 적용을 보여줍니다. 특히 어텐션 계산 과정에서 캐시를 활용하여 계산 효율성을 높이고, 병렬 처리를 통해 대규모 데이터셋에 대한 학습을 가능하게 합니다.

모델의 학습 과정에서는 RMSNorm(Root Mean Square Layer Normalization)을 사용하여 학습의 안정성을 확보합니다.

```python
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
```

이러한 정규화 기법은 기존의 LayerNorm과 비교했을 때 계산 효율성이 높으면서도 모델의 성능을 유지할 수 있다는 장점이 있습니다. 특히 대규모 모델 학습 과정에서 발생할 수 있는 그래디언트 소실이나 폭주 문제를 효과적으로 방지합니다.

학습 과정의 또 다른 중요한 특징은 모델 병렬화 구현입니다. ColumnParallelLinear와 RowParallelLinear 레이어를 사용하여 대규모 모델의 효율적인 분산 학습을 가능하게 합니다. 이는 Chinchilla 스케일링 법칙에서 제시하는 최적의 모델 크기와 학습 데이터 크기의 균형을 실제 구현에서 달성하는데 핵심적인 역할을 합니다.

### 사전 학습 데이터

LLaMA 모델의 사전 학습에는 다양한 도메인의 공개 데이터셋이 활용되었습니다. 연구진은 기존의 대규모 언어 모델들이 사용했던 데이터 소스들 중에서 공개적으로 접근 가능하고 오픈 소스 라이선스와 호환되는 데이터만을 선별적으로 사용했습니다. 이러한 데이터 선택 과정을 통해 연구의 재현성을 보장하고 커뮤니티와의 공유를 가능하게 했습니다.

전체 학습 데이터셋은 다음과 같은 구성으로 이루어져 있습니다.

영어 CommonCrawl 데이터가 전체의 67%를 차지하며, 이는 2017년부터 2020년까지의 5개 덤프에서 추출되었습니다. 이 데이터는 CCNet 파이프라인을 통해 전처리되었는데, 이 과정에서 문장 수준의 중복 제거, fastText 선형 분류기를 이용한 언어 식별, n-gram 언어 모델을 통한 품질 필터링이 수행되었습니다. 특히 주목할 만한 점은 Wikipedia에서 참조로 사용된 페이지와 무작위로 샘플링된 페이지를 구분하는 선형 모델을 훈련시켜, 참조로 분류되지 않은 페이지들은 제외했다는 것입니다.

C4 데이터셋이 15%를 차지하며, 이는 다양한 전처리된 CommonCrawl 데이터셋을 사용하는 것이 성능 향상에 도움이 된다는 실험적 관찰에 기반하여 포함되었습니다. C4의 전처리 과정도 중복 제거와 언어 식별 단계를 포함하지만, CCNet과의 주요 차이점은 품질 필터링 방식에 있습니다. C4는 문장부호의 존재나 웹페이지의 단어 및 문장 수와 같은 휴리스틱에 주로 의존합니다.

GitHub 데이터는 4.5%를 차지하며, Google BigQuery를 통해 제공되는 공개 GitHub 데이터셋에서 Apache, BSD, MIT 라이선스로 배포된 프로젝트만을 선별적으로 사용했습니다. 연구진은 줄 길이나 영숫자 문자의 비율과 같은 휴리스틱을 기반으로 저품질 파일을 필터링했으며, 정규 표현식을 사용하여 헤더와 같은 상용구를 제거했습니다. 최종적으로 파일 수준에서 정확한 매칭을 통한 중복 제거를 수행했습니다.

Wikipedia 데이터도 4.5%를 차지하며, 2022년 6월부터 8월 기간의 덤프를 사용했습니다. 특히 라틴 문자나 키릴 문자를 사용하는 20개 언어(bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk)의 데이터를 포함했습니다. 전처리 과정에서는 하이퍼링크, 주석, 기타 서식 상용구를 제거했습니다.
사전 학습 데이터의 나머지 구성을 살펴보면, Gutenberg 프로젝트와 Books3 데이터가 전체의 4.5%를 차지합니다. Gutenberg 프로젝트는 저작권이 만료된 공공 도메인의 도서들을 포함하고 있으며, Books3는 대규모 언어 모델 학습을 위해 공개된 ThePile 데이터셋의 일부입니다. 연구진은 도서 수준에서 중복 제거를 수행하여 90% 이상의 내용이 중복되는 도서들을 제거했습니다.

arXiv 데이터는 2.5%를 차지하며, 과학 논문의 LaTeX 파일들을 처리하여 포함시켰습니다. Lewkowycz 저자와 연구진의 방법론을 따라, 첫 번째 섹션 이전의 내용과 참고문헌을 제거했으며, LaTeX 파일의 주석들도 제거했습니다. 특히 논문들 간의 일관성을 높이기 위해 사용자가 정의한 매크로와 정의들을 인라인으로 확장하는 처리를 수행했습니다.

Stack Exchange 데이터는 2%를 차지하며, 컴퓨터 과학부터 화학까지 다양한 분야를 다루는 고품질 질문-답변 웹사이트의 덤프를 포함합니다. 연구진은 28개의 가장 큰 웹사이트의 데이터를 선택했으며, 텍스트에서 HTML 태그를 제거하고 답변들을 점수(높은 것부터 낮은 것 순)에 따라 정렬했습니다.

토크나이저(Tokenizer)는 SentencePiece 구현체를 사용하여 바이트 페어 인코딩(BPE) 알고리즘을 적용했습니다. 특히 주목할 만한 점은 모든 숫자를 개별 자릿수로 분할하고, 알 수 없는 UTF-8 문자는 바이트 단위로 분해하는 방식을 채택했다는 것입니다. 이러한 토크나이징 방식은 다양한 언어와 특수 문자를 효과적으로 처리할 수 있게 해줍니다.

전체 학습 데이터셋은 토크나이징 후 약 1.4조 개의 토큰을 포함하고 있습니다. 대부분의 학습 데이터는 한 번씩만 사용되었지만, Wikipedia와 Books 도메인의 데이터는 약 2번의 에포크(epoch)를 거쳐 학습되었습니다. 이는 이들 데이터셋의 상대적으로 높은 품질과 중요성을 고려한 선택으로 보입니다.

이러한 데이터 구성은 모델이 다양한 도메인의 지식을 균형있게 학습할 수 있도록 설계되었으며, 특히 공개적으로 접근 가능한 데이터만을 사용함으로써 연구의 재현성과 투명성을 보장했다는 점에서 큰 의의가 있습니다.

### 모델 아키텍처

LLaMA는 [Vaswani](https://arxiv.org/abs/1706.03762) 저자와 연구진이 제안한 트랜스포머 아키텍처를 기반으로 하며, PaLM과 같은 최신 대규모 언어 모델들에서 검증된 여러 개선사항들을 적용했습니다. 주요 아키텍처 변경사항은 다음과 같습니다.

먼저, GPT-3에서 영감을 받은 사전 정규화(Pre-normalization) 방식을 도입했습니다. 이는 트랜스포머의 각 서브레이어의 출력이 아닌 입력을 정규화하는 방식으로, 학습 안정성을 크게 향상시킵니다. 구체적으로, [Zhang과 Sennrich](https://arxiv.org/abs/1910.07467) 저자가 제안한 RMSNorm 정규화 함수를 사용합니다. RMSNorm은 다음과 같이 정의됩니다.

$$ \text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^n x_i^2 + \epsilon}} \cdot \gamma $$

여기서 \\(x\\)는 입력 벡터, \\(n\\)은 벡터의 차원, \\(\epsilon\\)은 수치 안정성을 위한 작은 상수, \\(\gamma\\)는 학습 가능한 스케일 파라미터입니다.

두 번째로, [Shazeer](https://arxiv.org/abs/2002.05202) 저자가 제안한 SwiGLU 활성화 함수를 도입했습니다. 기존 ReLU 비선형성을 대체하여 모델의 성능을 향상시켰습니다. PaLM에서는 피드포워드 네트워크의 중간 차원을 \\(4d\\)로 설정했지만, LLaMA에서는 \\(\frac{2}{3}4d\\)로 조정했습니다. SwiGLU는 다음과 같이 정의됩니다.

$$ \text{SwiGLU}(x) = \text{Swish}_1(xW) \otimes (xV) $$

여기서 \\(\text{Swish}_1(x) = x \cdot \sigma(x)\\)이며, \\(\sigma(x)\\)는 시그모이드 함수입니다.

마지막으로, GPTNeo에서 영감을 받아 절대 위치 임베딩을 제거하고 대신 [Su](https://arxiv.org/abs/2104.09864) 저자와 연구진이 제안한 회전 위치 임베딩(RoPE)을 네트워크의 각 레이어에 추가했습니다. RoPE는 복소수 공간에서의 회전을 통해 상대적 위치 정보를 인코딩합니다.

$$ \mathbf{q}_m = (\mathbf{W}_q \mathbf{x}_m)e^{im\theta} $$
$$ \mathbf{k}_n = (\mathbf{W}_k \mathbf{x}_n)e^{in\theta} $$

여기서 \\(\mathbf{x}_m\\)과 \\(\mathbf{x}_n\\)은 입력 임베딩, \\(\theta\\)는 회전 각도를 결정하는 하이퍼파라미터입니다.

![Training Loss Graph](https://ar5iv.org//html/2302.13971/assets/x1.png)

위 그래프는 7B, 13B, 33B, 65B 파라미터를 가진 네 가지 크기의 LLaMA 모델의 학습 손실을 보여줍니다. 33B와 65B 모델은 1.4T 토큰으로 학습되었고, 더 작은 모델들은 1.0T 토큰으로 학습되었습니다. 모든 모델은 4M 토큰의 배치 크기로 학습되었습니다. 그래프에서 볼 수 있듯이, 더 큰 모델들이 더 많은 데이터로 학습되었을 때 더 낮은 학습 손실을 달성하는 것을 확인할 수 있습니다.

### 옵티마이저

LLaMA 모델의 학습에는 [Loshchilov와 Hutter](https://arxiv.org/pdf/1711.05101) 저자가 제안한 AdamW 옵티마이저가 사용되었습니다. AdamW는 기존 Adam 옵티마이저의 가중치 감쇠(weight decay) 방식을 개선한 알고리즘으로, 적응적 학습률과 모멘텀을 결합하여 효율적인 모델 학습을 가능하게 합니다.

AdamW의 핵심 하이퍼파라미터는 다음과 같이 설정되었습니다.

$$ \beta_1 = 0.9, \beta_2 = 0.95 $$

여기서 \\(\beta_1\\)은 1차 모멘트 추정값의 지수 감소율을, \\(\beta_2\\)는 2차 모멘트 추정값의 지수 감소율을 나타냅니다. 이러한 값들은 모델이 학습 과정에서 그래디언트의 방향과 크기를 얼마나 빠르게 조정할지를 결정합니다.

학습률은 코사인 스케줄링을 통해 조절되며, 최종 학습률은 최대 학습률의 10%가 되도록 설정되었습니다. 코사인 스케줄링은 학습률을 코사인 함수의 형태로 점진적으로 감소시키는 방식으로, 학습 초기에는 빠른 학습을 가능하게 하고 후반부에는 안정적인 수렴을 돕습니다.

가중치 감쇠는 0.1로 설정되어 모델의 과적합을 방지하고 일반화 성능을 향상시킵니다. 또한 그래디언트 클리핑 값을 1.0으로 설정하여 그래디언트 폭주 문제를 방지하고 학습의 안정성을 확보했습니다. 이는 다음과 같은 수식으로 표현됩니다.

$$ \text{grad} = \text{clip}(\text{grad}, -1.0, 1.0) $$

학습 초기에는 2,000 스텝의 웜업 기간을 두어 모델이 점진적으로 학습률을 증가시키며 안정적으로 학습을 시작할 수 있도록 했습니다. 이 웜업 기간 동안의 학습률은 다음과 같이 계산됩니다.

$$ \text{lr}_t = \text{lr}_{\text{max}} \cdot \min(1, \frac{t}{2000}) $$

여기서 \\(\text{lr}_t\\)는 스텝 \\(t\\)에서의 학습률이고, \\(\text{lr}_{\text{max}}\\)는 최대 학습률입니다.

모델의 크기에 따라 학습률과 배치 크기를 적절히 조정했는데, 이는 모델의 규모가 커질수록 더 큰 배치 크기와 더 작은 학습률이 필요하다는 경험적 관찰에 기반합니다. 구체적인 값들은 논문의 표 2에서 확인할 수 있습니다.

### 효율적인 구현

LLaMA 모델의 학습 속도를 향상시키기 위해 연구진은 여러 가지 최적화 기법을 적용했습니다. 첫 번째로, 메모리 사용량과 실행 시간을 줄이기 위해 인과적 멀티헤드 어텐션(causal multi-head attention)의 효율적인 구현을 도입했습니다. [Facebook Research](https://github.com/facebookresearch/xformers)의 xformers 라이브러리에서 제공하는 이 구현은 [Rabe와 Staats](https://arxiv.org/abs/2112.05682) 저자의 연구에서 영감을 받았으며, [Dao](https://arxiv.org/pdf/2205.14135) 저자와 연구진이 제안한 역전파 방식을 활용합니다.

이 최적화의 핵심은 어텐션 가중치를 저장하지 않고, 언어 모델링 작업의 인과적 특성으로 인해 마스킹되는 키/쿼리 점수를 계산하지 않는 것입니다. 이는 다음과 같은 수학적 최적화를 통해 구현됩니다.

$$ \text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V $$

여기서 마스킹된 위치의 계산을 건너뛰고, 중간 결과인 어텐션 가중치 행렬을 저장하지 않음으로써 메모리 효율성을 크게 향상시킵니다.

학습 효율성을 더욱 개선하기 위해, 연구진은 체크포인팅(checkpointing)을 사용하여 역전파 과정에서 재계산되는 활성화(activation) 값의 양을 줄였습니다. 구체적으로, 선형 레이어의 출력과 같이 계산 비용이 큰 활성화 값들을 저장합니다. 이는 PyTorch의 자동 미분(autograd) 대신 트랜스포머 레이어의 역전파 함수를 수동으로 구현함으로써 달성됩니다.

이러한 최적화의 이점을 최대한 활용하기 위해, [Korthikanti](https://arxiv.org/pdf/2205.05198) 저자와 연구진이 제시한 모델 병렬화(model parallelism)와 시퀀스 병렬화(sequence parallelism)를 적용하여 모델의 메모리 사용량을 줄였습니다. 또한, GPU 간의 네트워크 통신(all_reduce 연산으로 인한)과 활성화 값 계산을 최대한 겹치도록 구현했습니다.

이러한 최적화 기법들의 결과로, 65B 파라미터 모델을 학습할 때 80GB RAM을 갖춘 2048개의 A100 GPU에서 GPU당 약 380 토큰/초의 처리 속도를 달성했습니다. 이는 1.4T 토큰으로 구성된 데이터셋에 대한 학습을 약 21일 만에 완료할 수 있게 해주었습니다.

### 주요 실험 결과

LLaMA 모델의 성능을 평가하기 위해 연구진은 Brown 저자와 연구진이 제시한 방법론을 따라 제로 샷(zero-shot)과 퓨 샷(few-shot) 학습 과제를 수행했습니다. 총 20개의 벤치마크에서 모델의 성능을 측정했는데, 이는 크게 두 가지 평가 방식으로 나뉩니다.

제로 샷 평가에서는 모델에게 과제에 대한 텍스트 설명과 테스트 예제만을 제공합니다. 모델은 이를 바탕으로 자유 형식의 텍스트를 생성하거나 주어진 답변 옵션들의 순위를 매기게 됩니다. 반면 퓨 샷 평가에서는 1개에서 64개 사이의 과제 예시를 함께 제공하여, 모델이 이러한 예시들을 참고하여 새로운 테스트 예제에 대한 답변을 생성하거나 순위를 매기도록 합니다.

연구진은 LLaMA를 GPT-3, Gopher, Chinchilla, PaLM과 같은 비공개 대규모 언어 모델들과 비교했습니다. 또한 OPT, GPT-J, GPT-Neo와 같은 오픈소스 모델들과도 성능을 비교 평가했습니다. 추가로 OPT-IML이나 Flan-PaLM과 같은 명령어 튜닝(instruction-tuned) 모델들과도 간단한 비교를 수행했습니다.

평가는 크게 자유 형식 생성 과제와 객관식 과제로 나누어 진행되었습니다. 객관식 과제의 경우, 주어진 문맥에 대해 가장 적절한 답변을 여러 옵션 중에서 선택하는 방식으로 진행되었습니다. 모델은 각 답변 옵션의 확률을 계산하여 가장 높은 확률을 가진 답변을 선택합니다. 

대부분의 데이터셋에서는 Gao 저자와 연구진이 제안한 방식을 따라 답변의 문자 수로 정규화된 확률을 사용했습니다. 하지만 OpenBookQA와 BoolQ와 같은 일부 데이터셋에서는 Brown 저자와 연구진의 방식을 채택하여, "Answer:"라는 문맥이 주어졌을 때의 확률로 정규화된 값을 사용했습니다. 이는 다음과 같은 수식으로 표현됩니다.

$$ P(\text{completion}|\text{context})/P(\text{completion}|\text{"Answer:"}) $$

![학습 과정에서의 성능 변화](https://ar5iv.org//html/2302.13971/assets/x2.png)

위 그래프는 LLaMA의 7B, 13B, 33B, 65B 파라미터 버전들과 Chinchilla 모델의 질문 응답 및 상식 추론 과제에서의 성능 변화를 보여줍니다. 대부분의 벤치마크에서 모델 크기가 증가할수록 성능이 향상되는 것을 확인할 수 있으며, 특히 65B 모델이 가장 우수한 성능을 보여주고 있습니다.
### 주요 실험 결과 (계속)

#### 상식 추론 능력 평가

LLaMA의 상식 추론 능력을 평가하기 위해 연구진은 8개의 표준 벤치마크를 사용했습니다. BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC(easy/challenge), OpenBookQA 등의 데이터셋이 활용되었으며, 이들은 Cloze 스타일 과제와 Winograd 스타일 과제, 그리고 객관식 문제 풀이를 포함합니다.

평가 결과, 65B 파라미터를 가진 LLaMA 모델은 BoolQ를 제외한 모든 벤치마크에서 70B 파라미터의 Chinchilla 모델을 능가했습니다. 또한 BoolQ와 WinoGrande를 제외한 모든 과제에서 540B 파라미터의 PaLM 모델과 비슷하거나 더 나은 성능을 보여주었습니다. 특히 주목할 만한 점은 13B 파라미터의 LLaMA 모델이 175B 파라미터의 GPT-3보다 대부분의 벤치마크에서 더 우수한 성능을 달성했다는 것입니다.

#### 폐쇄형 질문 응답 평가

연구진은 Natural Questions와 TriviaQA라는 두 가지 폐쇄형 질문 응답 벤치마크에서 LLaMA의 성능을 평가했습니다. 폐쇄형 평가란 모델이 외부 문서나 증거를 참조하지 않고 오직 자신의 파라미터에 저장된 지식만을 활용하여 질문에 답변해야 하는 방식을 의미합니다.

Natural Questions 데이터셋에서 LLaMA-65B는 제로 샷 설정에서 23.8%, 1-샷에서 31.0%, 5-샷에서 35.0%, 64-샷에서 39.9%의 정확도를 달성했습니다. 이는 기존의 대규모 언어 모델들과 비교했을 때 매우 경쟁력 있는 결과입니다. 특히 13B 파라미터 모델도 GPT-3와 Chinchilla에 견줄만한 성능을 보여주었는데, 이는 모델의 크기가 5-10배 작음에도 불구하고 달성한 놀라운 성과입니다.

TriviaQA 데이터셋에서도 LLaMA-65B는 제로 샷부터 64-샷까지의 모든 설정에서 최고 수준의 성능을 기록했습니다. 구체적으로 제로 샷에서 68.2%, 64-샷에서 73.0%의 정확도를 달성하여, Chinchilla-70B(64-샷에서 64.6%)와 같은 기존 최고 성능의 모델들을 크게 앞섰습니다.

이러한 결과들은 LLaMA가 단순히 모델 크기를 줄이는 데 성공했을 뿐만 아니라, 실제로 더 효율적인 학습을 통해 더 작은 모델로도 더 우수한 성능을 달성할 수 있다는 것을 입증합니다. 특히 13B 모델이 단일 V100 GPU에서 추론이 가능하면서도 GPT-3급의 성능을 보여준다는 점은, 대규모 언어 모델의 실용적 활용 가능성을 크게 높여주는 성과라고 할 수 있습니다.
#### 독해력 평가

LLaMA의 독해력을 평가하기 위해 연구진은 RACE(Reading Comprehension from Examinations) 벤치마크를 활용했습니다. 이 데이터셋은 중국 중고등학생들의 영어 독해 시험에서 수집된 것으로, 학생들의 실제 독해 능력을 평가하는 데 적합한 난이도와 다양성을 갖추고 있습니다.

Brown 저자와 연구진이 제시한 평가 방식을 따라 수행한 실험에서, LLaMA-65B는 RACE-middle에서 67.9%, RACE-high에서 51.6%의 정확도를 달성했습니다. 이는 540B 파라미터의 PaLM 모델(RACE-middle 68.1%, RACE-high 49.1%)과 비교했을 때 매우 경쟁력 있는 결과입니다. 특히 RACE-high에서는 PaLM을 2.5%p 앞서는 성과를 보여주었습니다.

#### 수학적 추론 능력 평가

수학적 추론 능력을 평가하기 위해 연구진은 MATH와 GSM8k 두 가지 벤치마크를 사용했습니다. MATH는 LaTeX 형식으로 작성된 12,000개의 중고등학교 수준 수학 문제를 포함하고 있으며, GSM8k는 중학교 수준의 수학 문제들로 구성되어 있습니다.

연구진은 LLaMA를 PaLM과 Minerva 모델들과 비교 평가했는데, 여기서 주목할 만한 점은 Minerva가 ArXiv와 수학 관련 웹페이지에서 추출한 38.5B 토큰으로 추가 학습된 모델인 반면, PaLM이나 LLaMA는 수학 데이터에 대한 특별한 학습을 받지 않았다는 것입니다.

평가는 일반적인 방식과 함께 maj@k라는 방식도 사용되었습니다. maj@k는 각 문제에 대해 k개의 답변을 생성한 후 다수결로 최종 답변을 선택하는 방식입니다. 구체적으로 MATH에서는 k=256, GSM8k에서는 k=100을 사용했습니다(Minerva 540B의 경우 각각 k=64와 k=40 사용).

GSM8k에서 LLaMA-65B는 단일 답변 생성 시 50.9%, maj@k 적용 시 69.7%의 정확도를 달성했습니다. 이는 수학 데이터로 특별히 학습된 Minerva-62B(단일 답변 52.4%, maj@k 68.5%)보다도 우수한 성능입니다. 이러한 결과는 LLaMA가 수학적 추론에 있어서도 효과적인 능력을 보유하고 있으며, 특히 다중 답변 생성과 다수결 방식을 통해 더욱 신뢰성 있는 답변을 제공할 수 있다는 것을 보여줍니다.
#### 코드 생성 능력 평가

LLaMA의 코드 생성 능력을 평가하기 위해 연구진은 HumanEval과 MBPP라는 두 가지 벤치마크를 활용했습니다. 이 벤치마크들은 자연어로 된 프로그램 설명과 입출력 예시를 제공하고, 모델이 이를 바탕으로 적절한 코드를 생성하는 능력을 평가합니다. HumanEval의 경우 함수 시그니처도 함께 제공되며, 자연스러운 코드 형식으로 문제 설명과 테스트를 독스트링(docstring)에 포함하여 제시합니다.

연구진은 LLaMA를 PaLM이나 LaMDA와 같이 코드에 특화되지 않은 일반 목적의 언어 모델들과 비교했습니다. 평가 결과, LLaMA는 비슷한 크기의 다른 모델들보다 우수한 성능을 보여주었습니다. 13B 이상의 파라미터를 가진 LLaMA 모델들은 137B 파라미터의 LaMDA를 HumanEval과 MBPP 모두에서 앞섰으며, 65B 모델은 62B PaLM을 능가했습니다.

구체적인 성능을 살펴보면, LLaMA-65B는 HumanEval에서 pass@1 기준 23.7%, pass@100 기준 79.3%의 정확도를 달성했습니다. MBPP에서는 pass@1 기준 37.7%, pass@80 기준 76.8%의 성능을 보여주었습니다. 여기서 pass@k는 k개의 서로 다른 코드를 생성한 후 그 중 하나라도 모든 테스트를 통과하는 경우를 의미합니다. 이러한 평가에서 단일 답안 생성(pass@1)은 0.1의 온도(temperature) 값을, 다중 답안 생성(pass@100, pass@80)은 0.8의 온도 값을 사용했습니다.

연구진은 코드 특화 학습을 통해 더 나은 성능을 달성할 수 있다는 점도 언급했습니다. 예를 들어 PaLM-Coder는 코드 관련 토큰에 대한 추가 학습을 통해 HumanEval의 pass@1 점수를 26.2%에서 36%로 향상시켰습니다. 다만 이러한 코드 특화 학습은 이번 연구의 범위를 벗어나는 것으로 판단하여 시도하지 않았습니다.

#### 대규모 다중 과제 언어 이해력 평가

Hendrycks 저자와 연구진이 개발한 Massive Multitask Language Understanding(MMLU) 벤치마크는 인문학, STEM, 사회과학 등 다양한 분야의 지식을 평가하는 객관식 문제들로 구성되어 있습니다. 연구진은 벤치마크에서 제공하는 5개의 예시를 활용한 5-샷 설정으로 평가를 진행했습니다.

LLaMA-65B는 MMLU에서 평균 63.4%의 정확도를 달성했는데, 이는 Chinchilla-70B(67.5%)와 PaLM-540B(69.3%)에 비해 몇 퍼센트 포인트 낮은 수준입니다. 이러한 차이의 원인으로 연구진은 학습 데이터의 구성 차이를 지목했습니다. LLaMA는 ArXiv, Gutenberg, Books3를 합쳐 총 177GB의 도서 및 학술 논문 데이터만을 사용한 반면, 다른 모델들은 최대 2TB의 도서 데이터를 활용했습니다. 이러한 학습 데이터의 차이가 MMLU와 같은 광범위한 지식을 요구하는 벤치마크에서의 성능 차이로 이어진 것으로 분석됩니다.
#### 학습 과정에서의 성능 변화 분석

연구진은 학습 과정에서 모델의 성능이 어떻게 변화하는지를 면밀히 관찰했습니다. 대부분의 벤치마크에서 모델의 성능은 학습이 진행됨에 따라 꾸준히 향상되었으며, 이는 모델의 학습 퍼플렉시티(perplexity)와 높은 상관관계를 보였습니다. 

하지만 SIQA와 WinoGrande 벤치마크에서는 예외적인 패턴이 관찰되었습니다. 특히 SIQA의 경우 성능의 변동성이 매우 크게 나타났는데, 이는 해당 벤치마크의 신뢰성에 의문을 제기하게 하는 결과입니다. WinoGrande에서는 학습 퍼플렉시티와 성능 간의 상관관계가 다른 벤치마크들에 비해 약했으며, LLaMA-33B와 LLaMA-65B가 학습 과정에서 유사한 수준의 성능을 보여주었습니다.

#### 명령어 튜닝을 통한 성능 향상

연구진은 기본 LLaMA 모델이 이미 기초적인 지시사항을 따를 수 있는 능력을 보여주었지만, 소량의 명령어 튜닝만으로도 MMLU 벤치마크에서의 성능을 크게 향상시킬 수 있다는 것을 발견했습니다. Chung 저자와 연구진의 프로토콜을 따라 단일 실험을 진행한 결과, 명령어 튜닝된 LLaMA-I 65B 모델은 MMLU에서 68.9%의 정확도를 달성했습니다.

이는 OPT-IML이나 Flan-PaLM과 같은 기존의 중간 규모 명령어 튜닝 모델들의 성능을 뛰어넘는 결과입니다. 하지만 GPT code-davinci-002가 달성한 77.4%의 최고 성능과는 여전히 상당한 격차가 있습니다. 이러한 결과는 간단한 명령어 튜닝 접근방식만으로도 모델의 성능을 효과적으로 향상시킬 수 있다는 가능성을 보여주는 동시에, 더 발전된 튜닝 방법론의 필요성도 시사합니다.

이러한 실험 결과들을 종합해보면, LLaMA는 다양한 자연어 처리 과제에서 우수한 성능을 보여주면서도, 모델 크기를 효율적으로 줄이는 데 성공했다는 것을 알 수 있습니다. 특히 명령어 튜닝과 같은 추가적인 학습 방법을 통해 모델의 성능을 더욱 향상시킬 수 있다는 점은, LLaMA가 실용적인 응용에서도 큰 잠재력을 가지고 있음을 보여줍니다.

### 편향성, 유해성 및 허위정보 평가

대규모 언어 모델들은 학습 데이터에 존재하는 편향성을 재생산하고 증폭시키는 경향이 있으며, 유해하거나 공격적인 내용을 생성할 수 있다는 것이 여러 연구를 통해 밝혀졌습니다. LLaMA 연구진은 웹 데이터를 대규모로 활용한 학습 과정을 고려할 때, 모델이 이러한 문제들을 얼마나 내포하고 있는지 파악하는 것이 매우 중요하다고 판단했습니다.

RealToxicityPrompts 벤치마크를 통한 평가 결과, 모델의 크기가 커질수록 유해한 내용을 생성할 가능성이 높아지는 것으로 나타났습니다. 특히 "Complete the following sentence in a polite, respectful, and unbiased manner:"와 같은 존중적인 프롬프트를 사용했을 때도 이러한 경향이 관찰되었습니다. 구체적으로, LLaMA-65B 모델은 기본 프롬프트에서 0.128, 존중적 프롬프트에서 0.141의 유해성 점수를 기록했는데, 이는 작은 모델들보다 높은 수치입니다.

CrowS-Pairs 벤치마크를 통해 평가한 사회적 편향성 분석에서는, LLaMA-65B가 종교(79.0%), 연령(70.1%), 성별(70.6%) 범주에서 특히 높은 편향성을 보였습니다. 이는 CommonCrawl 데이터에 내재된 편향성이 여러 필터링 단계를 거쳤음에도 모델에 반영된 것으로 해석됩니다.

WinoGender 벤치마크를 통한 성별 편향성 심층 분석에서는 더욱 흥미로운 패턴이 발견되었습니다. LLaMA-65B는 성중립적인 대명사("their/them/someone")에 대해서는 81.7%의 높은 정확도를 보였지만, 성별이 있는 대명사("her/her/she", "his/him/he")에 대해서는 각각 78.8%와 72.1%로 상대적으로 낮은 성능을 보였습니다. 특히 "gotcha" 케이스(직업의 다수 성별과 대명사의 성별이 일치하지 않는 경우)에서는 성능이 더욱 저하되어, 모델이 직업과 성별에 관한 사회적 고정관념을 학습했음을 시사합니다.

TruthfulQA 벤치마크를 통한 진실성 평가에서는, LLaMA-65B가 GPT-3보다는 우수한 성능을 보였지만(진실성 0.57, 진실성*정보성 0.53), 여전히 정답률이 낮아 허위 정보를 생성할 가능성이 있는 것으로 나타났습니다. 이는 모델이 학습 데이터에 존재하는 잘못된 정보나 미신을 그대로 학습했을 수 있음을 의미합니다.

이러한 평가 결과들은 LLaMA가 다른 대규모 언어 모델들과 마찬가지로 사회적 편향성과 유해성, 허위정보 생성의 위험을 내포하고 있음을 보여줍니다. 특히 모델의 규모가 커질수록 이러한 문제들이 더욱 심화되는 경향이 있어, 향후 이를 완화하기 위한 추가적인 연구와 기술적 개선이 필요할 것으로 판단됩니다.

### 탄소 발자국 분석

대규모 언어 모델의 학습 과정은 상당한 양의 에너지를 소비하며, 이는 필연적으로 이산화탄소 배출로 이어집니다. LLaMA 연구진은 Wu 저자와 연구진이 제시한 방법론을 활용하여 모델 학습에 따른 에너지 소비량과 탄소 배출량을 체계적으로 분석했습니다.

에너지 소비량 계산을 위해 다음과 같은 수식이 사용되었습니다.

$$ \text{Wh} = \text{GPU-h} \times \text{(GPU power consumption)} \times \text{PUE} $$

여기서 PUE(Power Usage Effectiveness)는 데이터 센터의 에너지 효율성을 나타내는 지표로, 1.1로 설정되었습니다. 이는 데이터 센터가 컴퓨팅에 직접 사용하는 에너지 외에 추가로 10%의 에너지를 냉각 등의 부대 설비에 사용한다는 것을 의미합니다.

탄소 배출량 산정에 있어 주목할 만한 점은 데이터 센터의 위치에 따라 배출 계수가 크게 달라진다는 것입니다. 예를 들어, BLOOM 모델이 학습된 데이터 센터는 킬로와트시당 0.057kg의 이산화탄소를 배출하여 총 27 tCO₂eq가 발생한 반면, OPT 모델의 경우 킬로와트시당 0.231kg의 배출 계수로 인해 82 tCO₂eq가 발생했습니다.

공정한 비교를 위해 연구진은 미국의 평균 탄소 집약도인 0.385 kg CO₂eq/KWh를 기준으로 통일하여 계산했습니다. 이에 따른 탄소 배출량은 다음 수식으로 산출됩니다.

$$ \text{tCO}_2\text{eq} = \text{MWh} \times 0.385 $$

LLaMA 모델 시리즈 개발에는 2,048개의 A100-80GB GPU가 약 5개월 동안 사용되었으며, 이는 약 2,638 MWh의 전력 소비와 1,015 tCO₂eq의 탄소 배출로 이어졌습니다. 이는 OPT 모델이 992개의 A100-80GB GPU로 34일간 학습된 것과 비교할 수 있는 수준입니다.

연구진은 이러한 상당한 환경 영향에도 불구하고, 이미 학습된 모델을 공개함으로써 향후 유사한 모델의 중복 학습을 방지하고, 특히 작은 규모의 모델들이 단일 GPU에서도 구동 가능하다는 점에서 전체적인 탄소 배출 감소에 기여할 수 있을 것으로 기대하고 있습니다.

### 관련 연구

언어 모델의 역사적 발전은 Shannon이 1948년과 1951년에 제시한 통계적 언어 모델링의 기초 이론에서 시작됩니다. Shannon은 언어를 단어, 토큰, 또는 문자의 시퀀스에 대한 확률 분포로 정의했으며, 이는 현대 언어 모델링의 근간이 되었습니다. 이러한 접근 방식은 Bahl 저자와 연구진, Brown 저자와 연구진이 1983년과 1990년에 각각 발전시켰으며, 다음 토큰을 예측하는 과제는 자연어 처리의 핵심 문제로 자리잡았습니다.

특히 주목할 만한 점은 Turing이 1950년에 제안한 "모방 게임"을 통한 기계 지능 측정 방법입니다. 이는 언어를 통해 인공지능의 발전을 평가할 수 있다는 아이디어를 제시했으며, Mahoney는 1999년에 이를 확장하여 언어 모델링을 인공지능 발전의 벤치마크로 제안했습니다.

언어 모델의 아키텍처 발전 과정을 살펴보면, 초기에는 n-gram 통계에 기반한 모델이 주를 이뤘습니다. 이러한 접근법에서는 희소한 이벤트의 확률을 더 정확하게 추정하기 위해 Katz, Kneser와 Ney 저자가 각각 제안한 다양한 스무딩 기법들이 활용되었습니다.

지난 20년간 신경망이 언어 모델링 분야에 성공적으로 적용되면서 큰 발전이 있었습니다. Bengio 저자와 연구진이 2000년에 제안한 피드포워드 모델을 시작으로, Elman, Mikolov 저자와 연구진이 각각 발전시킨 순환 신경망(RNN), 그리고 Hochreiter와 Schmidhuber, Graves 저자가 연구한 LSTM 구조가 차례로 등장했습니다. 최근에는 Vaswani 저자와 연구진이 제안한 셀프 어텐션 기반의 트랜스포머 네트워크가 등장하여, 특히 장거리 의존성을 포착하는 데 있어 큰 성능 향상을 이끌어냈습니다.

모델과 데이터셋의 규모 확장 측면에서도 주목할 만한 발전이 있었습니다. Brants 저자와 연구진은 2007년에 2조 개의 토큰으로 학습된 언어 모델이 기계 번역의 품질을 향상시킬 수 있음을 보여주었습니다. 이들은 'Stupid Backoff'라는 간단한 스무딩 기법을 사용했는데, 이후 Heafield 저자와 연구진이 2013년에 Kneser-Ney 스무딩을 웹 규모의 데이터에 적용할 수 있는 방법을 개발했습니다. 이를 통해 CommonCrawl의 9,750억 토큰으로 학습된 5-gram 모델이 5,000억 개의 n-gram을 포함할 수 있게 되었습니다.

2013년 Chelba 저자와 연구진이 도입한 One Billion Word 벤치마크는 언어 모델의 발전을 측정하기 위한 대규모 학습 데이터셋을 제공했습니다. 신경망 기반 언어 모델 분야에서는 Jozefowicz 저자와 연구진이 10억 개의 파라미터를 가진 LSTM을 확장하여 이 벤치마크에서 최고 성능을 달성했습니다.
이후 트랜스포머 아키텍처의 확장은 자연어 처리의 여러 과제에서 획기적인 발전을 이끌어냈습니다. BERT(Bidirectional Encoder Representations from Transformers)는 Devlin 저자와 연구진이 2018년에 발표한 양방향 트랜스포머 인코더로, 문맥을 양방향으로 고려하는 혁신적인 접근 방식을 도입했습니다. 이어서 GPT-2, Megatron-LM, T5와 같은 모델들이 등장하며 트랜스포머 기반 모델의 성능을 더욱 향상시켰습니다.

특히 Brown 저자와 연구진이 2020년에 발표한 GPT-3는 1,750억 개의 파라미터를 가진 대규모 언어 모델로, 자연어 처리 분야에 큰 전환점을 가져왔습니다. 이를 계기로 Jurassic-1, Megatron-Turing NLG, Gopher, Chinchilla, PaLM, OPT, GLM과 같은 대규모 언어 모델들이 잇따라 등장했습니다.

모델 규모 확장의 효과에 대한 체계적인 연구도 진행되었습니다. Hestness 저자와 연구진, Rosenfeld 저자와 연구진은 2017년과 2019년에 각각 딥러닝 모델의 성능이 모델 크기와 데이터셋 크기에 따라 멱법칙(power law) 관계를 보인다는 것을 밝혀냈습니다. 이러한 관계는 다음과 같은 수식으로 표현됩니다.

$$ \text{Performance} \propto N^\alpha D^\beta $$

여기서 \\(N\\)은 모델의 파라미터 수, \\(D\\)는 데이터셋의 크기, \\(\alpha\\)와 \\(\beta\\)는 스케일링 지수를 나타냅니다.

Kaplan 저자와 연구진은 2020년에 트랜스포머 기반 언어 모델에 특화된 스케일링 법칙을 도출했습니다. 이들의 연구는 모델 크기, 데이터셋 크기, 컴퓨팅 예산 사이의 최적 관계를 수식화했습니다.

$$ L(N, D) = \left(\frac{C}{ND}\right)^\alpha $$

여기서 \\(L\\)은 손실 함수, \\(C\\)는 컴퓨팅 예산, \\(\alpha\\)는 스케일링 지수를 나타냅니다.

이후 Hoffmann 저자와 연구진은 2022년에 이 스케일링 법칙을 더욱 정교화했습니다. 특히 데이터셋 크기를 확장할 때 학습률 스케줄을 적응적으로 조정하는 방법을 제안했습니다. 마지막으로 Wei 저자와 연구진은 스케일링이 대규모 언어 모델의 다양한 능력에 미치는 영향을 체계적으로 연구했습니다. 이들의 연구는 모델 규모가 커질수록 새로운 능력이 창발적으로 나타날 수 있다는 흥미로운 가능성을 제시했습니다.

### 결론

본 연구에서는 현재 최고 수준의 기초 모델들과 경쟁력을 갖추면서도 공개적으로 배포 가능한 일련의 언어 모델을 소개했습니다. 가장 주목할 만한 점은 13B 파라미터의 LLaMA 모델이 175B 파라미터의 GPT-3보다 10배 이상 작으면서도 더 우수한 성능을 달성했다는 것입니다. 또한 65B 파라미터의 LLaMA는 Chinchilla-70B와 PaLM-540B와 같은 최신 대규모 언어 모델들과 대등한 성능을 보여주었습니다.

이전 연구들과 달리, 본 연구는 독점적인 데이터셋에 의존하지 않고도 공개적으로 접근 가능한 데이터만으로 최고 수준의 성능을 달성할 수 있다는 것을 입증했습니다. 연구진은 이러한 모델들을 연구 커뮤니티에 공개함으로써 대규모 언어 모델의 발전을 가속화하고, 유해성이나 편향성과 같은 알려진 문제점들을 완화하기 위한 노력을 지원할 수 있기를 기대합니다.

[Chung](https://arxiv.org/abs/2210.11416) 저자와 연구진의 연구에서와 같이, 명령어에 대한 파인튜닝이 유망한 결과를 보여준다는 것을 확인했으며, 연구진은 향후 연구에서 이를 더욱 깊이 있게 탐구할 계획입니다. 또한 모델의 규모를 키우면서 지속적인 성능 향상이 관찰되었기 때문에, 연구진은 앞으로 더 큰 사전 학습 데이터셋으로 더 큰 모델을 공개할 계획을 가지고 있습니다.

이러한 결과들은 대규모 언어 모델 분야에서 중요한 전환점을 제시합니다. 특히 모델의 크기를 줄이면서도 성능을 유지하거나 향상시킬 수 있다는 점은, 향후 더 효율적이고 실용적인 언어 모델 개발의 가능성을 보여줍니다. 또한 공개 데이터만으로도 최고 수준의 성능을 달성할 수 있다는 사실은, 언어 모델 연구의 민주화와 투명성 향상에 기여할 것으로 기대됩니다.

- - -
### References
* [LLaMA: Open and Efficient Foundation Language Models](http://arxiv.org/pdf/2302.13971v1)