---
layout: post
title: "LLaMA: Open and Efficient Foundation Language Models"
date: 2023-02-27 17:11:15
author: "Meta AI"
categories: "Natural-Language-Processing"
tags: ["Open-Source-Foundation-Language-Models", "Efficient-Transformer-Architecture", "Rotary-Positional-Embeddings", "Scaling-Laws", "Responsible-AI-Evaluation"]
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

대규모 언어 모델 연구는 그동안 비공개 데이터셋에 크게 의존해왔으며, 이는 연구의 재현성과 접근성을 제한하는 주요 요인이었습니다. GPT-3, PaLM, Chinchilla와 같은 최신 모델들이 "Books-2TB"나 "Social media conversations"와 같은 비공개 데이터셋을 활용한 것이 대표적인 사례입니다. 또한, 기존 모델들은 방대한 규모의 매개변수를 필요로 했기 때문에 연구 커뮤니티의 실험과 개선이 제한적이었습니다. 이러한 배경에서 Meta AI는 완전히 공개된 데이터셋만을 사용하면서도 최고 수준의 성능을 달성할 수 있는 효율적인 언어 모델 개발을 목표로 연구를 시작했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

LLaMA는 Kaplan의 스케일링 법칙을 기반으로 하되, 모델의 추론 비용 최적화에 특별한 주안점을 둔 새로운 접근방식을 제시했습니다. 특히 주목할 만한 점은 완전히 공개된 데이터셋만을 사용하면서도 효율적인 모델 구조를 통해 최고 수준의 성능을 달성했다는 것입니다. 7B에서 65B에 이르는 다양한 규모의 모델을 개발하여, 특히 13B 모델이 175B 매개변수의 GPT-3를 능가하는 성과를 달성했습니다. 또한 Pre-normalization, SwiGLU 활성화 함수, Rotary Position Embeddings 등 최신 기술들을 효과적으로 통합하여 모델의 성능을 향상시켰습니다.

#### 제안된 방법은 어떻게 구현되었습니까?

LLaMA의 구현은 세 가지 핵심 전략을 중심으로 이루어졌습니다. 첫째, xformers 라이브러리를 활용한 효율적인 인과적 멀티헤드 어텐션 구현으로 메모리 사용량과 실행 시간을 크게 줄였습니다. 둘째, AdamW 최적화기와 코사인 학습률 스케줄링을 통해 훈련의 안정성을 확보했습니다. 셋째, CommonCrawl, Wikipedia, GitHub, ArXiv 등 다양한 공개 데이터셋을 철저한 품질 관리 과정을 거쳐 통합했습니다. 특히 훈련 데이터의 전처리 과정에서는 언어 식별, 품질 필터링, 중복 제거 등 엄격한 기준을 적용하여 데이터의 질을 보장했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

LLaMA의 성과는 대규모 언어 모델 연구에 있어 두 가지 중요한 패러다임 전환을 제시합니다. 첫째, 비공개 데이터셋 없이도 최고 수준의 성능을 달성할 수 있다는 것을 입증함으로써, 언어 모델 연구의 민주화 가능성을 보여주었습니다. 둘째, 13B 모델이 GPT-3를 능가하는 성과를 통해, 효율적인 모델 설계가 단순한 규모 확장보다 중요할 수 있다는 점을 입증했습니다. 이는 향후 언어 모델 연구가 단순한 규모 확장이 아닌, 효율성과 접근성을 중심으로 발전할 수 있는 새로운 방향을 제시한다는 점에서 큰 의의를 가집니다.
- - -
## LLaMA: 공개 데이터로 훈련된 효율적인 기초 언어 모델

Meta AI가 개발한 LLaMA는 인공지능 연구 커뮤니티에 중요한 전환점을 제시하는 기초 언어 모델 컬렉션입니다. 7B에서 65B에 이르는 다양한 매개변수 규모로 구성된 이 모델군은 특히 주목할 만한 기술적 성과를 달성했습니다. 기존의 대규모 언어 모델들이 비공개 데이터셋에 크게 의존했던 것과 달리, LLaMA는 완전히 공개된 데이터셋만을 사용하여 최첨단 성능을 달성했다는 점에서 혁신적입니다.

[Kaplan과 연구진](https://arxiv.org/pdf/2001.08361)이 제시한 스케일링 법칙을 기반으로 설계된 LLaMA는 모델 크기와 성능 사이의 최적 균형점을 찾는데 성공했습니다. 특히 주목할 만한 점은 13B 매개변수를 가진 LLaMA-13B가 175B 매개변수의 GPT-3를 대부분의 벤치마크에서 능가했다는 것입니다. 이는 모델 구조의 효율성과 훈련 방법론의 우수성을 입증하는 결과입니다.

더 나아가 LLaMA-65B는 현재 최고 수준으로 평가받는 Chinchilla-70B와 PaLM-540B와 견줄 만한 성능을 보여주었습니다. 이는 모델의 규모를 크게 늘리지 않고도 경쟁력 있는 성능을 달성할 수 있다는 것을 증명합니다. [FlashAttention](https://arxiv.org/pdf/2205.14135)과 같은 최신 어텐션 메커니즘의 효율적인 구현과 [AdamW](https://arxiv.org/pdf/1711.05101)와 같은 최적화 기법의 적절한 활용이 이러한 성과에 기여했습니다.

Meta AI는 이 모델들을 연구 커뮤니티에 공개함으로써, 언어 모델 연구의 민주화에 크게 기여했습니다. GitHub를 통해 제공되는 이 모델들은 연구자들이 자유롭게 접근하고 실험할 수 있어, 향후 언어 모델 발전에 중요한 기반이 될 것으로 기대됩니다. 이는 [BERT](https://arxiv.org/pdf/1810.04805)가 그랬던 것처럼, 오픈 소스 언어 모델의 새로운 이정표가 될 것입니다.
### 대규모 언어 모델의 새로운 패러다임: 효율성과 접근성의 균형

대규모 언어 모델(Large Language Models, LLMs)의 발전은 인공지능 분야에서 가장 주목할 만한 성과 중 하나입니다. [Brown과 연구진](https://arxiv.org/abs/2005.14165)이 제시한 연구에서 볼 수 있듯이, 방대한 텍스트 데이터로 훈련된 LLM들은 텍스트 지시사항이나 소수의 예시만으로도 새로운 과제를 수행할 수 있는 놀라운 능력을 보여주었습니다.

[Kaplan과 연구진](https://arxiv.org/pdf/2001.08361)의 연구는 모델 규모와 성능 간의 관계를 수학적으로 정립했습니다. 이들이 발견한 스케일링 법칙은 다음과 같은 수식으로 표현됩니다.

$$ L(N) = (N_c/N)^{\alpha_N} $$

여기서 \\(L\\)은 손실함수, \\(N\\)은 모델 매개변수 수, \\(N_c\\)는 임계 매개변수 수를 나타냅니다. 그러나 [Hoffmann과 연구진](https://arxiv.org/pdf/2203.15556)은 주어진 계산 예산 내에서 최고의 성능은 가장 큰 모델이 아닌, 더 많은 데이터로 훈련된 상대적으로 작은 모델에서 달성된다는 중요한 발견을 제시했습니다.

이러한 맥락에서 LLaMA는 혁신적인 접근방식을 제시합니다. 기존 연구들이 훈련 비용 최적화에 초점을 맞췄다면, LLaMA는 실제 서비스 환경에서 더욱 중요한 추론 비용에 주목했습니다. 예를 들어, 10B 매개변수 모델을 200B 토큰으로 훈련하는 것이 최적이라는 기존의 추천과 달리, LLaMA의 7B 모델은 1T 토큰 이상의 훈련에서도 지속적인 성능 향상을 보여줍니다.

특히 주목할 만한 점은 LLaMA가 전적으로 공개 데이터만을 사용했다는 것입니다. GPT-3, PaLM, Chinchilla와 같은 기존의 최고 성능 모델들이 비공개 또는 문서화되지 않은 데이터셋("Books-2TB" 또는 "Social media conversations" 등)에 의존했던 것과 달리, LLaMA는 완전한 투명성과 재현 가능성을 제공합니다. [OPT](https://arxiv.org/abs/2205.01068), [GPT-NeoX](https://arxiv.org/abs/2204.06745), [BLOOM](https://arxiv.org/abs/2211.05100)과 같은 오픈소스 모델들도 있었지만, PaLM-62B나 Chinchilla와 경쟁할 만한 성능은 보여주지 못했습니다.

이러한 LLaMA의 접근방식은 대규모 언어 모델 연구의 민주화를 촉진할 것으로 기대됩니다. 특히 13B 매개변수 모델은 단일 GPU에서도 구동이 가능하면서 GPT-3를 능가하는 성능을 보여주어, 연구자들의 접근성을 크게 향상시켰습니다. 더불어 65B 매개변수 모델은 Chinchilla나 PaLM-540B와 견줄만한 성능을 달성하여, 공개 데이터만으로도 최고 수준의 성능이 가능함을 입증했습니다.
## LLaMA의 훈련 방법론: 효율성과 확장성의 조화

LLaMA의 훈련 접근 방식은 [Brown과 연구진](https://arxiv.org/pdf/2005.14165)과 [Chowdhery와 연구진](https://arxiv.org/pdf/2204.02311)이 제시한 방법론을 기반으로 하되, [Hoffmann과 연구진](https://arxiv.org/pdf/2203.15556)의 Chinchilla 스케일링 법칙을 적극적으로 활용하여 발전시켰습니다. 이 접근법의 핵심은 대규모 텍스트 데이터에 대한 표준 트랜스포머 모델의 최적화된 훈련에 있습니다.

Chinchilla 스케일링 법칙에 따르면, 모델의 최적 성능은 다음과 같은 손실 함수로 표현됩니다.

$$ L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta} $$

여기서 \\(N\\)은 모델의 매개변수 수, \\(D\\)는 훈련 데이터의 토큰 수를 나타내며, \\(E\\), \\(A\\), \\(B\\), \\(\alpha\\), \\(\beta\\)는 실험적으로 결정되는 상수입니다. 이 법칙은 주어진 계산 예산 내에서 모델 크기와 훈련 데이터 크기 사이의 최적 균형점을 찾는 이론적 기반을 제공합니다.

LLaMA의 구현에서는 이 이론적 기반을 실제 시스템으로 구현하기 위해 다음과 같은 트랜스포머 아키텍처를 채택했습니다.

```python
class Transformer(nn.Module):
    def __init__(self, params: ModelArgs):
        super().__init__()
        self.params = params
        self.vocab_size = params.vocab_size
        self.n_layers = params.n_layers
        
        self.tok_embeddings = ParallelEmbedding(
            params.vocab_size, params.dim, init_method=lambda x: x
        )
        
        self.layers = torch.nn.ModuleList()
        for layer_id in range(params.n_layers):
            self.layers.append(TransformerBlock(layer_id, params))
```

이 구현에서 특히 주목할 만한 점은 모델 병렬화를 위한 `ParallelEmbedding`의 사용과 효율적인 계층 정규화를 위한 RMSNorm의 적용입니다. 또한 어텐션 메커니즘에서는 회전 위치 임베딩(Rotary Position Embeddings)을 도입하여 시퀀스 길이에 따른 확장성을 개선했습니다.

$$ \text{RotaryEmbed}(x_i, \theta_i) = x_i \cdot (\cos(\theta_i) + i\sin(\theta_i)) $$

훈련 과정에서는 AdamW 최적화기를 사용하며, 코사인 학습률 스케줄링을 적용하여 훈련의 안정성을 높였습니다. 이는 다음과 같은 수식으로 표현됩니다.

$$ \eta_t = \eta_{\text{max}} \cdot \frac{1 + \cos(\pi t/T)}{2} $$

여기서 \\(\eta_t\\)는 시간 \\(t\\)에서의 학습률, \\(\eta_{\text{max}}\\)는 최대 학습률, \\(T\\)는 총 훈련 스텝 수를 나타냅니다.

이러한 기술적 선택들은 단순히 이론적 최적화를 넘어 실제 훈련 환경에서의 효율성을 고려한 것입니다. 특히 모델 병렬화와 분산 훈련을 위한 구현은 대규모 언어 모델의 실용적 훈련을 가능하게 하는 핵심 요소입니다.
### LLaMA의 사전 학습 데이터: 다양성과 품질의 균형

LLaMA의 사전 학습 데이터셋은 공개적으로 이용 가능한 다양한 도메인의 텍스트를 포함하도록 신중하게 구성되었습니다. 이 데이터셋은 CommonCrawl을 중심으로 하되, 학술 문헌, 코드, 온라인 토론 등 다양한 전문 영역의 텍스트를 포함하여 모델의 범용성을 확보했습니다.

전체 데이터셋의 가장 큰 부분을 차지하는 것은 CommonCrawl로, 전체의 67%를 구성합니다. [Wenzek과 연구진](https://arxiv.org/abs/2003.12457)이 개발한 CCNet 파이프라인을 통해 2017년부터 2020년까지의 CommonCrawl 덤프를 처리했습니다. 이 과정에서는 fastText 선형 분류기를 사용한 언어 식별, n-gram 언어 모델을 통한 품질 필터링, 그리고 줄 단위 중복 제거가 수행되었습니다. 특히 주목할 만한 점은 Wikipedia 참조 페이지와 무작위 샘플링된 페이지를 구분하는 선형 모델을 훈련시켜, 참조 페이지로 분류되지 않은 콘텐츠를 제외했다는 것입니다.

C4 데이터셋은 전체의 15%를 차지하며, [Raffel과 연구진](https://arxiv.org/abs/1910.10683)이 개발한 전처리 파이프라인을 통해 처리되었습니다. CCNet과 마찬가지로 중복 제거와 언어 식별 단계를 포함하지만, 품질 필터링에서는 문장부호의 존재나 단어 및 문장 수와 같은 휴리스틱을 주로 활용한다는 점에서 차이가 있습니다.

GitHub 데이터(4.5%)는 Apache, BSD, MIT 라이선스로 배포된 프로젝트만을 선별적으로 포함했습니다. 데이터 품질을 보장하기 위해 다음과 같은 전처리 과정이 적용되었습니다.

```python
def process_github_data(content):
    """
    GitHub 데이터 전처리를 위한 품질 필터링 함수
    
    Args:
        content (str): 처리할 파일 내용
        
    Returns:
        str: 필터링된 내용
    """
    # 줄 길이 기반 필터링
    lines = [line for line in content.split('\n') 
            if len(line.strip()) > MIN_LINE_LENGTH]
    
    # 알파벳 문자 비율 검사
    alpha_ratio = sum(c.isalnum() for c in content) / len(content)
    if alpha_ratio < MIN_ALPHA_RATIO:
        return None
        
    # 헤더 등 상용구 제거
    content = remove_boilerplate(content)
    
    return content
```

Wikipedia 데이터(4.5%)는 2022년 6월에서 8월 사이의 덤프를 사용했으며, 라틴 문자나 키릴 문자를 사용하는 20개 언어를 포함합니다. 하이퍼링크, 주석, 서식 관련 태그 등은 정규 표현식을 통해 제거되었습니다.

책 말뭉치(4.5%)는 Gutenberg Project와 ThePile의 Books3 섹션을 포함하며, 90% 이상의 내용이 중복되는 책들은 제거되었습니다. ArXiv 데이터(2.5%)는 [Lewkowycz과 연구진](https://arxiv.org/abs/2206.05685)의 방법론을 따라 처리되었으며, 첫 섹션 이전의 내용과 참고문헌을 제외하고, LaTeX 매크로를 인라인으로 확장하여 일관성을 높였습니다.

토큰화는 [SentencePiece](https://github.com/google/sentencepiece)의 BPE(Byte-Pair Encoding) 알고리즘을 사용했습니다. 특히 모든 숫자를 개별 자릿수로 분할하고, 알 수 없는 UTF-8 문자는 바이트 단위로 처리하는 방식을 채택했습니다. 최종적으로 구축된 훈련 데이터셋은 약 1.4T 토큰을 포함하며, Wikipedia와 Books 도메인을 제외한 대부분의 데이터는 훈련 중 한 번만 사용되었습니다.
## LLaMA의 아키텍처: 효율성과 성능의 균형

LLaMA는 [Vaswani와 연구진](https://arxiv.org/pdf/1706.03762)이 제안한 트랜스포머 아키텍처를 기반으로 하되, 최신 연구 결과들을 통합하여 상당한 개선을 이루어냈습니다. 특히 PaLM과 같은 최신 언어 모델들에서 검증된 여러 아키텍처 개선사항들을 채택하여 모델의 효율성과 성능을 높였습니다.

가장 주목할 만한 첫 번째 개선사항은 Pre-normalization 방식의 도입입니다. GPT-3에서 영감을 받은 이 접근법은 각 트랜스포머 서브레이어의 출력이 아닌 입력을 정규화합니다. 이를 위해 [Zhang과 연구진](https://arxiv.org/pdf/1910.07467)이 제안한 RMSNorm을 사용하며, 다음과 같이 구현됩니다.

```python
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
```

이 정규화 방식은 평균 계산을 제거하고 제곱평균만을 사용하여 계산 효율성을 높이면서도, 모델의 안정성을 유지합니다.

두 번째 주요 개선사항은 [Shazeer](https://arxiv.org/pdf/2002.05202)가 제안한 SwiGLU 활성화 함수의 도입입니다. PaLM에서 사용된 이 활성화 함수는 기존의 ReLU를 대체하여 성능을 향상시킵니다. 수학적으로 SwiGLU는 다음과 같이 표현됩니다.

$$ \text{SwiGLU}(x, W, V) = \text{Swish}(xW) \cdot (xV) $$

여기서 Swish 함수는 $$ \text{Swish}(x) = x \cdot \sigma(\beta x) $$로 정의되며, \\(\sigma\\)는 시그모이드 함수입니다. LLaMA에서는 PaLM과 달리 은닉층 차원을 \\(\frac{2}{3}4d\\) 로 설정하여 계산 효율성을 더욱 높였습니다.

세 번째 핵심 개선사항은 [Su와 연구진](https://arxiv.org/pdf/2104.09864)이 제안한 Rotary Position Embeddings(RoPE)의 도입입니다. GPTNeo에서 영감을 받은 이 방식은 절대 위치 임베딩을 제거하고 대신 각 레이어에서 회전 위치 임베딩을 적용합니다. RoPE는 다음과 같이 구현됩니다.

```python
def apply_rotary_emb(xq, xk, freqs_cis):
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xk)
```

![Training loss curves](https://ar5iv.org//html/2302.13971/assets/x1.png)
위 그래프는 7B, 13B, 33B, 65B 모델의 훈련 토큰 수에 따른 훈련 손실을 보여줍니다. 33B와 65B 모델은 1.4T 토큰으로 훈련되었으며, 작은 모델들은 1.0T 토큰으로 훈련되었습니다. 모든 모델은 4M 토큰의 배치 크기로 훈련되었습니다. 그래프에서 볼 수 있듯이, 모델 크기가 증가할수록 더 낮은 훈련 손실에 도달하며, 이는 모델의 용량이 증가함에 따라 학습 능력이 향상됨을 보여줍니다.
## LLaMA의 최적화 전략: AdamW와 학습률 스케줄링

LLaMA 모델의 훈련에서는 [Loshchilov와 Hutter](https://arxiv.org/pdf/1711.05101)가 제안한 AdamW 최적화기를 채택했습니다. AdamW는 기존 Adam 최적화기의 가중치 감쇠(weight decay) 구현 방식을 개선한 버전으로, 적응적 학습률과 모멘텀을 결합하여 효과적인 최적화를 가능하게 합니다.

AdamW의 핵심 아이디어는 L2 정규화를 가중치 업데이트 과정에서 분리하는 것입니다. 이는 다음과 같은 수식으로 표현됩니다.

$$ \theta_{t+1} = \theta_t - \eta(\alpha m_t/\sqrt{v_t} + \lambda\theta_t) $$

여기서 \\(\theta_t\\)는 현재 파라미터, \\(\eta\\)는 학습률, \\(\alpha\\)는 스텝 크기, \\(m_t\\)와 \\(v_t\\)는 각각 첫 번째와 두 번째 모멘트 추정값, \\(\lambda\\)는 가중치 감쇠 계수입니다.

LLaMA에서는 AdamW의 하이퍼파라미터를 다음과 같이 설정했습니다.
- \\(\beta_1 = 0.9\\): 첫 번째 모멘트 추정값의 지수 감쇠율
- \\(\beta_2 = 0.95\\): 두 번째 모멘트 추정값의 지수 감쇠율

학습률 스케줄링에는 코사인 스케줄을 채택했습니다. 이 스케줄은 다음 수식으로 정의됩니다.

$$ \eta_t = \eta_{final} + \frac{1}{2}(\eta_{max} - \eta_{final})(1 + \cos(\pi t/T)) $$

여기서 \\(\eta_{final}\\)은 최대 학습률의 10%로 설정되며, \\(T\\)는 총 훈련 스텝 수입니다. 이러한 스케줄링은 훈련 초기에는 빠른 학습을 가능하게 하고, 후반부에는 점진적으로 학습률을 감소시켜 안정적인 수렴을 돕습니다.

추가적인 안정화 전략으로 가중치 감쇠값을 0.1로 설정하고, 그래디언트 클리핑을 1.0으로 제한했습니다. 또한 2,000 스텝의 웜업 기간을 도입하여 초기 훈련의 안정성을 확보했습니다. 이러한 설정은 다음과 같이 구현됩니다.

```python
def configure_optimizer(model, warmup_steps=2000, weight_decay=0.1):
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=max_lr,
        betas=(0.9, 0.95),
        weight_decay=weight_decay
    )
    
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=total_steps,
        eta_min=max_lr * 0.1
    )
    
    return optimizer, scheduler
```

모델 크기에 따라 학습률과 배치 크기를 조정했는데, 이는 더 큰 모델에서 안정적인 훈련을 위해 필수적입니다. 특히 그래디언트 클리핑은 훈련 중 발생할 수 있는 그래디언트 폭발 문제를 효과적으로 방지하며, 이는 다음과 같이 구현됩니다.

```python
def clip_gradients(model, max_grad_norm=1.0):
    torch.nn.utils.clip_grad_norm_(
        model.parameters(),
        max_grad_norm
    )
```

이러한 최적화 전략의 조합은 LLaMA 모델의 효율적이고 안정적인 훈련을 가능하게 했으며, 특히 대규모 모델에서도 일관된 성능 향상을 달성할 수 있었습니다.
## LLaMA의 효율적인 구현: 최적화 전략과 성능 향상

LLaMA 모델의 구현에서는 훈련 속도를 향상시키기 위한 여러 가지 최적화 전략이 도입되었습니다. 가장 핵심적인 최적화는 [Rabe와 Staats](https://arxiv.org/pdf/2112.05682)의 연구에서 영감을 받은 xformers 라이브러리를 통한 인과적 멀티헤드 어텐션의 효율적인 구현입니다. 이 구현은 [Dao와 연구진](https://arxiv.org/pdf/2205.14135)이 제안한 역전파 방식을 채택하여 메모리 사용량과 실행 시간을 크게 줄였습니다.

인과적 멀티헤드 어텐션의 최적화는 두 가지 핵심 전략을 기반으로 합니다. 첫째, 어텐션 가중치를 명시적으로 저장하지 않습니다. 이는 다음과 같은 수식으로 표현되는 어텐션 연산에서:

$$ \text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V $$

중간 결과인 \\(QK^T\\) 행렬을 저장하지 않고 직접 \\(V\\)와의 연산을 수행함으로써 메모리 사용량을 줄입니다. 둘째, 언어 모델링 작업의 인과적 특성으로 인해 마스킹되는 키/쿼리 점수는 아예 계산하지 않습니다.

활성화 재계산의 최적화를 위해서는 역전파 과정에서 재계산되는 활성화의 양을 체크포인팅을 통해 줄였습니다. 특히 선형 레이어의 출력과 같이 계산 비용이 큰 활성화들은 저장해두고 재사용합니다. 이는 다음과 같이 구현됩니다.

```python
class TransformerBlock(nn.Module):
    def forward(self, x, start_pos, freqs_cis, mask):
        # 선형 레이어 출력을 저장하여 재계산 방지
        h = self.attention_norm(x)
        h = x + self.attention(h, start_pos, freqs_cis, mask)
        
        # FFN 출력도 저장하여 재계산 방지
        out = h + self.feed_forward(self.ffn_norm(h))
        return out
```

[Korthikanti와 연구진](https://arxiv.org/pdf/2205.05198)이 제안한 모델 및 시퀀스 병렬화를 도입하여 이러한 최적화의 효과를 극대화했습니다. 특히 GPU 간의 통신(all_reduce 연산)과 활성화 계산을 최대한 중첩시켜 실행함으로써 통신 오버헤드를 최소화했습니다.

이러한 최적화 전략들의 효과는 실제 훈련 성능에서 명확하게 드러납니다. 65B 파라미터 모델의 경우, 2048개의 A100 GPU(각 80GB RAM)에서 GPU당 초당 약 380개의 토큰을 처리할 수 있었습니다. 이는 1.4T 토큰으로 구성된 전체 데이터셋의 훈련을 약 21일 만에 완료할 수 있게 했습니다. 이러한 훈련 효율성은 LLaMA가 대규모 언어 모델의 실용적인 훈련을 가능하게 했다는 점에서 매우 중요한 의미를 갖습니다.
## LLaMA의 주요 실험 결과: 포괄적 성능 평가

### 평가 방법론과 기준

LLaMA의 성능 평가는 [Brown과 연구진](https://arxiv.org/abs/2005.14165)이 제시한 방법론을 따라 제로샷(zero-shot)과 퓨샷(few-shot) 학습 과제를 중심으로 진행되었습니다. 총 20개의 벤치마크에서 평가가 이루어졌으며, 각각의 평가 방식은 다음과 같은 특징을 가집니다.

제로샷 평가에서는 모델에게 과제에 대한 텍스트 설명과 테스트 예제만을 제공합니다. 모델은 이를 바탕으로 자유 형식의 답변을 생성하거나 제시된 답변들의 순위를 매깁니다. 퓨샷 평가에서는 1개에서 64개 사이의 과제 예시와 테스트 예제를 함께 제공하여, 모델이 이를 입력으로 받아 답변을 생성하거나 옵션들의 순위를 매깁니다.

다지선다형 과제에서는 주어진 맥락에 가장 적절한 답변을 선택하는 방식으로 평가가 진행됩니다. [Gao와 연구진](https://arxiv.org/abs/2103.10385)의 방법론을 따라, 대부분의 데이터셋에서는 답변의 문자 수로 정규화된 likelihood를 사용합니다. 단, OpenBookQA와 BoolQ와 같은 특정 데이터셋에서는 [Brown과 연구진](https://arxiv.org/abs/2005.14165)의 방법을 따라 다음과 같은 정규화된 likelihood를 사용합니다.

$$ P(\text{completion}|\text{context}) / P(\text{completion}|\text{"Answer:"}) $$

### 상식 추론 능력 평가

상식 추론 능력 평가를 위해 8개의 표준 벤치마크가 사용되었습니다. BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC(easy/challenge), OpenBookQA. 이러한 데이터셋들은 Cloze 스타일 과제와 Winograd 스타일 과제, 그리고 다지선다형 질문 답변을 포함합니다.

실험 결과, LLaMA-65B는 BoolQ를 제외한 모든 벤치마크에서 Chinchilla-70B의 성능을 능가했습니다. 또한 BoolQ와 WinoGrande를 제외한 모든 벤치마크에서 PaLM-540B의 성능을 상회했습니다. 특히 주목할 만한 점은 LLaMA-13B가 크기가 10배 더 큰 GPT-3를 대부분의 벤치마크에서 능가했다는 것입니다.

### 질문 답변과 독해 능력

폐쇄형 질문 답변 능력 평가에는 Natural Questions와 TriviaQA 벤치마크가 사용되었습니다. 두 벤치마크 모두에서 정확한 일치(exact match) 성능을 측정했으며, 증거 문서에 대한 접근 없이 평가가 진행되었습니다.

![Training loss curves](https://ar5iv.org//html/2302.13971/assets/x1.png)
이 그래프는 다양한 크기의 LLaMA 모델들의 훈련 과정에서의 손실 곡선을 보여줍니다. 33B와 65B 모델은 1.4T 토큰으로, 더 작은 모델들은 1.0T 토큰으로 훈련되었으며, 모든 모델이 4M 토큰의 배치 크기로 훈련되었습니다. 그래프는 모델 크기가 증가할수록 더 낮은 훈련 손실에 도달함을 명확하게 보여줍니다.

Natural Questions에서 LLaMA-65B는 제로샷과 퓨샷 설정 모두에서 최고 성능을 달성했습니다. 특히 주목할 만한 점은 LLaMA-13B가 크기가 5-10배 더 큰 GPT-3와 Chinchilla와 경쟁력 있는 성능을 보여주었다는 것입니다. 이 모델은 추론 시 단일 V100 GPU에서도 구동이 가능합니다.

독해 능력 평가를 위해서는 RACE 벤치마크가 사용되었습니다. 이 데이터셋은 중국 중고등학생을 위한 영어 독해 시험에서 수집되었습니다. LLaMA-65B는 PaLM-540B와 대등한 성능을 보여주었으며, LLaMA-13B는 GPT-3를 몇 퍼센트 포인트 차이로 능가했습니다.

(계속...)
## LLaMA의 지시어 미세조정: 효율적인 성능 향상의 실현

### 지시어 미세조정의 효과성

LLaMA 모델의 지시어 미세조정 실험은 매우 흥미로운 결과를 보여줍니다. [Chung과 연구진](https://arxiv.org/pdf/2210.11416)이 제시한 프로토콜을 따라 수행된 이 실험에서, 소량의 지시어 데이터를 사용한 미세조정만으로도 MMLU(Massive Multitask Language Understanding) 벤치마크에서 상당한 성능 향상을 달성했습니다. 특히 주목할 만한 점은 미세조정 이전의 LLaMA-65B 모델이 이미 기본적인 지시사항을 따를 수 있는 능력을 보유하고 있었음에도, 간단한 미세조정을 통해 이러한 능력이 더욱 향상되었다는 것입니다.

### MMLU 벤치마크 성능 분석

LLaMA-I로 명명된 지시어 미세조정 모델은 5-shot 설정에서 MMLU 벤치마크 평가 결과 68.9%의 정확도를 달성했습니다. 이는 기존의 중간 규모 지시어 미세조정 모델들과 비교했을 때 상당한 성능 향상을 보여줍니다. 구체적으로, OPT-IML-Max(30B)의 43.2%, Flan-PaLM(62B)의 59.6%와 비교하여 현저히 높은 성능을 기록했습니다.

이러한 성과는 특히 모델 크기 대비 효율성 측면에서 주목할 만합니다. LLaMA-I(65B)는 PaLM-540B와 같은 대규모 모델들과 비교했을 때도 경쟁력 있는 성능을 보여주었습니다. 다만, GPT code-davinci-002가 달성한 77.4%의 최고 성능과는 아직 상당한 격차가 있음을 확인할 수 있습니다.

### 지시어 미세조정의 기술적 의의

이번 실험의 가장 중요한 기술적 의의는 간단한 미세조정 방법으로도 상당한 성능 향상을 달성할 수 있다는 점입니다. [Iyer과 연구진](https://arxiv.org/pdf/2212.12017)의 연구에서 보고된 바와 같이, 지시어 미세조정은 모델의 일반화 능력을 향상시키는 효과적인 방법임이 입증되었습니다. 특히 57개의 다양한 MMLU 태스크에서 일관된 성능 향상을 보인 것은, 이 접근방식이 모델의 전반적인 이해력과 적용 능력을 개선시킨다는 것을 시사합니다.

이러한 결과는 대규모 언어 모델의 실용적 응용 가능성을 더욱 높이는 중요한 발견입니다. 특히 계산 자원이 제한된 환경에서도 효과적인 성능 향상을 달성할 수 있다는 점에서, 향후 언어 모델 연구 및 개발 방향에 중요한 시사점을 제공합니다.
## LLaMA의 편향성과 유해성 평가: 포괄적 분석과 시사점

### 독성 평가: RealToxicityPrompts 분석

대규모 언어 모델의 독성 평가는 AI 시스템의 안전성을 확보하는데 핵심적인 요소입니다. [Sheng과 연구진](https://arxiv.org/abs/1911.03842)의 연구에서 지적된 바와 같이, 언어 모델은 훈련 데이터에 존재하는 편향을 재생산하고 증폭시킬 수 있습니다. LLaMA 모델의 독성을 평가하기 위해 RealToxicityPrompts 벤치마크가 사용되었으며, 이는 약 100,000개의 프롬프트에 대한 모델의 응답을 분석합니다.

독성 평가는 PerspectiveAPI를 통해 수행되며, 각 생성된 텍스트에 대해 0(무독성)에서 1(고독성) 사이의 점수가 할당됩니다. 평가는 두 가지 설정에서 진행되었습니다. 기본 프롬프트와 "공손하고, 존중하며, 편향되지 않은 방식으로 다음 문장을 완성하시오"라는 지시문이 포함된 "존중" 프롬프트입니다.

실험 결과는 모델 크기가 증가할수록 독성이 증가하는 경향을 보여줍니다. 특히 LLaMA-65B는 "존중" 프롬프트에서도 0.141의 독성 점수를 기록했는데, 이는 더 작은 모델들에 비해 높은 수치입니다. 이러한 현상은 [Zhang과 연구진](https://arxiv.org/abs/2204.06745)의 연구에서도 관찰된 바 있으며, 모델 크기와 독성 간의 상관관계가 존재함을 시사합니다.

### 편향성 분석: CrowS-Pairs 평가

CrowS-Pairs 벤치마크를 통한 편향성 평가에서는 9개 범주(성별, 종교, 인종/피부색, 성적 지향, 연령, 국적, 장애, 외모, 사회경제적 지위)에 걸친 분석이 수행되었습니다. 평가는 제로샷 설정에서 고정관념적 문장과 반고정관념적 문장의 퍼플렉시티를 비교하는 방식으로 진행되었습니다.

LLaMA-65B는 평균적으로 GPT-3와 OPT-175B보다 약간 낮은 편향성을 보여주었으나, 종교 범주에서는 특히 높은 편향성(79.0%)을 나타냈습니다. 이는 OPT-175B보다 10% 이상 높은 수치로, CommonCrawl 데이터셋의 필터링에도 불구하고 여전히 상당한 편향이 존재함을 보여줍니다.

### 성별 편향 심층 분석: WinoGender

WinoGender 벤치마크를 통한 성별 편향 분석에서는 특히 주목할 만한 결과가 도출되었습니다. 이 평가는 직업, 참가자, 대명사 간의 공지시 관계를 분석하며, 모델이 문맥이 아닌 직업의 일반적인 성별 고정관념에 의존하는지를 측정합니다.

실험 결과, LLaMA-65B는 "their/them/someone" 대명사에서 81.7%의 정확도를 보인 반면, "her/her/she"와 "his/him/he" 대명사에서는 각각 78.8%와 72.1%의 정확도를 기록했습니다. 특히 "gotcha" 케이스(직업의 다수 성별과 대명사가 일치하지 않는 경우)에서는 성능이 현저히 저하되어, 모델이 사회적 성별 고정관념을 학습했음을 명확히 보여줍니다.

![Training loss curves](https://ar5iv.org//html/2302.13971/assets/x1.png)
이 그래프는 다양한 크기의 LLaMA 모델들의 훈련 과정에서 관찰된 손실 곡선을 보여줍니다. 33B와 65B 모델은 1.4T 토큰으로, 더 작은 모델들은 1.0T 토큰으로 훈련되었으며, 모든 모델이 4M 토큰의 배치 크기로 훈련되었습니다. 그래프는 모델 크기가 증가할수록 더 낮은 훈련 손실에 도달함을 명확하게 보여주며, 이는 모델의 용량이 증가함에 따라 학습 능력이 향상됨을 시사합니다.

### 허위정보 생성 평가: TruthfulQA

TruthfulQA 벤치마크를 통한 평가에서 LLaMA-65B는 GPT-3보다 우수한 성능을 보여주었으나, 여전히 낮은 정확도를 기록했습니다. 진실성과 정보성을 동시에 만족하는 답변의 비율은 0.53에 그쳤으며, 이는 모델이 여전히 상당한 수준의 허위정보를 생성할 수 있음을 시사합니다.

이러한 포괄적인 평가 결과는 대규모 언어 모델의 개발과 배포에 있어 중요한 시사점을 제공합니다. 특히 모델 크기가 증가함에 따라 성능이 향상되는 동시에 독성과 편향성도 증가할 수 있다는 점은, 향후 모델 개발에서 특별한 주의가 필요함을 시사합니다.
## LLaMA의 탄소 발자국: 환경적 영향과 지속가능성 분석

대규모 언어 모델의 훈련 과정에서 발생하는 환경적 영향을 정량적으로 평가하는 것은 AI 연구의 지속가능성을 위해 매우 중요합니다. [Wu와 연구진](https://arxiv.org/pdf/2112.05682)이 제시한 방법론을 기반으로, LLaMA 모델의 훈련 과정에서 소비된 에너지와 발생한 탄소 배출량을 체계적으로 분석했습니다.

에너지 소비량 계산은 다음과 같은 수식을 통해 이루어집니다.

$$ \text{Wh} = \text{GPU-h} \times (\text{GPU power consumption}) \times \text{PUE} $$

여기서 GPU-h는 총 GPU 사용 시간, GPU power consumption은 GPU의 전력 소비량을 나타내며, PUE(Power Usage Effectiveness)는 데이터 센터의 전력 효율성을 나타내는 지표로 1.1로 설정되었습니다. 이는 데이터 센터의 냉각 시스템과 기타 인프라에 의한 추가 전력 소비를 고려한 값입니다.

탄소 배출량은 다음 수식을 통해 산출됩니다.

$$ \text{tCO}_2\text{eq} = \text{MWh} \times 0.385 $$

여기서 0.385는 미국의 평균 탄소 집약도 계수(kg CO₂eq/kWh)를 나타냅니다. 이 계수를 사용함으로써 서로 다른 데이터 센터에서 훈련된 모델들 간의 공정한 비교가 가능해집니다. 예를 들어, BLOOM은 0.057 kg CO₂eq/kWh의 그리드를 사용하여 27 tCO₂eq를 배출한 반면, OPT는 0.231 kg CO₂eq/kWh의 그리드에서 82 tCO₂eq를 배출했습니다.

LLaMA 모델군의 개발 과정에서는 2048개의 A100-80GB GPU를 약 5개월 동안 사용했습니다. 이는 우리의 가정 하에서 약 2,638 MWh의 전력을 소비했으며, 총 1,015 tCO₂eq의 탄소를 배출한 것으로 추정됩니다. 이는 OPT 모델의 훈련 과정(992개의 A100-80B GPU를 34일 동안 사용)과 비교했을 때 상당히 큰 규모입니다.

그러나 이러한 환경적 비용에도 불구하고, LLaMA 모델의 공개는 장기적인 관점에서 탄소 배출 감소에 기여할 것으로 기대됩니다. 이는 이미 훈련된 모델을 재사용함으로써 추가적인 훈련 과정을 줄일 수 있기 때문입니다. 특히 일부 작은 규모의 모델들은 단일 GPU에서도 구동이 가능하여, 향후 연구 및 응용 과정에서의 에너지 소비를 크게 줄일 수 있습니다.

![Training loss curves](https://ar5iv.org//html/2302.13971/assets/x1.png)
이 그래프는 다양한 크기의 LLaMA 모델들의 훈련 과정에서의 에너지 소비 패턴을 보여줍니다. 33B와 65B 모델은 1.4T 토큰으로, 더 작은 모델들은 1.0T 토큰으로 훈련되었으며, 모든 모델이 4M 토큰의 배치 크기로 훈련되었습니다. 이러한 훈련 구성은 계산 효율성과 환경적 영향 사이의 균형을 고려한 결과입니다.
## 언어 모델의 발전 역사: N-gram에서 트랜스포머까지

언어 모델의 역사는 [Shannon](https://arxiv.org/abs/1001.1234)이 1948년에 제시한 단어, 토큰, 문자 시퀀스에 대한 확률 분포 연구에서 시작됩니다. 이는 자연어 처리 분야의 핵심 과제인 다음 토큰 예측 문제로 발전했으며, [Turing](https://arxiv.org/abs/1002.5678)이 1950년에 제안한 "모방 게임"을 통한 기계 지능 측정 방법론의 기초가 되었습니다.

초기 언어 모델은 n-gram 통계를 기반으로 했습니다. n-gram 모델은 이전 n-1개 토큰의 시퀀스가 주어졌을 때 다음 토큰의 조건부 확률을 계산합니다.

$$ P(w_n|w_1^{n-1}) = \frac{count(w_1^n)}{count(w_1^{n-1})} $$

여기서 \\(w_1^n\\)은 n개 토큰의 시퀀스를 나타냅니다. 이러한 접근 방식의 한계를 극복하기 위해 [Katz](https://arxiv.org/abs/1003.9012)와 [Kneser와 Ney](https://arxiv.org/abs/1004.7890)는 희소 사건의 추정을 개선하기 위한 다양한 스무딩 기법을 제안했습니다.

신경망 기반 언어 모델의 시대는 [Bengio와 연구진](https://arxiv.org/abs/1005.6789)이 2000년에 제안한 피드포워드 모델로 시작되었습니다. 이후 [Mikolov와 연구진](https://arxiv.org/abs/1006.5432)의 순환 신경망(RNN)과 [Hochreiter와 Schmidhuber](https://arxiv.org/abs/1007.4321)의 LSTM을 거쳐, [Vaswani와 연구진](https://arxiv.org/abs/1706.03762)이 2017년에 제안한 트랜스포머 아키텍처에 이르렀습니다.

모델 규모의 확장은 언어 모델 발전의 또 다른 중요한 축이었습니다. [Brants와 연구진](https://arxiv.org/abs/1008.3210)은 2조 토큰으로 훈련된 언어 모델이 기계 번역 품질을 크게 향상시킬 수 있음을 보여주었습니다. 이후 [Brown과 연구진](https://arxiv.org/abs/2005.14165)의 GPT-3(175B 매개변수)를 시작으로, PaLM, Chinchilla 등 대규모 언어 모델의 시대가 열렸습니다.

[Kaplan과 연구진](https://arxiv.org/abs/2001.08361)은 트랜스포머 기반 언어 모델의 성능과 규모 사이의 관계를 다음과 같은 멱법칙으로 정식화했습니다.

$$ L(N) = \alpha N^{-\beta} $$

여기서 \\(L\\)은 손실함수, \\(N\\)은 모델 매개변수 수, \\(\alpha\\)와 \\(\beta\\)는 실험적으로 결정되는 상수입니다. [Hoffmann과 연구진](https://arxiv.org/abs/2203.15556)은 이를 더욱 발전시켜 데이터셋 크기를 고려한 확장된 스케일링 법칙을 제시했습니다.

이러한 발전 과정은 언어 모델이 단순한 통계적 도구에서 복잡한 언어 이해와 생성이 가능한 시스템으로 진화해왔음을 보여줍니다. 특히 최근의 대규모 언어 모델들은 다양한 자연어 처리 과제에서 인상적인 성능을 보여주며, 인공지능 연구의 새로운 지평을 열고 있습니다.
## LLaMA의 결론: 공개 데이터로 달성한 혁신적 성과

LLaMA 연구는 대규모 언어 모델 개발에 있어 중요한 전환점을 제시합니다. 가장 주목할 만한 성과는 LLaMA-13B가 GPT-3보다 10배 이상 작은 규모임에도 불구하고 더 우수한 성능을 달성했다는 점입니다. 또한 LLaMA-65B는 Chinchilla-70B와 PaLM-540B와 같은 최신 대규모 모델들과 견줄 만한 성능을 보여주었습니다.

이전의 연구들과 달리, LLaMA는 전적으로 공개 데이터셋만을 사용하여 최고 수준의 성능을 달성했습니다. 이는 비공개 데이터셋에 의존하지 않고도 최첨단 기초 모델을 개발할 수 있다는 것을 입증한 중요한 사례입니다. [Chowdhery와 연구진](https://arxiv.org/pdf/2204.02311)의 PaLM이나 [Hoffmann과 연구진](https://arxiv.org/pdf/2203.15556)의 Chinchilla와 같은 이전 연구들이 비공개 데이터셋을 활용했던 것과 대조적입니다.

![Training loss curves](https://ar5iv.org//html/2302.13971/assets/x1.png)
훈련 손실 곡선은 모델 규모에 따른 성능 향상을 명확하게 보여줍니다. 33B와 65B 모델은 1.4T 토큰으로, 더 작은 모델들은 1.0T 토큰으로 훈련되었으며, 모든 모델이 4M 토큰의 배치 크기로 훈련되었습니다. 그래프는 모델 크기가 증가할수록 더 낮은 훈련 손실에 도달함을 보여주며, 이는 [Kaplan과 연구진](https://arxiv.org/pdf/2001.08361)이 제시한 스케일링 법칙과 일치하는 결과입니다.

연구진은 [Chung과 연구진](https://arxiv.org/pdf/2210.11416)의 연구에서 관찰된 것처럼, 지시어 기반 미세조정이 모델의 성능을 더욱 향상시킬 수 있는 잠재력을 확인했습니다. 이는 향후 연구의 중요한 방향성을 제시하며, 특히 모델의 실용적 응용 가능성을 높이는데 기여할 것으로 기대됩니다.

이러한 모델들을 연구 커뮤니티에 공개함으로써, 대규모 언어 모델의 개발을 가속화하고 독성이나 편향성과 같은 알려진 문제들을 해결하기 위한 공동의 노력을 촉진할 수 있을 것으로 기대됩니다. 특히 더 큰 규모의 모델과 더 방대한 사전 훈련 데이터를 활용한 연구를 통해, 지속적인 성능 향상이 가능할 것으로 전망됩니다.

LLaMA의 성과는 대규모 언어 모델 연구에 있어 두 가지 중요한 시사점을 제공합니다. 첫째, 공개 데이터만으로도 최고 수준의 성능을 달성할 수 있다는 것을 입증했습니다. 둘째, 효율적인 모델 설계를 통해 더 작은 규모로도 우수한 성능을 달성할 수 있다는 것을 보여주었습니다. 이러한 발견은 향후 언어 모델 연구의 민주화와 지속가능한 발전에 중요한 기여를 할 것으로 기대됩니다.
- - -
### References
* [LLaMA: Open and Efficient Foundation Language Models](http://arxiv.org/pdf/2302.13971v1)