---
layout: post
title: "LLaMA: Open and Efficient Foundation Language Models"
date: 2023-02-27 17:11:15
author: "Meta AI"
categories: "Foundation-Models"
tags: ["Open-and-Efficient-Foundation-Language-Models", "Rotary-Positional-Embeddings", "SwiGLU-Activation-Function", "Scaling-Laws-for-Large-Language-Models", "Responsible-AI-Evaluation"]
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
Meta AI 연구진은 대규모 언어 모델의 발전이 주로 독점적이거나 접근이 제한된 데이터셋에 의존하고 있다는 문제점을 인식했습니다. 이는 학술 연구의 재현성과 투명성을 저해하고, 언어 모델 연구의 민주화를 방해하는 요인이 되어왔습니다. 이러한 배경에서 연구진은 공개적으로 이용 가능한 데이터셋만을 활용하여 최신 성능의 언어 모델을 개발하고자 했습니다. 특히 모델의 규모를 효율적으로 축소하면서도 우수한 성능을 달성하는 것을 목표로 했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
LLaMA는 효율적인 학습 방법과 양질의 데이터 선별을 통해 기존 모델들보다 훨씬 작은 규모로도 동등하거나 더 나은 성능을 달성하는 접근 방식을 제시했습니다. 특히 Hoffmann의 스케일링 법칙을 기반으로 하되, 추론 비용까지 고려한 최적화된 모델 설계를 채택했습니다. 연구진은 시퀀스 병렬화와 선택적 활성화 재계산이라는 혁신적인 기법을 도입하여 메모리 효율성을 크게 향상시켰으며, FlashAttention과 같은 최적화된 어텐션 구현을 통해 계산 효율성도 개선했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
LLaMA는 7B에서 65B 매개변수에 이르는 다양한 규모의 모델로 구현되었습니다. 학습 데이터는 CommonCrawl, Wikipedia, GitHub 등 공개적으로 이용 가능한 데이터셋을 엄격한 품질 기준으로 선별하여 총 1.4조 개의 토큰으로 구성했습니다. 모델 구조는 트랜스포머 아키텍처를 기반으로 하되, RMSNorm을 통한 Pre-normalization, SwiGLU 활성화 함수, 회전 위치 임베딩(RoPE) 등의 최신 개선사항들을 적용했습니다. AdamW 옵티마이저와 코사인 스케줄링을 통한 학습률 조정을 사용했으며, 그래디언트 클리핑과 가중치 감쇠를 통해 학습 안정성을 확보했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
이 연구는 대규모 언어 모델 개발에 있어 중요한 패러다임 전환을 제시합니다. 특히 13B 매개변수 모델이 175B 매개변수의 GPT-3를 능가하는 성능을 보여준 것은, 모델의 규모보다 효율적인 설계와 양질의 데이터가 더 중요할 수 있다는 것을 입증합니다. 또한 공개 데이터만으로도 최고 수준의 성능을 달성할 수 있다는 것을 보여줌으로써, 언어 모델 연구의 민주화에 큰 기여를 했습니다. 다만 편향성, 유해성, 허위정보 생성 등의 문제는 여전히 과제로 남아있어, 이에 대한 지속적인 연구와 개선이 필요함을 시사합니다.
- - -
## LLaMA: 개방형 고효율 기초 언어 모델

### 서론

Meta AI의 연구진은 7B에서 65B 매개변수 규모를 가진 기초 언어 모델 시리즈인 LLaMA(Large Language Model Meta AI)를 소개했습니다. 이 연구의 가장 주목할 만한 특징은 독점적이거나 접근이 제한된 데이터셋을 사용하지 않고, 공개적으로 이용 가능한 데이터셋만을 활용하여 최신 성능의 모델을 학습시켰다는 점입니다.

특히 13B 매개변수를 가진 LLaMA-13B는 175B 매개변수의 GPT-3를 대부분의 벤치마크에서 능가하는 성능을 보여주었습니다. 더 나아가 LLaMA-65B는 Chinchilla-70B와 PaLM-540B와 같은 최고 수준의 모델들과 견줄 만한 성능을 달성했습니다. 이는 모델의 규모를 키우는 것보다 효율적인 학습 방법과 양질의 데이터를 활용하는 것이 더 중요할 수 있다는 것을 시사합니다.

Hoffmann과 연구진이 제시한 스케일링 법칙에 따르면, 주어진 컴퓨팅 예산 내에서 최고의 성능은 가장 큰 모델이 아닌 더 많은 데이터로 학습된 작은 모델에서 달성된다고 합니다. 그러나 이 법칙은 추론(inference) 비용을 고려하지 않았다는 한계가 있습니다. 실제 서비스 환경에서는 학습 속도보다 추론 속도가 더 중요한 요소가 될 수 있기 때문입니다.

연구진은 1조 개 이상의 토큰으로 모델을 학습시켰으며, 7B 매개변수 모델의 경우 1조 토큰 이후에도 성능이 계속 향상되는 것을 발견했습니다. 이는 기존의 연구들이 제안한 것보다 더 많은 데이터로 학습하는 것이 효과적일 수 있다는 점을 보여줍니다.

LLaMA의 개발은 대규모 언어 모델 연구의 민주화에 큰 기여를 할 것으로 기대됩니다. 특히 LLaMA-13B는 단일 GPU에서도 구동이 가능한 규모이면서도 GPT-3급의 성능을 보여주어, 더 많은 연구자들이 대규모 언어 모델을 연구하고 발전시킬 수 있는 기회를 제공할 것입니다.
### 접근 방법

LLaMA의 학습 방법은 Brown과 연구진이 GPT-3에서 제시한 방법과 Chowdhery와 연구진이 PaLM에서 사용한 방법을 기반으로 하며, Hoffmann과 연구진이 제안한 Chinchilla 스케일링 법칙에서 영감을 받았습니다. 이는 대규모 텍스트 데이터를 사용하여 표준 최적화 기법으로 트랜스포머 모델을 학습시키는 방식입니다.

Chinchilla 스케일링 법칙은 컴퓨팅 자원이 제한된 상황에서 언어 모델의 성능을 최적화하기 위한 이론적 기반을 제공합니다. 이 법칙에 따르면, 모델의 크기(\\(N\\))와 학습 데이터의 크기(\\(D\\))는 다음과 같은 관계를 가집니다.

$$ L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta} $$

여기서 \\(L\\)은 최종 사전 학습 손실, \\(E\\)는 달성 가능한 최소 손실, \\(A\\)와 \\(B\\)는 스케일링 상수, \\(\alpha\\)와 \\(\beta\\)는 스케일링 지수를 나타냅니다. 이 법칙은 주어진 컴퓨팅 예산 내에서 모델 크기와 학습 데이터 크기 간의 최적 균형점을 찾는 데 도움을 줍니다.

실제 구현에서는 다음과 같은 Python 코드로 트랜스포머 모델의 기본 구조를 정의합니다.

```python
class Transformer(nn.Module):
    def __init__(self, params: ModelArgs):
        super().__init__()
        self.params = params
        self.vocab_size = params.vocab_size
        self.n_layers = params.n_layers
        
        self.tok_embeddings = ParallelEmbedding(
            params.vocab_size, params.dim, init_method=lambda x: x
        )
        
        self.layers = torch.nn.ModuleList()
        for layer_id in range(params.n_layers):
            self.layers.append(TransformerBlock(layer_id, params))
            
        self.norm = RMSNorm(params.dim, eps=params.norm_eps)
        self.output = ColumnParallelLinear(
            params.dim, params.vocab_size, bias=False, init_method=lambda x: x
        )
```

이 구현에서는 모델 병렬화를 위해 FairScale 라이브러리의 `ParallelEmbedding`과 `ColumnParallelLinear`를 사용하여 대규모 모델의 효율적인 학습을 가능하게 합니다. 또한 RMSNorm을 사용하여 학습 안정성을 향상시키고, 계층적 구조를 통해 깊은 신경망의 효과적인 학습을 지원합니다.LLaMA의 학습 과정에서는 모델의 최적화를 위해 AdamW 옵티마이저를 사용하며, 학습률(learning rate)은 코사인 스케줄링을 통해 조절됩니다. 이는 학습 초기에는 큰 학습률로 빠른 학습을 진행하고, 후반부로 갈수록 점진적으로 학습률을 감소시켜 안정적인 수렴을 도모하는 방식입니다.

학습 과정의 안정성을 높이기 위해 그래디언트 클리핑(gradient clipping)을 적용하며, 이는 그래디언트의 크기가 특정 임계값을 넘지 않도록 제한하는 기법입니다. 수식으로 표현하면 다음과 같습니다.

$$ g_t = \text{clip}(\nabla L_t, -\tau, \tau) $$

여기서 \\(\tau\\)는 클리핑 임계값이며, \\(\nabla L_t\\)는 시간 \\(t\\)에서의 손실 함수의 그래디언트입니다.

또한, 가중치 감쇠(weight decay)를 통한 정규화를 적용하여 과적합을 방지합니다. 이는 다음과 같은 수정된 손실 함수를 사용하는 것과 동일합니다.

$$ L_{\text{total}} = L_{\text{original}} + \lambda \sum_{i} w_i^2 $$

여기서 \\(\lambda\\)는 가중치 감쇠 계수이고, \\(w_i\\)는 모델의 가중치들입니다.

학습 데이터의 처리에 있어서는 데이터 오염(data contamination)을 방지하기 위한 체계적인 도구들을 개발하여 사용했습니다. 이는 학습 데이터가 벤치마크 테스트 세트와 중복되는 것을 최소화하고, 실제 모델의 일반화 능력을 정확하게 평가할 수 있도록 합니다.

모델의 학습 효율성을 높이기 위해 FlashAttention과 같은 최적화된 어텐션 구현을 사용합니다. 이는 메모리 접근을 최소화하고 계산 효율성을 향상시키는 방식으로, 다음과 같은 IO 복잡도를 가집니다.

$$ \text{HBM 접근 횟수} = \Theta(N^2d^2/M) $$

여기서 \\(N\\)은 시퀀스 길이, \\(d\\)는 헤드 차원, \\(M\\)은 온칩 SRAM의 크기입니다. 이는 기존 어텐션 구현의 \\(\Theta(Nd + N^2)\\)보다 훨씬 효율적입니다.### 접근 방법

LLaMA의 학습 과정에서는 시퀀스 병렬화(sequence parallelism)와 선택적 활성화 재계산(selective activation recomputation)이라는 두 가지 혁신적인 기법을 도입하여 대규모 트랜스포머 모델의 메모리 효율성을 크게 향상시켰습니다. 시퀀스 병렬화는 트랜스포머 레이어의 비텐서 병렬 영역(예: 레이어 정규화, 드롭아웃)을 시퀀스 차원에서 분할하여 활성화에 필요한 메모리를 줄입니다. 이는 다음과 같은 수식으로 표현될 수 있습니다.

$$ g(x_1, ..., x_n) = \text{concat}(g(x_1), ..., g(x_n)) $$

여기서 \\(g\\)는 비텐서 병렬 연산을, \\(x_i\\)는 분할된 입력 시퀀스를 나타냅니다.

선택적 활성화 재계산은 어텐션 연산과 같이 메모리 사용량이 많지만 계산 비용이 상대적으로 적은 부분을 식별하여, 이러한 활성화값들을 저장하는 대신 필요할 때 재계산하는 방식입니다. 이를 통해 GPT-3나 MT-NLG와 같은 대규모 모델에서 활성화 메모리를 약 70% 감소시킬 수 있으며, 계산 오버헤드는 약 2%에 불과합니다.

학습 과정의 효율성을 더욱 높이기 위해 모델은 혼합 정밀도 훈련(mixed precision training)을 채택했습니다. 이는 FP16(16비트 부동소수점)과 FP32(32비트 부동소수점)을 선택적으로 사용하여 메모리 사용량을 줄이고 계산 속도를 향상시키는 방법입니다. 특히 어텐션 계산에서는 다음과 같은 스케일링된 점곱 어텐션을 사용합니다.

$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

여기서 \\(d_k\\)는 어텐션 헤드의 차원을 나타내며, 스케일링 팩터 \\(\sqrt{d_k}\\)는 그래디언트의 안정성을 보장합니다.

이러한 최적화 기법들의 조합을 통해 LLaMA는 1조 개 이상의 토큰으로 효율적인 학습을 수행할 수 있었으며, 특히 7B 모델의 경우 이 규모의 데이터 이후에도 지속적인 성능 향상을 보여주었습니다. 이는 기존의 스케일링 법칙들이 제안한 것보다 더 많은 데이터로 학습하는 것이 효과적일 수 있다는 중요한 발견을 제시합니다.
### 사전 학습 데이터

LLaMA 모델의 학습에는 다양한 도메인의 공개 데이터셋이 사용되었습니다. 연구진은 기존 대규모 언어 모델들이 사용한 데이터 소스들 중에서 공개적으로 이용 가능하고 오픈 소스 라이선스와 호환되는 데이터만을 선별하여 활용했습니다. 전체 학습 데이터는 약 1.4조 개의 토큰으로 구성되어 있으며, 각 데이터 소스별 비중과 특성은 다음과 같습니다.

영어 CommonCrawl이 전체 데이터의 67%를 차지하며, 이는 2017년부터 2020년까지의 5개 덤프에서 추출되었습니다. CCNet 파이프라인을 통해 전처리된 이 데이터는 Wenzek와 연구진이 제안한 방식을 따라 처리되었습니다. 구체적으로는 문장 수준의 중복 제거(deduplication)가 수행되었고, fastText 선형 분류기를 사용하여 영어가 아닌 페이지들을 제거했습니다. 또한 n-gram 언어 모델을 통해 저품질 콘텐츠를 필터링했으며, 위키피디아에서 참조로 사용된 페이지와 무작위로 샘플링된 페이지를 구분하는 선형 모델을 학습시켜 참조 페이지로 분류되지 않은 콘텐츠는 제외했습니다.

C4 데이터셋은 전체의 15%를 차지하며, 이는 Raffel과 연구진이 공개한 데이터셋입니다. 초기 실험에서 다양한 전처리된 CommonCrawl 데이터셋을 사용하는 것이 성능 향상에 도움이 된다는 것을 발견했기 때문에 C4를 포함시켰습니다. C4의 전처리 과정도 중복 제거와 언어 식별 단계를 포함하지만, CCNet과의 주요 차이점은 품질 필터링 방식에 있습니다. C4는 문장부호의 존재나 웹페이지의 단어 및 문장 수와 같은 휴리스틱에 주로 의존합니다.

GitHub 데이터는 4.5%를 차지하며, Google BigQuery에서 제공하는 공개 데이터셋을 사용했습니다. Apache, BSD, MIT 라이선스로 배포된 프로젝트만을 선별했으며, 줄 길이나 영숫자 문자 비율 등의 휴리스틱을 기반으로 저품질 파일을 필터링했습니다. 또한 정규표현식을 사용하여 헤더와 같은 상용구(boilerplate)를 제거했고, 파일 수준에서 정확한 매칭을 통한 중복 제거를 수행했습니다.

위키피디아 데이터도 4.5%를 차지하며, 2022년 6월부터 8월까지의 덤프를 사용했습니다. 라틴 문자나 키릴 문자를 사용하는 20개 언어(bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk)의 데이터가 포함되었습니다. 하이퍼링크, 주석, 기타 서식 관련 상용구를 제거하는 전처리 과정을 거쳤습니다.Gutenberg 프로젝트와 Books3 데이터는 전체의 4.5%를 차지하며, 이는 두 가지 주요 도서 말뭉치를 포함합니다. Gutenberg 프로젝트는 저작권이 만료된 공공 도메인의 도서들을 제공하며, Books3는 ThePile이라는 대규모 언어 모델 학습용 공개 데이터셋의 일부입니다. 연구진은 도서 수준에서 중복 제거를 수행하여 90% 이상의 내용이 중복되는 도서들을 제거했습니다.

arXiv 데이터는 2.5%를 차지하며, 과학 논문의 LaTeX 파일들을 처리하여 포함시켰습니다. Lewkowycz와 연구진의 방법을 따라 첫 번째 섹션 이전의 모든 내용과 참고문헌을 제거했으며, .tex 파일에서 주석을 제거하고 사용자가 작성한 정의와 매크로를 인라인으로 확장하여 논문 간의 일관성을 높였습니다.

Stack Exchange 데이터는 2%를 차지하며, 컴퓨터 과학부터 화학까지 다양한 분야를 다루는 고품질 질문과 답변 웹사이트의 덤프를 포함합니다. 28개의 가장 큰 웹사이트의 데이터를 선택했으며, 텍스트에서 HTML 태그를 제거하고 점수(높은 것부터 낮은 순)에 따라 답변을 정렬했습니다.

토크나이저로는 SentencePiece 구현체를 사용하여 바이트 쌍 인코딩(BPE) 알고리즘을 적용했습니다. 특히 모든 숫자를 개별 자릿수로 분할하고, 알 수 없는 UTF-8 문자는 바이트 단위로 분해하는 방식을 채택했습니다. 이러한 토크나이저 설계는 다양한 언어와 특수 문자를 효과적으로 처리할 수 있게 해줍니다.

전체 학습 데이터셋은 토크나이즈 후 약 1.4조 개의 토큰으로 구성되었습니다. 대부분의 학습 데이터는 한 번만 사용되었지만, 위키피디아와 도서 도메인의 데이터는 약 2.4회와 2.2회의 에포크를 거쳤습니다. 이는 이들 데이터의 상대적으로 높은 품질과 중요성을 반영한 것입니다.

이러한 데이터 구성은 다음과 같은 수식으로 표현될 수 있습니다.

$$ T_{total} = \sum_{i=1}^{n} E_i \cdot S_i $$

여기서 \\(T_{total}\\)은 총 학습 토큰 수, \\(E_i\\)는 각 데이터 소스의 에포크 수, \\(S_i\\)는 각 데이터 소스의 크기입니다. 이는 전체 학습 과정에서 각 데이터 소스가 모델에 미치는 영향을 정량화합니다.
### 모델 아키텍처

LLaMA는 Vaswani와 연구진이 제안한 트랜스포머 아키텍처를 기반으로 하며, 이후 PaLM과 같은 최신 언어 모델들에서 도입된 다양한 개선사항들을 적용했습니다. 주요 아키텍처 변경사항은 다음과 같습니다.

먼저, 학습 안정성을 향상시키기 위해 Pre-normalization 기법을 도입했습니다. 이는 GPT-3에서 영감을 받은 것으로, 트랜스포머의 각 서브레이어 출력을 정규화하는 대신 입력을 정규화하는 방식입니다. 정규화 함수로는 Zhang과 Sennrich가 제안한 RMSNorm을 사용했습니다. RMSNorm은 다음과 같이 정의됩니다.

$$ \text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^n x_i^2 + \epsilon}} \cdot \gamma $$

여기서 \\(\epsilon\\)은 수치 안정성을 위한 작은 상수이며, \\(\gamma\\)는 학습 가능한 스케일 파라미터입니다.

활성화 함수의 경우, PaLM에서 영감을 받아 기존의 ReLU 대신 SwiGLU 활성화 함수를 사용했습니다. SwiGLU는 Shazeer가 제안한 것으로, 다음과 같이 정의됩니다.

$$ \text{SwiGLU}(x, W, V, b, c) = \text{Swish}_\beta(xW + b) \otimes (xV + c) $$

여기서 \\(\text{Swish}_\beta(x) = x \cdot \sigma(\beta x)\\)이며, \\(\sigma\\)는 시그모이드 함수입니다. PaLM에서는 은닉층 차원을 \\(4d\\)로 설정했지만, LLaMA에서는 \\(\frac{2}{3}4d\\)로 축소했습니다.

위치 정보 인코딩의 경우, GPTNeo의 접근 방식을 따라 절대 위치 임베딩을 제거하고 대신 Su와 연구진이 제안한 회전 위치 임베딩(RoPE)을 각 레이어에 추가했습니다. RoPE는 복소수 회전을 사용하여 상대적 위치 정보를 인코딩하며, 다음과 같이 정의됩니다.

$$ \text{RoPE}(x_m, \theta) = xe^{im\theta} $$

여기서 \\(m\\)은 토큰의 위치이고, \\(\theta\\)는 회전 각도를 결정하는 파라미터입니다.

아래 그래프는 7B, 13B, 33B, 65B 파라미터를 가진 네 가지 LLaMA 모델의 학습 손실을 보여줍니다. 33B와 65B 모델은 1.4조 토큰으로 학습되었고, 더 작은 모델들은 1조 토큰으로 학습되었습니다. 모든 모델은 4M 토큰의 배치 크기로 학습되었습니다. 그래프를 통해 더 큰 모델들이 더 많은 데이터로 학습될 때 더 낮은 학습 손실을 달성하는 것을 확인할 수 있습니다.

![Training Loss](https://ar5iv.org//html/2302.13971/assets/x1.png)
### 옵티마이저

LLaMA 모델의 학습에는 AdamW 옵티마이저가 사용되었습니다. AdamW는 Loshchilov와 Hutter가 제안한 Adam 옵티마이저의 변형으로, 가중치 감쇠(weight decay)를 그래디언트 업데이트와 분리하여 적용하는 것이 특징입니다. 이 옵티마이저의 주요 하이퍼파라미터는 다음과 같이 설정되었습니다.

$$ \beta_1 = 0.9, \beta_2 = 0.95 $$

여기서 \\(\beta_1\\)은 그래디언트의 1차 모멘트(평균)에 대한 지수 감소율을, \\(\beta_2\\)는 2차 모멘트(분산)에 대한 지수 감소율을 나타냅니다. 이러한 설정은 학습 과정에서 그래디언트의 변동성을 효과적으로 제어하면서도 빠른 수렴을 가능하게 합니다.

학습률(learning rate)은 코사인 스케줄링을 통해 조절되며, 최종 학습률은 최대 학습률의 10%가 되도록 설정되었습니다. 이러한 학습률 조절 방식은 다음과 같은 수식으로 표현됩니다.

$$ \eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{t\pi}{T})) $$

여기서 \\(\eta_t\\)는 시간 \\(t\\)에서의 학습률, \\(\eta_{max}\\)는 초기 최대 학습률, \\(\eta_{min}\\)은 최종 학습률(\\(\eta_{max}\\)의 10%), \\(T\\)는 총 학습 스텝 수를 나타냅니다.

모델의 안정적인 학습을 위해 가중치 감쇠는 0.1로 설정되었으며, 그래디언트 클리핑은 1.0으로 설정되었습니다. 그래디언트 클리핑은 그래디언트 폭주(gradient explosion)를 방지하기 위한 기법으로, 그래디언트의 L2 노름이 1.0을 초과할 경우 이를 1.0으로 스케일링합니다.

$$ g_t = \min(1.0, \frac{1.0}{\|g_t\|_2})g_t $$

학습 초기에는 2,000 스텝의 웜업(warmup) 기간을 두어 학습률을 점진적으로 증가시켰습니다. 이는 초기 학습 과정의 안정성을 높이는 데 도움이 됩니다. 구체적인 학습률과 배치 크기는 모델의 크기에 따라 다르게 설정되었으며, 이는 모델의 규모에 따른 최적의 학습 조건을 반영한 것입니다.
### 효율적인 구현

LLaMA 모델의 학습 속도를 향상시키기 위해 연구진은 여러 가지 최적화 기법을 도입했습니다. 가장 핵심적인 최적화는 인과적 다중 헤드 어텐션(causal multi-head attention)의 효율적인 구현입니다. 이는 xformers 라이브러리를 통해 구현되었으며, Rabe와 연구진이 제안한 방식에 Dao와 연구진의 역전파 방식을 결합한 것입니다.

이 최적화된 어텐션 구현의 핵심은 어텐션 가중치를 저장하지 않고, 인과적 언어 모델링 작업의 특성상 마스킹되는 키/쿼리 점수를 계산하지 않는다는 점입니다. 수학적으로 표현하면, 기존 어텐션의 메모리 복잡도가 \\(O(n^2)\\)였던 것에 비해, 최적화된 구현에서는 상수 메모리만을 사용합니다.

$$ \text{Memory}(n) = O(1) $$

여기서 \\(n\\)은 시퀀스 길이입니다.

또한 역전파 과정에서 재계산되는 활성화(activation)의 양을 줄이기 위해 체크포인팅(checkpointing) 기법을 도입했습니다. 특히 선형 레이어의 출력과 같이 계산 비용이 큰 활성화값들을 저장하도록 했습니다. 이를 위해 PyTorch의 자동 미분(autograd) 대신 트랜스포머 레이어의 역전파 함수를 직접 구현했습니다.

```python
class TransformerBlock(nn.Module):
    def backward_pass(self, grad_output):
        # 저장된 활성화값을 사용하여 효율적인 역전파 수행
        saved_activations = self.saved_activations
        grad_input = torch.zeros_like(grad_output)
        
        # 선형 레이어의 출력값은 저장되어 있으므로 재계산하지 않음
        grad_input = self.attention.backward(grad_output, saved_activations)
        return grad_input
```

이러한 최적화의 효과를 최대한 활용하기 위해 Korthikanti와 연구진이 제안한 모델 병렬화와 시퀀스 병렬화를 적용했습니다. 또한 GPU 간의 네트워크 통신(all_reduce 연산)과 활성화값 계산을 최대한 중첩시켜 처리했습니다.

이러한 최적화 기법들의 결과로, 65B 파라미터 모델을 학습할 때 80GB RAM을 탑재한 2048개의 A100 GPU에서 GPU당 초당 약 380개의 토큰을 처리할 수 있었습니다. 이는 1.4조 개의 토큰으로 구성된 전체 데이터셋을 약 21일 만에 학습할 수 있다는 것을 의미합니다.
### 주요 실험 결과

LLaMA 모델의 성능을 평가하기 위해 연구진은 제로샷(zero-shot)과 퓨샷(few-shot) 학습 방식을 적용하여 총 20개의 벤치마크에서 실험을 진행했습니다. 제로샷 평가에서는 모델에게 과제에 대한 텍스트 설명과 테스트 예제만을 제공하고, 모델이 자유 형식의 생성이나 제안된 답변의 순위를 매기는 방식으로 진행되었습니다. 퓨샷 평가의 경우, 1개에서 64개 사이의 예제를 함께 제공하여 모델이 이를 바탕으로 답변을 생성하거나 순위를 매기도록 했습니다.

연구진은 LLaMA를 GPT-3, Gopher, Chinchilla, PaLM과 같은 비공개 대규모 언어 모델들과 비교했으며, OPT, GPT-J, GPT-Neo와 같은 오픈소스 모델들과도 성능을 비교했습니다. 또한 OPT-IML과 Flan-PaLM과 같은 명령어 튜닝(instruction-tuned) 모델들과도 간단한 비교를 수행했습니다.

평가는 자유 형식 생성 과제와 객관식 과제로 나누어 진행되었습니다. 객관식 과제에서는 주어진 문맥을 바탕으로 가장 적절한 답변을 선택하는 것이 목표였습니다. 대부분의 데이터셋에서는 각 답변의 문자 수로 정규화된 확률값을 기준으로 답변을 선택했습니다. 다만 OpenBookQA와 BoolQ와 같은 일부 데이터셋에서는 Brown과 연구진이 제안한 방식을 따라, "Answer:"를 문맥으로 주었을 때의 확률값으로 정규화하여 답변을 선택했습니다. 이는 다음과 같은 수식으로 표현됩니다.

$$ P(\text{completion}|\text{context}) / P(\text{completion}|\text{"Answer:"}) $$

실험 결과를 살펴보면, Natural Questions 데이터셋에서 LLaMA-65B는 제로샷과 퓨샷 설정 모두에서 최고 수준의 성능을 달성했습니다. 특히 주목할 만한 점은 LLaMA-13B가 GPT-3와 Chinchilla에 비해 5-10배 작은 크기임에도 불구하고 이들과 경쟁력 있는 성능을 보여주었다는 것입니다. 이는 단일 V100 GPU에서도 추론이 가능한 규모라는 점에서 매우 의미있는 결과입니다.

공통 상식 추론 분야에서는 BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC easy/challenge, OpenBookQA와 같은 8개의 표준 벤치마크를 사용했습니다. 이러한 데이터셋들은 Cloze 테스트와 Winograd 스타일의 과제, 그리고 객관식 문제 풀이를 포함합니다. LLaMA-65B는 BoolQ를 제외한 모든 벤치마크에서 Chinchilla-70B의 성능을 능가했으며, BoolQ와 WinoGrande를 제외한 모든 분야에서 PaLM-540B보다 우수한 성능을 보여주었습니다.LLaMA 모델의 성능 평가는 폐쇄형 질의응답 분야에서도 주목할 만한 결과를 보여주었습니다. Natural Questions와 TriviaQA 데이터셋에서 진행된 평가에서는 문서 접근 없이 순수하게 모델의 지식만을 활용하는 'closed-book' 방식으로 진행되었습니다. TriviaQA에서 LLaMA-65B는 제로샷 설정에서 68.2%, 1-shot에서 71.6%, 5-shot에서 72.6%, 64-shot에서 73.0%의 정확도를 달성하며, 모든 설정에서 최고 수준의 성능을 보여주었습니다.

독해력 평가를 위해서는 RACE 벤치마크를 활용했는데, 이는 중국 중고등학생들을 위한 영어 독해 시험에서 수집된 데이터셋입니다. RACE-middle에서 LLaMA-65B는 67.9%, RACE-high에서는 51.6%의 정확도를 달성하며 PaLM-540B와 대등한 성능을 보여주었습니다. 특히 LLaMA-13B는 GPT-3보다 몇 퍼센트 포인트 높은 성능을 기록했습니다.

수학적 추론 능력 평가에서는 MATH와 GSM8k 두 가지 벤치마크를 사용했습니다. MATH는 LaTeX로 작성된 12,000개의 중고등학교 수학 문제로 구성되어 있으며, GSM8k는 중학교 수준의 수학 문제들로 이루어져 있습니다. 특히 주목할 만한 점은 LLaMA-65B가 수학 데이터로 추가 학습을 하지 않았음에도 불구하고, 수학 데이터로 미세조정된 Minerva-62B의 성능을 뛰어넘었다는 것입니다. maj@k 평가 방식을 적용했을 때, GSM8k에서 LLaMA-65B는 69.7%의 정확도를 달성했습니다.

코드 생성 능력 평가에서는 HumanEval과 MBPP 벤치마크를 사용했습니다. 이 과제들에서 모델은 자연어로 된 프로그램 설명과 입출력 예제를 받아 Python 프로그램을 생성해야 합니다. HumanEval의 경우 함수 시그니처도 함께 제공되며, 설명과 테스트는 docstring 형태로 포함됩니다. LLaMA는 코드 데이터에 대한 특별한 미세조정 없이도 LaMDA와 PaLM과 같은 일반 모델들보다 우수한 성능을 보여주었습니다. 특히 LLaMA-65B는 PaLM-62B보다 더 나은 성능을 달성했으며, 이는 더 오래 학습된 버전과 비교해도 마찬가지였습니다.

다양한 지식 분야에 대한 이해도를 평가하기 위해 MMLU(Massive Multitask Language Understanding) 벤치마크를 사용했습니다. 이 벤치마크는 인문학, STEM, 사회과학 등 다양한 영역의 객관식 문제들로 구성되어 있습니다. 5-shot 설정에서 LLaMA-65B는 평균 63.4%의 정확도를 달성했는데, 이는 Chinchilla-70B와 PaLM-540B보다는 다소 낮은 수준입니다. 연구진은 이러한 차이가 학습 데이터에서 책과 학술 논문의 비중이 상대적으로 적었기 때문일 수 있다고 분석했습니다.### 주요 실험 결과 (계속)

학습 과정에서 모델의 성능 변화를 추적하기 위해 연구진은 여러 질의응답과 상식 추론 벤치마크에서의 성능을 지속적으로 모니터링했습니다. 대부분의 벤치마크에서 모델의 성능은 학습이 진행됨에 따라 꾸준히 향상되었으며, 이는 모델의 학습 과정에서 관찰된 퍼플렉시티(perplexity) 감소와도 높은 상관관계를 보였습니다.

그러나 SIQA와 WinoGrande 데이터셋에서는 다소 다른 패턴이 관찰되었습니다. 특히 SIQA의 경우, 성능의 변동성이 매우 크게 나타났는데, 이는 해당 벤치마크의 신뢰성에 대한 의문을 제기하게 만드는 결과입니다. WinoGrande에서는 학습 퍼플렉시티와 성능 간의 상관관계가 상대적으로 약했으며, LLaMA-33B와 LLaMA-65B가 학습 과정에서 유사한 수준의 성능을 보여주었습니다.

명령어 튜닝(instruction finetuning)의 효과를 검증하기 위해 연구진은 LLaMA-65B를 기반으로 LLaMA-I라는 새로운 모델을 개발했습니다. 이 모델은 Chung과 연구진이 제안한 프로토콜을 따라 매우 적은 양의 명령어 데이터로 미세조정되었습니다. MMLU 벤치마크에서 LLaMA-I는 68.9%의 정확도를 달성하며, OPT-IML과 Flan-PaLM과 같은 기존의 명령어 튜닝 모델들의 성능을 크게 향상시켰습니다.

![성능 변화 추이](https://ar5iv.org//html/2302.13971/assets/x2.png)

위 그래프는 LLaMA 모델의 다양한 버전(7B, 13B, 33B, 65B)과 Chinchilla 모델의 학습 과정에서 질의응답과 상식 추론 과제에서의 성능 변화를 보여줍니다. 이 결과는 모델의 규모가 커질수록 대체로 더 높은 성능을 달성할 수 있음을 명확하게 보여주고 있습니다. 특히 주목할 만한 점은 더 큰 모델들이 학습 초기부터 빠른 성능 향상을 보이며, 최종적으로도 더 높은 성능에 도달한다는 것입니다.

이러한 실험 결과들은 LLaMA가 기존의 대규모 언어 모델들과 비교하여 매우 효율적인 아키텍처를 가지고 있음을 입증합니다. 특히 모델 크기 대비 성능이 우수하다는 점은, 연구 커뮤니티에서 더 많은 실험과 개선이 가능하도록 하는 중요한 의미를 가집니다.
### 편향성, 유해성 및 허위정보

대규모 언어 모델의 발전과 함께 이러한 모델들이 가진 잠재적 위험성에 대한 평가의 중요성이 커지고 있습니다. LLaMA 연구진은 모델의 편향성, 유해 콘텐츠 생성, 그리고 허위정보 생성 가능성을 체계적으로 평가했습니다.

RealToxicityPrompts 벤치마크를 통한 평가에서는 모델이 생성할 수 있는 유해 콘텐츠의 범위를 측정했습니다. 이 벤치마크는 약 10만 개의 프롬프트로 구성되어 있으며, 각 프롬프트에 대한 모델의 응답은 PerspectiveAPI를 통해 유해성 점수를 측정받습니다. 실험 결과, 모델의 크기가 커질수록 유해 콘텐츠 생성 경향이 증가하는 것으로 나타났으며, 특히 "공손하고 존중하는 방식으로 답변하라"는 지시가 포함된 '존중' 프롬프트에서 이러한 경향이 더욱 두드러졌습니다.

CrowS-Pairs 벤치마크를 통해서는 9가지 범주(성별, 종교, 인종/피부색, 성적 지향, 연령, 국적, 장애, 외모, 사회경제적 지위)에서의 편향성을 평가했습니다. LLaMA-65B는 GPT-3와 OPT-175B와 비교했을 때 전반적으로 비슷하거나 약간 나은 수준의 편향성을 보였으나, 종교 관련 편향성에서는 OPT-175B보다 10% 높은 편향성을 나타냈습니다.

성별 편향성을 더 깊이 분석하기 위해 WinoGender 벤치마크를 활용했습니다. 이는 대명사 해결(pronoun resolution) 과제로, 문장 내의 직업, 참여자, 대명사 간의 관계를 파악하는 능력을 측정합니다. 연구 결과, LLaMA는 성 중립적인 "their/them/someone" 대명사에 대해서는 우수한 성능을 보였으나, "her/her/she"와 "his/him/he"와 같은 성별 특정적 대명사에 대해서는 상대적으로 낮은 성능을 보였습니다. 이는 모델이 문맥상의 증거보다는 특정 직업에 대한 사회적 성별 고정관념에 기반하여 판단을 내리고 있음을 시사합니다.

TruthfulQA 벤치마크를 통해서는 모델의 진실성, 즉 실제 세계에 대한 사실을 정확하게 식별하는 능력을 평가했습니다. LLaMA-65B는 GPT-3보다 높은 점수를 기록했으나, 여전히 정답률이 낮은 수준에 머물러 있어 허위정보 생성의 위험성이 존재함을 보여주었습니다. 이는 대규모 언어 모델이 가진 근본적인 한계를 드러내는 것으로, 모델이 학습 데이터에 존재하는 잘못된 정보나 편향된 관점을 그대로 반영할 수 있다는 점을 시사합니다.
### 탄소 발자국

대규모 언어 모델의 학습 과정에서 발생하는 환경적 영향을 정량적으로 평가하기 위해, 연구진은 에너지 소비량과 탄소 배출량을 체계적으로 분석했습니다. Wu와 연구진이 제안한 방법론을 따라, 모델 학습에 필요한 전력 소비량(Wh)을 다음과 같은 수식으로 계산했습니다.

$$ \text{Wh} = \text{GPU-h} \times \text{(GPU 전력 소비량)} \times \text{PUE} $$

여기서 GPU-h는 총 GPU 사용 시간을, PUE(Power Usage Effectiveness)는 데이터 센터의 전력 효율성을 나타내는 지표로 1.1로 설정되었습니다. 이는 데이터 센터의 냉각 시스템과 기타 인프라에 의한 추가 전력 소비를 고려한 값입니다.

탄소 배출량 계산을 위해 연구진은 미국의 평균 탄소 집약도 계수인 0.385 kg CO₂eq/KWh를 적용했습니다. 이는 다음과 같은 수식으로 표현됩니다.

$$ \text{tCO}_2\text{eq} = \text{MWh} \times 0.385 $$

이러한 표준화된 방법론을 통해 다른 대규모 언어 모델들과의 공정한 비교가 가능해졌습니다. 예를 들어, BLOOM의 경우 실제로는 0.057 kg CO₂eq/KWh의 낮은 탄소 집약도를 가진 전력망을 사용하여 27 tCO₂eq를 배출했고, OPT는 0.231 kg CO₂eq/KWh의 전력망에서 82 tCO₂eq를 배출했습니다.

LLaMA 모델 시리즈의 개발 과정에서는 2,048개의 A100-80GB GPU를 약 5개월 동안 사용했으며, 이는 연구진의 가정 하에서 총 2,638 MWh의 전력을 소비하고 1,015 tCO₂eq의 탄소를 배출한 것으로 추정됩니다. 이는 OPT의 경우와 비교했을 때(992개의 A100-80GB를 34일 동안 사용) 상당히 큰 규모입니다.

연구진은 이러한 대규모 탄소 배출의 환경적 영향을 인정하면서도, 학습된 모델을 공개함으로써 향후 유사한 모델의 재학습 필요성을 줄일 수 있다는 점을 강조했습니다. 특히 작은 규모의 모델들(예: LLaMA-7B)은 단일 GPU에서도 구동이 가능하여, 추가적인 환경 부담을 최소화하면서도 효과적으로 활용될 수 있습니다.
### 관련 연구

언어 모델의 역사적 발전 과정은 Shannon이 1948년에 제시한 수학적 기반에서 시작됩니다. Shannon은 언어를 단어, 토큰, 또는 문자의 시퀀스에 대한 확률 분포로 정의했으며, 이는 현대 언어 모델의 이론적 토대가 되었습니다. 이러한 접근 방식은 다음 토큰 예측이라는 과제로 구체화되었으며, 이는 자연어 처리 분야의 핵심 문제로 자리잡았습니다. 특히 Turing이 1950년에 제안한 "모방 게임"을 통한 기계 지능 측정 방식은 언어 모델링을 인공지능 발전의 중요한 지표로 확립하는 계기가 되었습니다.

언어 모델의 아키텍처 발전 과정을 살펴보면, 초기에는 n-gram 통계에 기반한 모델이 주를 이뤘습니다. 이러한 모델들은 희소한 이벤트의 추정을 개선하기 위해 다양한 스무딩 기법을 도입했습니다. 지난 20년 동안 신경망이 언어 모델링 분야에 성공적으로 적용되기 시작했는데, 이는 피드포워드 모델에서 시작하여 순환 신경망(RNN), LSTM으로 발전했습니다. 최근에는 자기 주의 메커니즘에 기반한 트랜스포머 네트워크가 등장하여 특히 장거리 의존성 포착에서 큰 발전을 이루었습니다.

모델과 데이터셋의 규모 확장 측면에서도 주목할 만한 발전이 있었습니다. 2007년 Brants와 연구진은 2조 개의 토큰으로 학습된 언어 모델이 기계 번역 품질 향상에 기여함을 보여주었습니다. 이후 Heafield와 연구진은 웹 규모의 데이터에 Kneser-Ney 스무딩을 적용하는 방법을 개발했고, 이를 통해 CommonCrawl의 9,750억 토큰으로 학습된 5-gram 모델이 구현되었습니다.

신경망 기반 언어 모델의 발전 과정에서는 Jozefowicz와 연구진이 10억 개의 매개변수를 가진 LSTM으로 One Billion Word 벤치마크에서 최고 성능을 달성했습니다. 이후 트랜스포머 모델의 등장과 함께 BERT, GPT-2, Megatron-LM, T5 등이 연이어 발표되었고, GPT-3가 1,750억 개의 매개변수로 큰 돌파구를 마련했습니다. 이는 Jurassic-1, Megatron-Turing NLG, Gopher, Chinchilla, PaLM, OPT, GLM과 같은 대규모 언어 모델의 시대를 여는 계기가 되었습니다.

최근의 연구들은 모델과 데이터셋 규모가 성능에 미치는 영향에 대해 체계적인 분석을 시도하고 있습니다. 특히 Kaplan과 연구진은 트랜스포머 기반 언어 모델에 대한 구체적인 스케일링 법칙을 도출했으며, 이는 Hoffmann과 연구진에 의해 데이터셋 스케일링에 대한 학습률 스케줄 조정을 통해 더욱 정교화되었습니다. Wei와 연구진은 이러한 스케일링이 대규모 언어 모델의 능력에 미치는 영향을 종합적으로 분석했습니다.
### 결론

본 연구에서는 공개적으로 이용 가능한 데이터만을 사용하여 최신 기초 언어 모델들과 경쟁력 있는 성능을 달성한 LLaMA 모델 시리즈를 소개했습니다. 특히 주목할 만한 점은 LLaMA-13B가 GPT-3보다 10배 이상 작은 규모임에도 불구하고 더 우수한 성능을 보여주었다는 것입니다. 또한 LLaMA-65B는 Chinchilla-70B와 PaLM-540B와 같은 최고 수준의 모델들과 대등한 성능을 달성했습니다.

이전 연구들과는 달리, 본 연구는 독점적인 데이터셋을 사용하지 않고도 최고 수준의 성능을 달성할 수 있다는 것을 입증했습니다. 이는 대규모 언어 모델 연구의 민주화에 큰 기여를 할 것으로 기대됩니다. 연구진은 이러한 모델들을 연구 커뮤니티에 공개함으로써 대규모 언어 모델의 발전을 가속화하고, 유해성과 편향성과 같은 알려진 문제점들을 개선하기 위한 노력을 지원하고자 합니다.

Chung과 연구진의 연구 결과와 마찬가지로, 본 연구에서도 이러한 모델들을 명령어에 대해 미세조정했을 때 매우 유망한 결과를 얻을 수 있었습니다. 연구진은 이러한 방향의 연구를 향후 더욱 심도 있게 진행할 계획입니다. 또한 모델의 규모를 키울수록 지속적인 성능 향상이 관찰되었기 때문에, 향후에는 더 큰 규모의 모델을 더 많은 사전 학습 데이터로 학습시켜 공개할 계획입니다.

이러한 연구 결과는 대규모 언어 모델 분야에서 중요한 이정표를 제시합니다. 특히 공개 데이터만으로도 최고 수준의 성능을 달성할 수 있다는 점은, 향후 이 분야의 연구가 더욱 개방적이고 협력적인 방향으로 발전할 수 있는 가능성을 보여줍니다. 또한 모델의 규모와 성능 간의 관계에 대한 새로운 통찰을 제공하며, 효율적인 모델 설계의 중요성을 강조합니다.
- - -
### References
* [LLaMA: Open and Efficient Foundation Language Models](http://arxiv.org/pdf/2302.13971v1)