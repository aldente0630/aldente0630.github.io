---
layout: post
title: "Llama 2: Open Foundation and Fine-Tuned Chat Models"
date: 2023-07-18 14:31:57
author: "GenAI, Meta"
categories: "Natural-Language-Processing"
tags: ["Large-Language-Models", "Reinforcement-Learning-with-Human-Feedback", "Safety-Alignment", "Dialogue-Systems", "Prompt-Engineering"]
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

메타(Meta)는 인공지능 기술의 민주화와 책임있는 발전이라는 두 가지 목표를 동시에 추구하고자 Llama 2를 개발했습니다. 기존의 대규모 언어 모델들이 폐쇄적으로 운영되면서 발생하는 연구 발전의 제한과, 개방형 모델들의 성능 한계를 극복하고자 했습니다. 특히 도움이 되면서도 안전한 AI 시스템을 구축하는 것이 핵심 동기였으며, 이는 학계와 산업계가 함께 발전할 수 있는 기반을 마련하고자 하는 의도를 반영합니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

Llama 2는 세 가지 주요 혁신을 통해 기존 모델들의 한계를 극복했습니다. 첫째, 사전 학습 데이터의 규모를 40% 증가시키고 컨텍스트 길이를 2배로 확장했습니다. 둘째, 그룹화된 쿼리 어텐션(GQA) 메커니즘을 도입하여 모델의 효율성을 크게 향상시켰습니다. 셋째, Ghost Attention(GAtt) 기술을 개발하여 다중 턴 대화에서의 일관성을 개선했습니다. 또한 체계적인 안전성 프레임워크를 구축하여 모델의 유해성을 최소화했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?

구현은 크게 세 단계로 진행되었습니다. 먼저, 공개적으로 이용 가능한 데이터로 사전 학습을 수행했습니다. 다음으로, 지도학습 기반 미세조정을 통해 초기 대화 모델을 생성했습니다. 마지막으로, 인간 피드백 강화학습(RLHF)을 적용하여 모델의 응답 품질과 안전성을 개선했습니다. 특히 RLHF 과정에서는 보상 모델링과 거부 샘플링을 결합한 혁신적인 접근 방식을 사용했으며, 350명 이상의 전문가로 구성된 레드팀을 통해 모델의 안전성을 지속적으로 검증했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

Llama 2의 성공은 개방형 AI 모델이 상용 모델들과 경쟁할 수 있는 수준의 성능을 달성할 수 있다는 것을 입증했습니다. 특히 70B 모델은 ChatGPT와의 비교에서 36%의 승률과 31.5%의 동률을 기록하며 경쟁력을 입증했습니다. 더불어 안전성 측면에서도 유해 콘텐츠 생성률을 효과적으로 0%까지 낮추는데 성공했습니다. 이러한 성과는 AI 기술의 민주화와 책임있는 발전이 양립 가능하다는 것을 보여주며, 향후 AI 연구 발전의 새로운 패러다임을 제시했다는 점에서 중요한 의미를 가집니다.
- - -
## Llama 2: 개방형 기초 및 미세조정된 대화 모델

메타(Meta)에서 공개한 Llama 2는 인공지능 분야에서 주목할 만한 진전을 이루어낸 개방형 대규모 언어 모델입니다. 이 모델은 원래의 LLaMA를 기반으로 하여 상당한 개선을 이루었으며, 특히 기초 모델과 대화에 특화된 버전을 모두 제공한다는 점에서 큰 의미를 가집니다.

Llama 2는 [Vaswani와 연구진](https://arxiv.org/pdf/1706.03762v7)이 제안한 트랜스포머 아키텍처를 기반으로 하되, 이를 현대적으로 개선했습니다. 특히 주목할 만한 점은 [Hoffmann과 연구진](https://arxiv.org/pdf/2203.15556)이 제시한 계산 최적화 원칙을 충실히 반영했다는 것입니다. 이 원칙에 따르면, 모델의 크기(\\(N\\))와 학습 데이터의 양(\\(D\\))은 계산 자원(\\(C\\))에 대해 다음과 같은 관계를 가져야 합니다.

$$ N_{opt} \propto C^{0.5}, D_{opt} \propto C^{0.5} $$

이러한 이론적 기반을 토대로, Llama 2는 공개적으로 이용 가능한 데이터셋만을 사용하여 학습되었음에도 불구하고 뛰어난 성능을 달성했습니다. [Touvron과 연구진](https://arxiv.org/pdf/2302.13971)의 원래 LLaMA 모델의 장점을 계승하면서도, 더욱 발전된 형태의 사전 정규화(pre-normalization)와 회전 위치 임베딩(rotary positional embeddings)을 도입했습니다.

특히 주목할 만한 발전은 대화 모델 버전에서 이루어졌습니다. [Bai와 연구진](https://arxiv.org/pdf/2204.05862)이 제시한 인간 피드백 강화학습(RLHF) 방법론을 적용하여, 모델이 더욱 도움이 되고 안전한 응답을 생성할 수 있도록 했습니다. 이 과정에서 [Longpre와 연구진](https://arxiv.org/pdf/2301.13688)의 Flan 컬렉션에서 얻은 통찰을 활용하여, 다양한 형태의 프롬프트와 지시사항을 학습에 통합했습니다.

이러한 종합적인 접근 방식을 통해 Llama 2는 단순한 언어 모델을 넘어서, 실제 응용 환경에서 유용하게 활용될 수 있는 도구로 발전했습니다. 특히 모델의 개방성은 학계와 산업계가 함께 발전할 수 있는 기회를 제공하며, 이는 인공지능 기술의 민주화에 큰 기여를 할 것으로 기대됩니다.
## Llama 2의 혁신적 발전과 기술적 특징

메타(Meta)가 공개한 Llama 2는 인공지능 분야에 획기적인 진전을 가져온 대규모 언어 모델 시리즈입니다. 이 모델은 7B에서 70B에 이르는 다양한 매개변수 규모로 개발되었으며, 특히 대화형 사용 사례에 최적화된 Llama 2-Chat 버전을 함께 제공합니다. 기존의 개방형 모델들과 비교할 때, Llama 2는 도움말과 안전성 벤치마크에서 우수한 성능을 보여주며, 일부 상용 모델들과 대등한 수준의 성능을 달성했습니다.

Llama 2의 주요 기술적 혁신은 세 가지 핵심 영역에서 이루어졌습니다. 첫째, 사전 학습 데이터의 규모를 40% 증가시켰으며, 모델의 컨텍스트 길이를 2배로 확장했습니다. 둘째, [Ainslie와 연구진](https://arxiv.org/pdf/2305.13245)이 제안한 그룹화된 쿼리 어텐션(grouped-query attention) 메커니즘을 도입하여 모델의 효율성을 크게 향상시켰습니다. 셋째, 인간 피드백 강화학습(RLHF)을 통해 모델의 응답 품질과 안전성을 체계적으로 개선했습니다.

![Helpfulness Evaluation Results](https://ar5iv.org//html/2307.09288/assets/x1.png)
모델의 도움말 성능 평가에서는 약 4,000개의 단일 및 다중 턴 프롬프트를 사용하여 인간 평가자들이 모델 생성을 비교했습니다. 평가 결과는 95% 신뢰 구간에서 1-2% 범위의 정확도를 보여주었습니다. 이러한 평가는 프롬프트 세트의 한계, 평가 지침의 주관성, 개별 평가자의 주관성 등으로 인한 노이즈가 존재할 수 있음을 고려해야 합니다.

안전성 측면에서는 특별히 주목할 만한 발전이 있었습니다. 약 2,000개의 적대적 프롬프트를 사용한 평가에서 Llama 2-Chat은 기존 개방형 모델들보다 우수한 안전성을 보여주었습니다. 이는 안전성에 특화된 데이터 주석 작업과 미세 조정, 레드팀 평가, 반복적 평가 과정을 통해 달성되었습니다.

모델의 학습 과정은 세 단계로 구성됩니다. 먼저 공개적으로 이용 가능한 온라인 소스를 사용하여 Llama 2의 사전 학습을 수행합니다. 다음으로 지도 학습 기반 미세 조정을 통해 Llama 2-Chat의 초기 버전을 생성합니다. 마지막으로 거부 샘플링과 근접 정책 최적화(PPO)를 포함한 RLHF 방법론을 통해 모델을 반복적으로 개선합니다.

![Training Process](https://ar5iv.org//html/2307.09288/assets/x1.png)
RLHF 단계에서는 모델 개선과 병행하여 반복적인 보상 모델링 데이터를 축적하는 것이 매우 중요합니다. 이는 보상 모델이 분포 내에서 유지되도록 보장하는 핵심 요소입니다. 이러한 체계적인 접근 방식을 통해 Llama 2는 안전하고 유용한 AI 도구로서의 역할을 수행할 수 있게 되었습니다.
## Llama 2의 사전학습 방법론과 기술적 혁신

### 사전학습 데이터와 처리 방법론

Llama 2는 [Touvron과 연구진](https://arxiv.org/pdf/2302.13971)이 제시한 최적화된 자기회귀 트랜스포머 모델을 기반으로 하되, 성능 향상을 위한 여러 혁신적인 변화를 도입했습니다. 가장 주목할 만한 개선사항은 데이터 처리 방식의 강화, 학습 데이터 구성의 최적화, 전체 토큰 수의 40% 증가, 컨텍스트 길이의 2배 확장, 그리고 대형 모델의 추론 확장성 향상을 위한 그룹화된 쿼리 어텐션(GQA)의 도입입니다.

사전학습 데이터는 공개적으로 이용 가능한 소스에서만 수집되었으며, 메타의 제품이나 서비스 데이터는 포함하지 않았습니다. 특히 개인정보 보호를 위해 개인 정보가 많이 포함된 것으로 알려진 사이트의 데이터는 제외했습니다. 총 2조 개의 토큰으로 구성된 데이터셋에서는 사실에 기반한 소스의 비중을 높여 모델의 지식을 강화하고 환각 현상을 줄이고자 했습니다.

### 모델 아키텍처와 학습 세부사항

Llama 2는 기존 Llama 1의 기본 구조를 유지하면서도 여러 중요한 개선을 도입했습니다. [Vaswani와 연구진](https://arxiv.org/pdf/1706.03762v7)이 제안한 표준 트랜스포머 아키텍처를 사용하며, [Zhang과 연구진](https://arxiv.org/pdf/1910.07467v1)의 RMSNorm을 통한 사전 정규화, SwiGLU 활성화 함수, 그리고 회전 위치 임베딩(RoPE)을 적용했습니다.

학습 과정에서는 [Loshchilov와 Hutter](https://arxiv.org/pdf/1711.05101)가 제안한 AdamW 최적화기를 사용했으며, 다음과 같은 하이퍼파라미터를 적용했습니다.

$$ \beta_1 = 0.9, \beta_2 = 0.95, \epsilon = 10^{-5} $$

코사인 학습률 스케줄링을 채택하여 2000 스텝의 웜업 기간을 거친 후 최종 학습률을 피크 학습률의 10%까지 감소시켰습니다. 가중치 감쇠는 0.1, 그래디언트 클리핑은 1.0으로 설정했습니다.

![Training Loss for Llama 2 models](https://ar5iv.org//html/2307.09288/assets/x1.png)
이 그래프는 Llama 2 모델 계열의 학습 손실을 보여줍니다. 2조 개의 토큰으로 사전학습을 진행한 후에도 모델이 포화 상태에 도달하지 않았음을 확인할 수 있습니다.

### 학습 인프라와 환경적 영향

Llama 2의 학습은 메타의 연구 슈퍼클러스터(RSC)와 내부 프로덕션 클러스터에서 진행되었습니다. 두 클러스터 모두 NVIDIA A100을 사용하되, 네트워크 연결 방식과 GPU 전력 소비 제한에서 차이가 있습니다. RSC는 NVIDIA Quantum InfiniBand를, 프로덕션 클러스터는 RoCE를 사용했으며, GPU 전력 제한은 각각 400W와 350W로 설정되었습니다.

환경적 영향 측면에서, 전체 학습 과정에서 3.3M GPU 시간이 소요되었으며, 총 539 tCO₂eq의 탄소가 배출되었습니다. 이는 메타의 지속가능성 프로그램을 통해 100% 상쇄되었습니다. 더욱 중요한 점은, 모델의 공개 배포를 통해 다른 기업들이 추가적인 사전학습을 수행할 필요가 없어져 전체적인 환경 영향을 줄일 수 있다는 것입니다.
## Llama 2의 미세조정과 RLHF 구현

### 지도학습 기반 미세조정(SFT)

Llama 2의 미세조정 과정은 수개월에 걸친 연구와 정렬 기술의 반복적 적용을 통해 이루어졌습니다. 이 과정에서는 지도학습 기반 미세조정과 인간 피드백 강화학습(RLHF)이 핵심적인 역할을 했습니다. 특히 주목할 만한 것은 새롭게 개발된 Ghost Attention(GAtt) 기술로, 이는 다중 턴 대화에서 일관성을 유지하는 데 큰 도움을 주었습니다.

미세조정의 첫 단계는 [Chung과 연구진](https://arxiv.org/pdf/2210.11416)이 제안한 공개 지시 데이터셋을 활용하는 것으로 시작되었습니다. 그러나 연구 과정에서 발견된 중요한 점은 데이터의 양보다 질이 더 중요하다는 것이었습니다. 수백만 개의 제3자 데이터셋을 제쳐두고, 수만 개의 고품질 주석 데이터만을 사용했을 때 더 나은 결과를 얻을 수 있었습니다.

미세조정을 위한 학습 설정은 다음과 같습니다.
- 코사인 학습률 스케줄 사용
- 초기 학습률: \\( 2 \times 10^{-5} \\)
- 가중치 감쇠: 0.1
- 배치 크기: 64
- 시퀀스 길이: 4096 토큰

### 보상 모델링과 RLHF

RLHF 구현에서는 두 가지 주요 알고리즘을 탐구했습니다.

1. 근접 정책 최적화(PPO)
2. 거부 샘플링 미세조정

이 중 거부 샘플링은 RLHF-V4까지 주로 사용되었으며, 이후에는 두 방법을 순차적으로 결합하여 사용했습니다. 보상 모델은 다음과 같은 이진 순위 손실을 사용하여 학습되었습니다.

$$ \mathcal{L}_{ranking} = -\text{log}(\sigma(r_{\theta}(x,y_{c})-r_{\theta}(x,y_{r})-m(r))) $$

여기서 \\( r_{\theta}(x,y) \\)는 프롬프트 \\(x\\)와 응답 \\(y\\)에 대한 보상 점수를, \\(m(r)\\)은 선호도 등급에 따른 마진을 나타냅니다.

![보상 모델의 스케일링 트렌드](https://ar5iv.org//html/2307.09288/assets/x1.png)
이 그래프는 데이터와 모델 크기에 따른 보상 모델의 정확도 향상을 보여줍니다. 더 많은 데이터와 더 큰 모델 크기가 일관되게 성능을 향상시키며, 아직 학습 포화점에 도달하지 않았음을 알 수 있습니다.

### Ghost Attention(GAtt) 기술

다중 턴 대화의 일관성을 개선하기 위해 개발된 GAtt는 Context Distillation에서 영감을 받아 개발되었습니다. 이 기술은 주의력 메커니즘을 수정하여 시스템 메시지나 초기 지시사항이 전체 대화에 걸쳐 영향을 미치도록 합니다.

![GAtt 주의력 시각화](https://ar5iv.org//html/2307.09288/assets/x1.png)
이 시각화는 GAtt를 적용했을 때와 적용하지 않았을 때의 주의력 활성화를 보여줍니다. GAtt를 적용한 모델은 시스템 메시지에 대해 더 지속적이고 강한 주의력 활성화를 유지합니다.

### 평가 결과

인간 평가에서 Llama 2-Chat은 오픈소스 모델들을 크게 앞섰으며, ChatGPT와 비교했을 때도 경쟁력 있는 성능을 보여주었습니다. 70B 모델은 ChatGPT 대비 36%의 승률과 31.5%의 동률을 기록했습니다. 특히 도움말과 안전성 측면에서 균형 잡힌 성능을 보여주었으며, GAtt의 도입으로 다중 턴 대화에서의 일관성도 크게 개선되었습니다.
## Llama 2의 안전성 측정과 완화 방법론

### 사전학습 데이터의 안전성 분석

Llama 2의 개발 과정에서 안전성은 가장 중요한 고려사항 중 하나였습니다. [Weidinger와 연구진](https://arxiv.org/pdf/2112.04359)이 제시한 프레임워크를 기반으로, 메타 연구진은 사전학습 데이터부터 최종 모델 배포까지 포괄적인 안전성 분석과 완화 전략을 구현했습니다.

사전학습 데이터의 안전성 분석은 크게 세 가지 차원에서 이루어졌습니다. 첫째, 인구통계학적 대표성 분석에서는 성별 대명사와 정체성 용어의 분포를 조사했습니다. 영어 말뭉치에서 'He' 계열 대명사가 'She' 계열보다 더 높은 빈도로 등장하는 것으로 나타났으며, 이는 모델의 생성 패턴에 영향을 미칠 수 있는 중요한 발견이었습니다.

둘째, 데이터의 유해성 측정을 위해 ToxiGen 데이터셋으로 미세조정된 HateBERT 분류기를 사용했습니다. 분석 결과, 전체 문서의 약 0.2%만이 0.5 이상의 유해성 점수를 기록했습니다. 이는 비교적 낮은 수준이지만, 완전히 무시할 수 없는 수준입니다.

### 안전성 미세조정 프레임워크

안전성 미세조정은 세 가지 핵심 기술을 통합적으로 적용했습니다.

1. 지도학습 기반 안전성 미세조정에서는 적대적 프롬프트와 안전한 응답 예시를 수집하여 초기 모델을 훈련했습니다. 이는 [Ganguli와 연구진](https://arxiv.org/pdf/2302.07459)이 제안한 도덕적 자기교정 능력을 강화하는 방향으로 설계되었습니다.

2. 안전성 RLHF에서는 다음과 같은 보상 모델 손실 함수를 사용했습니다.

$$ \mathcal{L}_{safety} = -\log(\sigma(r_{\theta}(x,y_{safe})-r_{\theta}(x,y_{unsafe}))) $$

여기서 \\(r_{\theta}\\)는 보상 모델의 점수를, \\(x\\)는 프롬프트를, \\(y_{safe}\\)와 \\(y_{unsafe}\\)는 각각 안전하고 안전하지 않은 응답을 나타냅니다.

3. 안전성 컨텍스트 증류에서는 안전 관련 프리프롬프트를 사용하여 더 안전한 응답을 생성한 후, 이를 모델에 내재화하는 방식을 채택했습니다.

### 레드팀 평가와 안전성 검증

[Ganguli와 연구진](https://arxiv.org/pdf/2209.07858)의 방법론을 확장하여, 350명 이상의 전문가로 구성된 레드팀을 운영했습니다. 이들은 사이버보안, 선거 사기, 소셜 미디어 허위정보 등 다양한 위험 영역에서 모델을 시험했습니다. 모델의 견고성(\\(\gamma\\))은 다음과 같이 정의되었습니다.

$$ \gamma = \frac{\text{위반 응답을 유발하는 프롬프트 수}}{\text{전문가 수 } \times \text{ 시간}} $$

이러한 종합적인 안전성 프레임워크를 통해 Llama 2-Chat은 유해성 생성률을 효과적으로 0%까지 낮추는데 성공했으며, 이는 다른 공개 모델들과 비교했을 때 가장 우수한 수준입니다.

[계속...]
## Llama 2의 주요 발견과 논의사항

### RLHF를 통한 모델 행동 개선

Llama 2 개발 과정에서 가장 주목할 만한 발견은 인간 피드백 강화학습(RLHF)의 효과성입니다. 초기에는 많은 연구자들이 더 밀도 높은 신호를 제공하는 지도학습 주석을 선호했으나, RLHF가 비용과 시간 효율성 측면에서 매우 효과적임이 입증되었습니다. 

![RLHF 분포 이동](https://ar5iv.org//html/2307.09288/assets/x1.png)
위 그래프는 SFT 모델에서 RLHF로 진행되면서 나타나는 분포 이동을 보여줍니다. RLHF는 낮은 품질의 응답들을 점진적으로 제거하면서 전체적인 분포를 우측으로 이동시켰습니다. 이는 보상 메커니즘이 바람직하지 않은 응답들에 낮은 점수를 빠르게 할당하고 인간의 선호도에 맞춰 조정되는 것을 보여줍니다.

### 동적 온도 조절 메커니즘

연구진은 RLHF를 통해 이전에 보고되지 않은 흥미로운 현상을 발견했습니다. 모델이 프롬프트의 맥락에 따라 동적으로 온도를 조절하는 능력을 보여준 것입니다. 이를 검증하기 위해 다음과 같은 온도 범위에서 실험을 진행했습니다.

$$ T \in \{k/10 \mid k \in \mathbb{N} : 1 \leq k \leq 15\} $$

![온도 적응 결과](https://ar5iv.org//html/2307.09288/assets/x1.png)
실험 결과는 프롬프트 유형에 따른 뚜렷한 차이를 보여줍니다. 창의적인 프롬프트(예: "시 쓰기")에 대해서는 높은 온도에서도 다양성이 유지되는 반면, 사실 기반 프롬프트(예: "수도는?")에 대해서는 일관된 응답을 생성하도록 Self-BLEU 기울기가 감소했습니다.

### 시간적 지식 조직화

Llama 2-Chat은 최소한의 시간 관련 데이터(1,000개의 SFT 예제)만으로도 강력한 시간적 추론 능력을 보여주었습니다. 이는 대규모 언어 모델이 다음 토큰 예측과 무작위로 섞인 데이터만으로 학습됨에도 불구하고, 시간 개념을 예상보다 더 깊이 내재화했음을 시사합니다.

### 도구 사용의 자발적 출현

![도구 사용 시연](https://ar5iv.org//html/2307.09288/assets/x1.png)
Llama 2-Chat은 도구 사용에 대한 명시적인 학습 없이도 외부 도구를 효과적으로 활용하는 능력을 보여주었습니다. 수학 데이터셋에서의 성능 평가 결과, Llama 2-Chat은 67.1%의 정확도를 달성하여 기존의 GPT-3(14.0%)나 Toolformer(40.4%)를 크게 앞섰습니다.

### 한계점과 윤리적 고려사항

Llama 2-Chat은 사전학습 이후 지식 업데이트 중단, 검증되지 않은 조언 생성 가능성, 환각 현상 등 다른 대규모 언어 모델들과 유사한 한계를 가지고 있습니다. 특히 영어 이외의 언어에서는 제한된 사전학습 데이터로 인해 성능이 불안정할 수 있습니다. 또한 공개적으로 이용 가능한 온라인 데이터셋으로 학습되었기 때문에, 유해하거나 편향된 콘텐츠를 생성할 가능성이 있습니다.

[계속...]
## 대규모 언어 모델의 발전과 관련 연구

### 모델 규모와 계산 효율성의 진화

대규모 언어 모델(LLM) 분야는 지난 몇 년간 괄목할 만한 발전을 이루었습니다. [Kaplan과 연구진](https://arxiv.org/pdf/2001.08361)이 제시한 스케일링 법칙을 기반으로, GPT-3에서 Gopher에 이르기까지 1000억 개 이상의 매개변수를 가진 여러 모델들이 등장했습니다. 특히 주목할 만한 것은 700억 개의 매개변수를 가진 Chinchilla 모델로, 이는 기존의 스케일링 법칙을 모델 가중치에서 토큰 수로 재정의했습니다. 이러한 스케일링 관계는 다음과 같은 수학적 형태로 표현됩니다.

$$ L(N,D) = \left[\left(\frac{N_c}{N}\right)^{\frac{\alpha_N}{\alpha_D}} + \frac{D_c}{D}\right]^{\alpha_D} $$

여기서 \\(N\\)은 모델 크기, \\(D\\)는 데이터셋 크기를 나타내며, \\(\alpha_N\\)과 \\(\alpha_D\\)는 각각 모델과 데이터에 대한 스케일링 지수입니다.

### 개방형 vs 폐쇄형 모델의 발전

개방형 모델과 폐쇄형 모델 사이의 경쟁은 인공지능 발전의 중요한 축을 형성해왔습니다. BLOOM, OPT, Falcon과 같은 개방형 모델들이 GPT-3와 Chinchilla 같은 폐쇄형 모델들에 도전장을 내밀었습니다. 그러나 ChatGPT, Bard, Claude와 같은 "프로덕션 수준"의 LLM들은 여전히 성능과 사용성 면에서 뚜렷한 차이를 보입니다. 이러한 차이는 주로 인간 선호도에 맞춘 정교한 조정 기술에서 비롯되며, 이는 개방형 커뮤니티에서 아직 완전히 해결하지 못한 과제입니다.

### 지시 학습과 RLHF의 혁신

지시 학습 분야에서는 [Wei와 연구진](https://arxiv.org/abs/2109.01652)이 다수의 데이터셋에 대한 미세조정을 통해 미견 과제에 대한 제로샷 성능을 달성했습니다. 이러한 접근방식은 체인오브소트(chain-of-thought) 프롬프팅과 결합되어 더욱 발전했으며, 복잡한 문제에 대한 모델의 추론 능력을 크게 향상시켰습니다.

인간 피드백 강화학습(RLHF)은 특히 주목할 만한 발전을 이루었습니다. [Christiano와 연구진](https://arxiv.org/abs/1706.03741)이 제안한 이 방법론은 다음과 같은 보상 함수를 통해 구현됩니다.

$$ R(s,a) = \mathbb{E}_{h \sim H}[r_h(s,a)] $$

여기서 \\(r_h\\)는 인간 평가자의 보상이며, \\(H\\)는 평가자 분포를 나타냅니다.

### 안전성 과제와 해결 방안

LLM의 안전성 문제는 [Bender와 연구진](https://arxiv.org/abs/2112.04359)이 지적한 바와 같이 편향성, 유해성, 개인정보 유출 등 다양한 측면에서 발생합니다. 이러한 문제들은 시스템 내부에서 평가할 수 있는 것과 사회적 맥락에서 평가해야 하는 것으로 구분됩니다. 레드팀 평가를 통해 미세조정된 LLM들의 특정 취약점들이 발견되었으며, 이는 유해 콘텐츠 생성과 관련된 다양한 공격 유형을 포함합니다.

더 넓은 사회적 맥락에서는 AI 연구 가속화로 인한 일자리 대체와 LLM에 대한 과도한 의존이 훈련 데이터 품질 저하로 이어질 수 있다는 우려가 제기되고 있습니다. 이러한 도전과제들을 해결하기 위해서는 정책, 학계, 산업계가 협력하여 지속적인 연구와 논의를 이어가야 할 것입니다.
## Llama 2의 결론과 연구 의의

### 주요 성과와 기술적 혁신

Llama 2는 7B에서 70B에 이르는 매개변수를 가진 사전학습 및 미세조정된 모델 시리즈로서, 기존 오픈소스 대화 모델들과의 경쟁에서 우수한 성능을 보여주었습니다. 특히 주목할 만한 점은 일부 상용 모델들과 대등한 수준의 성능을 달성했다는 것입니다. 이러한 성과는 다음과 같은 수학적 지표로 검증되었습니다.

$$ \text{Performance}_{\text{relative}} = \frac{\text{Score}_{\text{Llama2}}}{\text{Score}_{\text{baseline}}} \geq 1.0 $$

여기서 \\(\text{Score}\\)는 표준화된 평가 지표를 나타냅니다.

### 안전성과 책임있는 개발

[Weidinger와 연구진](https://arxiv.org/pdf/2112.04359)이 제시한 프레임워크를 기반으로, Llama 2는 도움이 되면서도 안전한 AI 시스템 개발이라는 두 가지 목표를 동시에 추구했습니다. 안전성 평가는 다음과 같은 종합 지표를 통해 측정되었습니다.

$$ S = \alpha H + (1-\alpha)R $$

여기서 \\(H\\)는 도움말 점수, \\(R\\)은 안전성 점수, \\(\alpha\\)는 가중치를 나타냅니다.

### 연구 공동체에 대한 기여

메타는 Llama 2와 Llama 2-Chat을 책임감 있게 공개함으로써 AI 연구의 발전 속도를 가속화하고 사회적 기여를 확대하고자 했습니다. 이는 [Touvron과 연구진](https://arxiv.org/pdf/2302.13971)이 주장한 AI 민주화의 중요성을 실천한 것입니다.

### 향후 연구 방향

GPT-4와 같은 최신 모델들과 비교했을 때 아직 성능 격차가 존재하지만, 이는 다음과 같은 영역에서의 지속적인 개선을 통해 해소될 수 있을 것으로 기대됩니다.

1. 모델 아키텍처의 최적화
2. 학습 데이터의 품질 향상
3. 안전성 프레임워크의 강화

특히 안전성 측면에서는 [Ganguli와 연구진](https://arxiv.org/pdf/2209.07858)이 제안한 레드팀 평가 방법론을 더욱 발전시켜, 모델의 견고성을 지속적으로 개선할 계획입니다.

이러한 종합적인 접근을 통해 Llama 2는 AI 기술의 민주화와 책임있는 발전이라는 두 가지 목표를 동시에 추구하고 있으며, 이는 향후 AI 연구 발전의 중요한 이정표가 될 것으로 기대됩니다.
## 부록 A.1: 기여자 및 감사의 글

### 팀 구성과 주요 기여

Llama 2 프로젝트는 과학/공학 리더십, 기술/관리 리더십, 핵심 기여자, 일반 기여자로 구성된 대규모 팀의 협력으로 이루어졌습니다. 과학/공학 리더십 팀은 Guillem Cucurull, Naman Goyal을 포함한 7명의 연구자들이 맡았으며, 기술/관리 리더십은 Sergey Edunov, Angela Fan 등 6명이 담당했습니다. 핵심 기여자 그룹에는 Peter Albert를 비롯한 23명의 전문가들이 참여했으며, 일반 기여자 그룹에는 17명의 연구자들이 포함되었습니다.

### 주석 작업과 품질 관리

모델의 성능 향상에 핵심적인 역할을 한 인간 주석자들의 공헌이 특별히 강조되었습니다. Eric Alamillo, Tamara Best 등의 주석 작업 리더들은 데이터의 품질 관리를 담당했습니다. 대규모 내부 레드팀은 Dan Bikel, Joanna Bitton 등의 조직자들의 지휘 아래 모델의 안전성과 견고성 향상에 기여했습니다.

## 부록 A.2: 사전학습 세부사항

### 아키텍처 변경사항

Llama 1과 비교하여 가장 주목할 만한 변경사항은 컨텍스트 길이의 확장과 그룹화된 쿼리 어텐션(GQA)의 도입입니다. 컨텍스트 창을 2048 토큰에서 4096 토큰으로 확장함으로써, 더 긴 대화 기록과 문서 이해를 지원할 수 있게 되었습니다.

컨텍스트 길이 확장의 효과는 SCROLLS와 SQUAD 벤치마크를 통해 검증되었습니다. 평균 입력 길이가 3.5k인 SCROLLS에서 현저한 성능 향상이 관찰되었으며, SQUAD에서는 성능 저하 없이 일반적인 작업에서도 강건한 성능을 유지했습니다.

### 그룹화된 쿼리 어텐션(GQA)

자기회귀 디코딩에서는 이전 토큰들의 키(K)와 값(V) 쌍을 캐싱하여 어텐션 계산을 가속화합니다. 그러나 컨텍스트 창이나 배치 크기가 증가하면 다중 헤드 어텐션(MHA) 모델의 KV 캐시 크기가 크게 증가하는 문제가 있습니다. 이를 해결하기 위해 키와 값 프로젝션을 여러 헤드에서 공유하는 방식을 도입했습니다.

GQA 구현에서는 피드포워드 레이어의 차원을 다음과 같이 조정했습니다.
- MQA 변형: \\( 1.33 \\) 배 증가
- GQA 변형: \\( 1.3 \\) 배 증가

이러한 조정을 통해 전체 매개변수 수를 유지하면서도 어텐션 레이어의 감소를 보상할 수 있었습니다.

![GQA 성능 비교](https://ar5iv.org//html/2307.09288/assets/x1.png)
이 그래프는 다중 쿼리 변형이 더 큰 배치 크기에서 더 높은 처리량을 달성하고, 작은 배치에서는 유사한 지연 시간을 보여줌을 나타냅니다. MHA 변형은 256 토큰 컨텍스트에서 1024 배치 크기, 2k 컨텍스트에서 128 배치 크기에서 메모리 부족 오류가 발생한 반면, MQA와 GQA는 이러한 설정에서도 성공적으로 실행되었습니다.

[계속...]
- - -
### References
* [Llama 2: Open Foundation and Fine-Tuned Chat Models](http://arxiv.org/pdf/2307.09288v2)
