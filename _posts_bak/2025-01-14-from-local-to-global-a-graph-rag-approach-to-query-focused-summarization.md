---
layout: post
title: "From Local to Global: A Graph RAG Approach to Query-Focused Summarization"
date: 2024-04-24 18:38:11
author: "Microsoft Research"
categories: "Foundation-Models"
tags: ["Graph-RAG", "Hierarchical-Summarization", "Query-Focused-Summarization", "Retrieval-Augmented-Generation"]
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?
대규모 문서 컬렉션에 대한 포괄적인 이해와 분석이 필요한 현대의 정보 환경에서, 기존의 검색 증강 생성(RAG) 시스템들은 주로 국소적인 정보 검색에 초점을 맞추고 있어 전체 데이터셋에 대한 글로벌 질의에 효과적으로 대응하지 못하는 한계가 있었습니다. 특히 "이 데이터셋의 주요 주제는 무엇인가요?"와 같은 전체적인 맥락을 요구하는 질문에 대해서는 기존 RAG 시스템의 성능이 현저히 저하되었습니다. 또한 질의 중심 요약(QFS) 방식은 대규모 텍스트를 처리하는 데 어려움이 있었습니다. 이러한 한계를 극복하고 문서의 전체적인 맥락을 보존하면서도 효율적인 정보 검색과 요약이 가능한 새로운 접근법의 필요성이 이 연구의 주요 동기가 되었습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?
이 연구는 그래프 RAG(Graph RAG)라는 혁신적인 접근법을 제시합니다. 이 방법은 문서를 단순한 텍스트 청크의 집합이 아닌 의미적으로 연결된 그래프 구조로 표현하며, Leiden 알고리즘을 활용하여 밀접하게 연관된 엔티티들의 커뮤니티를 탐지하고 계층적으로 구조화합니다. 특히 맵-리듀스 패러다임을 도입하여 각 커뮤니티에 대한 독립적인 요약을 생성하고 이를 종합하는 방식으로, 대규모 언어 모델의 컨텍스트 윈도우 제한을 효과적으로 우회하면서도 문서의 전체적인 맥락을 보존할 수 있게 했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?
그래프 RAG의 구현은 크게 세 단계로 이루어집니다. 첫째, 소스 문서로부터 엔티티와 관계를 추출하여 지식 그래프를 구축합니다. 둘째, Leiden 알고리즘을 사용하여 그래프 커뮤니티를 탐지하고 각 커뮤니티에 대한 요약을 생성합니다. 마지막으로, 사용자 질의에 대해 관련 커뮤니티 요약들을 활용하여 응답을 생성합니다. 특히 구현 과정에서는 비동기 처리와 병렬화를 통해 대규모 문서 처리의 효율성을 확보했으며, 캐싱 메커니즘을 도입하여 중간 결과물들을 효과적으로 재사용할 수 있도록 했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?
실험 결과는 그래프 RAG가 기존의 단순 RAG 방식과 비교하여 포괄성(72-83% 승률)과 다양성(75-82% 승률) 측면에서 현저한 성능 향상을 보여주었습니다. 특히 주목할 만한 점은 하위 레벨 커뮤니티 요약이 소스 텍스트 요약 방식보다 26-33% 더 적은 컨텍스트 토큰으로도 더 나은 성능을 달성했다는 것입니다. 이는 그래프 RAG가 대규모 문서 컬렉션의 효율적인 처리와 이해를 위한 실용적인 솔루션이 될 수 있음을 보여줍니다. 또한 이 연구의 성과는 오픈소스로 공개될 예정이어서, 향후 그래프 기반 RAG 시스템의 발전과 응용 연구에 중요한 기여를 할 것으로 기대됩니다.
- - -
## From Local to Global: 그래프 RAG 기반의 질의 중심 요약 접근법

이 연구는 Microsoft Research, Microsoft Strategic Missions and Technologies, 그리고 Microsoft Office of the CTO의 연구진들이 공동으로 수행한 작업입니다. Darren Edge와 Ha Trinh이 공동 제1저자로 참여했으며, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Jonathan Larson이 함께 연구를 진행했습니다.

이 논문은 최근 자연어 처리 분야에서 주목받고 있는 검색 증강 생성(Retrieval-Augmented Generation, RAG) 기술을 그래프 기반 접근법으로 확장하여 질의 중심 요약 작업의 성능을 향상시키는 새로운 방법론을 제시합니다. RAG는 대규모 언어 모델의 생성 능력과 외부 지식 검색을 결합하여 더 정확하고 신뢰할 수 있는 결과를 만들어내는 기술입니다.

특히 이 연구는 로컬 컨텍스트와 글로벌 컨텍스트를 효과적으로 통합하는 방법에 초점을 맞추고 있습니다. 기존의 RAG 시스템들이 주로 단편적인 텍스트 청크를 검색하는 데 그쳤다면, 이 연구에서는 그래프 구조를 활용하여 문서의 전체적인 맥락과 세부적인 정보를 동시에 고려할 수 있는 방법을 제시합니다.

최근 [Gao와 연구진](https://arxiv.org/pdf/2312.10997)이 제시한 RAG 프레임워크의 발전 방향을 기반으로, 이 연구는 그래프 기반의 검색 메커니즘을 도입하여 기존 RAG 시스템의 한계를 극복하고자 했습니다. 특히 [RAPTOR](https://arxiv.org/pdf/2401.18059)에서 제시된 계층적 문서 구조화 방식에서 영감을 받아, 문서의 구조적 특성을 보다 효과적으로 활용할 수 있는 방법을 개발했습니다.

이 연구의 핵심 아이디어는 문서를 단순한 텍스트 청크의 집합이 아닌, 의미적으로 연결된 그래프 구조로 표현하는 것입니다. 이를 통해 질의와 관련된 정보를 더 정확하게 검색하고, 문서의 전체적인 맥락을 유지하면서도 세부적인 정보를 효과적으로 요약할 수 있게 되었습니다.

## 그래프 RAG: 대규모 텍스트 말뭉치에 대한 글로벌 질의응답 시스템

검색 증강 생성(Retrieval-Augmented Generation, RAG)은 대규모 언어 모델이 외부 지식 소스에서 관련 정보를 검색하여 활용함으로써 비공개 문서나 이전에 보지 못한 문서 컬렉션에 대한 질문에 답변할 수 있게 해주는 혁신적인 기술입니다. 하지만 "이 데이터셋의 주요 주제는 무엇인가요?"와 같이 전체 텍스트 말뭉치를 대상으로 하는 글로벌 질문에 대해서는 RAG가 제대로 작동하지 않습니다. 이는 이러한 유형의 질문이 단순한 정보 검색이 아닌 질의 중심 요약(Query-Focused Summarization, QFS) 작업의 성격을 띠기 때문입니다.

반면, 기존의 QFS 방법들은 일반적인 RAG 시스템이 다루는 방대한 양의 텍스트를 처리하는 데 한계가 있습니다. 이러한 상반된 두 방법론의 장점을 결합하기 위해, 연구진은 그래프 RAG(Graph RAG)라는 새로운 접근 방식을 제안했습니다. 이 방법은 사용자 질문의 일반성과 색인해야 할 소스 텍스트의 양 모두에 효과적으로 대응할 수 있습니다.

그래프 RAG는 대규모 언어 모델을 활용하여 두 단계로 그래프 기반 텍스트 색인을 구축합니다. 첫째, 소스 문서로부터 엔티티 지식 그래프를 도출하고, 둘째, 밀접하게 연관된 엔티티들의 모든 그룹에 대해 커뮤니티 요약을 미리 생성합니다. 사용자가 질문을 하면, 각 커뮤니티 요약을 바탕으로 부분 응답을 생성한 후, 이러한 부분 응답들을 다시 종합하여 최종 답변을 만들어냅니다.

연구진은 100만 토큰 규모의 데이터셋에 대한 글로벌 질문들을 대상으로 실험을 수행했습니다. 그 결과, 그래프 RAG가 기본적인 RAG 방식에 비해 생성된 답변의 포괄성과 다양성 측면에서 상당한 개선을 보여주었습니다. 이 연구의 성과는 https://aka.ms/graphrag에서 공개될 예정이며, 글로벌 및 로컬 그래프 RAG 접근법을 모두 구현한 파이썬 기반 오픈소스 코드를 통해 연구 커뮤니티와 공유될 것입니다.

### 서론: 대규모 문서 컬렉션에 대한 글로벌 질의응답의 도전과제

인간의 다양한 활동 영역에서 우리는 방대한 문서 컬렉션을 읽고 이해하며, 때로는 문서에 명시적으로 언급되지 않은 결론까지도 도출해내야 합니다. Klein과 연구진이 정의한 바와 같이, 이러한 센스메이킹(sensemaking)은 "사람, 장소, 사건 간의 연결을 이해하고 이를 통해 향후 진행 방향을 예측하고 효과적으로 행동하기 위한 지속적이고 동기 부여된 노력"입니다.

최근 대규모 언어 모델(Large Language Models, LLMs)의 발전으로 과학적 발견이나 정보 분석과 같은 복잡한 영역에서도 인간과 유사한 센스메이킹 자동화를 시도하고 있습니다. 하지만 전체 문서 컬렉션에 대한 인간 중심의 센스메이킹을 지원하기 위해서는 사용자가 데이터에 대한 자신의 멘탈 모델을 적용하고 개선할 수 있도록 하는 글로벌한 특성의 질문이 가능해야 합니다.

기존의 검색 증강 생성(Retrieval-Augmented Generation, RAG) 방식은 전체 데이터셋에 대한 사용자 질문에 답변하는 데 활용되어 왔습니다. 하지만 RAG는 답변이 텍스트의 특정 영역 내에 국소적으로 존재하고, 그 검색만으로도 생성 작업의 충분한 근거가 되는 상황에 맞춰 설계되었습니다. 이보다 더 적절한 접근 방식은 질의 중심 요약(Query-Focused Summarization, QFS)이며, 특히 단순히 발췌문을 연결하는 것이 아닌 자연어 요약을 생성하는 추상적 방식의 QFS가 필요합니다.

![Graph RAG pipeline using an LLM-derived graph index of source document text](image_path)

이러한 배경에서 본 논문은 LLM 기반의 그래프 인덱스를 활용한 Graph RAG 접근법을 제시합니다. 위 도식에서 보여지듯이, 이 방식은 소스 문서로부터 노드(예: 엔티티), 엣지(예: 관계), 그리고 공변량(예: 주장)을 도메인에 맞게 추출하고 요약합니다. 특히 주목할 만한 점은 그래프의 본질적인 모듈성을 활용한다는 것입니다. Leiden 알고리즘과 같은 커뮤니티 탐지 기법을 사용하여 그래프 인덱스를 밀접하게 연관된 요소들의 그룹으로 분할하고, 이러한 커뮤니티들에 대한 LLM 기반 요약을 통해 전체 문서 컬렉션을 포괄적으로 이해할 수 있게 됩니다.
전체 말뭉치에 대한 질의 중심 요약을 수행할 때의 주요 과제는 LLM의 컨텍스트 윈도우 제한을 극복하는 것입니다. 최근 연구에 따르면, 컨텍스트 윈도우가 확장되더라도 긴 컨텍스트의 중간 부분에서 정보가 "손실"되는 현상이 발생할 수 있습니다. Liu와 연구진은 이러한 "중간 손실(lost in the middle)" 현상이 LLM의 성능을 저하시킬 수 있다고 지적했습니다.

이러한 문제를 해결하기 위해 Graph RAG는 맵-리듀스(map-reduce) 방식의 접근법을 채택합니다. 먼저 각 커뮤니티 요약을 독립적이고 병렬적으로 처리하여 주어진 질의에 대한 답변을 생성하고, 이후 관련된 부분 답변들을 종합하여 최종적인 글로벌 답변을 만들어냅니다. 이러한 방식은 두 가지 중요한 이점을 제공합니다.

1. 정보의 모듈화: Newman이 제시한 그래프의 모듈성 개념을 활용하여, 의미적으로 연관된 정보들을 커뮤니티로 그룹화함으로써 정보의 구조적 특성을 보존합니다.

2. 계층적 처리: Traag와 연구진이 개발한 Leiden 알고리즘을 통해 그래프를 의미 있는 커뮤니티들로 분할하고, 이를 통해 다양한 수준의 추상화가 가능해집니다.

이러한 접근법의 효과를 검증하기 위해, 연구진은 두 가지 실제 데이터셋(팟캐스트 전사본과 뉴스 기사)에 대해 활동 중심의 센스메이킹 질문들을 LLM을 통해 생성하고 평가했습니다. 평가는 포괄성(comprehensiveness), 다양성(diversity), 그리고 임파워먼트(empowerment)라는 세 가지 주요 품질 지표를 기준으로 수행되었으며, 이는 넓은 주제와 테마에 대한 이해도를 측정하는 데 중점을 두었습니다.

특히 연구진은 질의에 답변할 때 사용되는 커뮤니티 요약의 계층적 수준을 다양화하여 그 영향을 탐구했으며, 기존의 단순 RAG 방식 및 소스 텍스트의 글로벌 맵-리듀스 요약과도 비교 분석을 수행했습니다. 실험 결과, 모든 글로벌 접근법이 단순 RAG보다 포괄성과 다양성 측면에서 우수한 성능을 보였으며, 특히 중간 및 낮은 수준의 커뮤니티 요약을 활용한 Graph RAG가 더 적은 토큰 비용으로도 소스 텍스트 요약 방식에 비해 이러한 지표들에서 좋은 성능을 보여주었습니다.

### 그래프 RAG의 접근 방식과 파이프라인

그래프 RAG의 데이터 처리 파이프라인은 크게 세 가지 주요 단계로 구성됩니다. 첫째, 소스 문서로부터 엔티티와 관계를 추출하여 지식 그래프를 구축하는 단계, 둘째, 그래프 커뮤니티를 탐지하고 각 커뮤니티에 대한 요약을 생성하는 단계, 그리고 마지막으로 사용자 질의에 대해 관련 커뮤니티 요약들을 활용하여 응답을 생성하는 단계입니다.

먼저 지식 그래프 구축 단계에서는 대규모 언어 모델(LLM)을 활용하여 소스 문서에서 엔티티와 그들 간의 관계를 추출합니다. 이 과정에서는 문서를 의미 있는 단위로 분할하고, 각 단위에서 엔티티를 식별한 후, 엔티티 간의 관계를 파악합니다. 이때 엔티티 추출과 관계 추출은 병렬적으로 처리되며, 비동기 처리 방식을 통해 처리 효율을 높입니다.

두 번째 단계인 커뮤니티 탐지와 요약 생성에서는 Leiden 알고리즘을 사용하여 그래프 내의 밀접하게 연결된 엔티티 그룹들을 식별합니다. 이 알고리즘은 그래프의 모듈성(modularity)을 최적화하여 의미적으로 연관된 엔티티들을 하나의 커뮤니티로 그룹화합니다. 각 커뮤니티에 대해서는 LLM을 사용하여 해당 커뮤니티의 주요 내용을 요약하는 텍스트를 생성합니다.

마지막 단계인 질의 응답 생성에서는 맵-리듀스(Map-Reduce) 패러다임을 적용합니다. 사용자의 질문이 입력되면, 먼저 각 커뮤니티 요약에 대해 독립적으로 부분 응답을 생성하는 '맵' 단계를 수행합니다. 이후 '리듀스' 단계에서는 이러한 부분 응답들을 종합하여 최종적인 답변을 생성합니다. 이러한 접근 방식은 LLM의 컨텍스트 윈도우 제한을 효과적으로 우회하면서도, 전체 문서 컬렉션의 맥락을 고려한 포괄적인 응답을 생성할 수 있게 합니다.

이러한 파이프라인의 구현은 Python을 기반으로 하며, 비동기 처리와 병렬화를 통해 대규모 문서 처리의 효율성을 확보합니다. 특히 엔티티 추출, 관계 분석, 커뮤니티 탐지 등의 작업은 모두 병렬적으로 처리될 수 있도록 설계되어 있으며, 중간 결과물들은 캐싱되어 재사용될 수 있습니다.
### 그래프 RAG의 접근 방식과 파이프라인 구현 세부사항

그래프 RAG의 파이프라인 구현에서 가장 중요한 기술적 특징은 확장성과 효율성을 고려한 모듈식 설계입니다. 파이프라인의 각 단계는 독립적인 워크플로우로 구현되어 있으며, 이는 `PipelineWorkflowReference` 클래스를 통해 관리됩니다. 각 워크플로우는 고유한 설정과 실행 컨텍스트를 가지며, 비동기 처리를 위한 `async/await` 패턴을 활용합니다.

파이프라인의 설정은 `GraphRagConfig` 클래스를 통해 관리되며, 다음과 같은 주요 구성 요소들을 포함합니다.

```python
class PipelineConfig:
    root_dir: str
    input: PipelineInputConfigTypes
    reporting: PipelineReportingConfigTypes
    storage: PipelineStorageConfigTypes
    cache: PipelineCacheConfigTypes
    workflows: list[PipelineWorkflowReference]
```

텍스트 처리와 그래프 구축 과정에서는 다양한 스토리지 옵션을 지원합니다. 파일 시스템, Blob 스토리지, CosmosDB 등의 스토리지 백엔드를 선택적으로 사용할 수 있으며, 이는 다음과 같은 설정을 통해 구성됩니다.

```python
def _get_storage_config(settings: GraphRagConfig) -> PipelineStorageConfigTypes:
    match settings.storage.type:
        case StorageType.file:
            return PipelineFileStorageConfig(
                base_dir=str(Path(settings.root_dir) / settings.storage.base_dir)
            )
        case StorageType.blob:
            return PipelineBlobStorageConfig(
                connection_string=settings.storage.connection_string,
                container_name=settings.storage.container_name
            )
```

커뮤니티 탐지 과정에서는 Leiden 알고리즘의 수학적 기반이 중요한 역할을 합니다. 그래프 \\(G = (V, E)\\)에서 커뮤니티 구조의 품질은 모듈성 \\(Q\\)로 측정되며, 이는 다음과 같이 정의됩니다.

$$ Q = \frac{1}{2m} \sum_{ij} \left(A_{ij} - \frac{k_i k_j}{2m}\right) \delta(c_i, c_j) $$

여기서:
- \\(A_{ij}\\)는 노드 \\(i\\)와 \\(j\\) 사이의 엣지 가중치
- \\(k_i\\)는 노드 \\(i\\)의 차수
- \\(m\\)은 전체 엣지의 가중치 합
- \\(\delta(c_i, c_j)\\)는 노드 \\(i\\)와 \\(j\\)가 같은 커뮤니티에 속하면 1, 아니면 0

이러한 수학적 기반을 바탕으로, 파이프라인은 `compute_communities` 워크플로우에서 커뮤니티 탐지를 수행하고, 각 커뮤니티에 대한 요약을 생성합니다. 이 과정은 다음과 같이 구현됩니다.

```python
async def run_workflow(
    config: GraphRagConfig,
    context: PipelineRunContext,
    callbacks: WorkflowCallbacks,
) -> pd.DataFrame:
    communities = await compute_communities(
        graph,
        config.cluster_graph,
        context.cache
    )
    return await create_final_community_reports(
        communities,
        config.community_reports
    )
```
### 그래프 RAG의 파이프라인 최적화와 성능 향상 전략

그래프 RAG 파이프라인의 핵심적인 기술적 최적화는 캐싱 메커니즘과 비동기 처리 전략에 있습니다. 캐싱 시스템은 `PipelineCacheConfigTypes` 인터페이스를 통해 구현되며, 메모리 기반 캐시부터 영구 저장소 기반 캐시까지 다양한 옵션을 제공합니다. 특히 중간 처리 결과를 효율적으로 저장하고 재사용함으로써 전체 파이프라인의 성능을 크게 향상시킬 수 있습니다.

```python
def _get_cache_config(settings: GraphRagConfig) -> PipelineCacheConfigTypes:
    match settings.cache.type:
        case CacheType.memory:
            return PipelineMemoryCacheConfig()
        case CacheType.file:
            return PipelineFileCacheConfig(
                base_dir=settings.cache.base_dir
            )
        case CacheType.none:
            return PipelineNoneCacheConfig()
```

엔티티 추출과 관계 분석 과정에서는 LLM의 처리 효율을 높이기 위해 배치 처리 전략을 사용합니다. 이는 다음과 같은 수학적 모델을 기반으로 합니다.

$$ E(B) = \frac{T_{total}}{N} \sum_{i=1}^{N} \left(\frac{L_i}{B} + O\right) $$

여기서:
- \\(E(B)\\)는 배치 크기 \\(B\\)에 대한 기대 처리 시간
- \\(T_{total}\\)은 전체 처리 시간
- \\(N\\)은 전체 문서 수
- \\(L_i\\)는 각 문서의 길이
- \\(O\\)는 오버헤드 상수

이러한 배치 처리 최적화는 `ParallelizationConfig` 클래스를 통해 구현되며, 다음과 같이 설정됩니다.

```python
class ParallelizationConfig:
    num_threads: int
    batch_size: int
    max_retries: int
    retry_delay: float
    timeout: float
```

커뮤니티 요약 생성 과정에서는 계층적 접근 방식을 사용하여 요약의 품질을 개선합니다. 각 커뮤니티는 그래프 구조상의 위치와 크기에 따라 다른 수준의 상세도로 요약됩니다. 이 과정은 정보 엔트로피를 기반으로 한 최적화 문제로 정형화됩니다.

$$ H(C) = -\sum_{i=1}^{|C|} p(c_i) \log p(c_i) $$

여기서:
- \\(H(C)\\)는 커뮤니티 \\(C\\)의 정보 엔트로피
- \\(p(c_i)\\)는 커뮤니티 내 노드 \\(c_i\\)의 상대적 중요도

이러한 수학적 기반을 바탕으로, 파이프라인은 각 커뮤니티의 특성에 맞는 최적의 요약 전략을 동적으로 선택하고 적용합니다.
### 그래프 RAG의 접근 방식과 파이프라인 확장

그래프 RAG의 파이프라인은 유연한 확장성을 위해 플러그인 아키텍처를 채택하고 있습니다. 이는 `WorkflowCallbacks` 인터페이스를 통해 구현되며, 파이프라인의 각 단계에서 발생하는 이벤트를 처리하고 모니터링할 수 있게 합니다. 특히 대규모 문서 처리 과정에서 발생할 수 있는 다양한 예외 상황과 성능 병목 현상을 효과적으로 관리하기 위해, 진행 상황 모니터링과 오류 처리를 위한 콜백 시스템을 제공합니다.

파이프라인의 진행 상황 추적은 `ProgressLogger` 클래스를 통해 구현되며, 이는 다음과 같은 수학적 모델을 기반으로 합니다.

$$ P(t) = \sum_{i=1}^{N} w_i \cdot \frac{c_i(t)}{T_i} $$

여기서:
- \\(P(t)\\)는 시간 \\(t\\)에서의 전체 진행률
- \\(w_i\\)는 각 워크플로우의 가중치
- \\(c_i(t)\\)는 완료된 작업 수
- \\(T_i\\)는 전체 작업 수

이러한 진행 상황 추적 시스템은 다음과 같이 구현됩니다.

```python
async def build_index(
    config: GraphRagConfig,
    run_id: str,
    progress_logger: ProgressLogger,
    callbacks: list[WorkflowCallbacks]
) -> list[PipelineRunResult]:
    workflows = _get_workflows_list(config)
    async for output in run_workflows(
        workflows,
        config,
        callbacks=callbacks,
        logger=progress_logger,
        run_id=run_id
    ):
        if output.errors:
            progress_logger.error(output.workflow)
        else:
            progress_logger.success(output.workflow)
```

또한, 그래프 RAG는 문서 처리 과정에서 발생하는 텍스트 임베딩을 효율적으로 관리하기 위해 계층적 인덱싱 구조를 사용합니다. 이는 다음과 같은 수학적 거리 메트릭을 기반으로 합니다.

$$ d(x, y) = \sqrt{\sum_{i=1}^{n} \alpha_i(x_i - y_i)^2} $$

여기서:
- \\(d(x, y)\\)는 두 임베딩 벡터 간의 가중치 유클리드 거리
- \\(\alpha_i\\)는 각 차원의 중요도를 나타내는 가중치
- \\(x_i, y_i\\)는 임베딩 벡터의 각 차원 값

이러한 임베딩 관리 시스템은 효율적인 유사도 검색을 가능하게 하며, 이는 질의 처리 과정에서 관련 커뮤니티를 빠르게 식별하는 데 핵심적인 역할을 합니다.

### 소스 문서에서 텍스트 청크로의 변환

그래프 RAG 시스템의 첫 번째 핵심 설계 결정은 소스 문서에서 추출한 텍스트를 어떤 크기의 청크(chunk)로 분할할 것인지 결정하는 것입니다. 이는 시스템의 성능과 효율성에 직접적인 영향을 미치는 중요한 요소입니다. 각각의 텍스트 청크는 그래프 인덱스의 다양한 요소들을 추출하기 위해 설계된 LLM 프롬프트들에 입력으로 전달됩니다.

텍스트 청크 크기 선택에는 다음과 같은 트레이드오프가 존재합니다. 더 긴 텍스트 청크를 사용하면 LLM 호출 횟수를 줄일 수 있어 처리 효율성이 향상됩니다. 하지만 [Liu와 연구진](https://arxiv.org/pdf/2307.03172)과 [Kuratov와 연구진](https://arxiv.org/pdf/2402.10790v2)이 지적한 것처럼, LLM의 컨텍스트 윈도우가 길어질수록 정보 회수(recall) 성능이 저하되는 문제가 발생합니다.

이러한 현상은 HotPotQA 데이터셋을 사용한 실험 결과에서 명확하게 관찰됩니다. 단일 추출 라운드(gleaning 횟수가 0인 경우)에서, 600 토큰 크기의 청크를 사용했을 때는 2400 토큰 크기의 청크를 사용했을 때보다 거의 두 배 많은 엔티티 참조를 추출할 수 있었습니다. 이는 다음 그래프에서 확인할 수 있습니다.

![HotPotQA 데이터셋에서 청크 크기와 gleaning에 따른 엔티티 참조 검출 수의 변화](image_path)

이 그래프는 gpt-4-turbo 모델을 사용하여 일반적인 엔티티 추출 프롬프트로 실험한 결과를 보여줍니다. 세 가지 다른 청크 크기(600, 1200, 2400 토큰)에 대해 gleaning 횟수를 증가시키면서 검출된 엔티티 참조의 수를 측정했습니다.

더 많은 엔티티 참조를 추출하는 것이 일반적으로 더 나은 결과를 제공하지만, 모든 추출 프로세스는 목표 작업에 맞게 회수율(recall)과 정밀도(precision) 사이의 균형을 맞춰야 합니다. 이를 위해 연구진은 "gleaning"이라는 반복적 추출 방식을 도입했습니다.

### 텍스트 청크에서 요소 인스턴스로의 변환

텍스트 청크에서 그래프 요소를 추출하는 기본적인 요구사항은 각 텍스트 청크에서 그래프 노드와 엣지의 인스턴스들을 식별하고 추출하는 것입니다. 이를 위해 연구진은 다단계 LLM 프롬프트를 사용합니다. 먼저 텍스트에서 모든 엔티티를 식별하는데, 이때 엔티티의 이름, 유형, 설명을 함께 추출합니다. 그 다음 명확하게 연관된 엔티티들 간의 모든 관계를 식별하며, 이 과정에서 소스 엔티티, 타겟 엔티티, 그리고 그들의 관계에 대한 설명을 추출합니다. 이렇게 추출된 두 종류의 요소 인스턴스는 구분자로 분리된 튜플 리스트 형태로 출력됩니다.

[Brown과 연구진](https://arxiv.org/pdf/2005.14165v4)이 제시한 in-context learning의 개념을 활용하여, 문서 말뭉치의 도메인에 맞게 프롬프트를 조정할 수 있습니다. 예를 들어, 기본 프롬프트는 사람, 장소, 조직과 같은 일반적인 "명명된 엔티티"를 추출하도록 설계되어 있지만, 과학, 의학, 법률과 같은 전문 지식이 필요한 도메인에서는 해당 도메인에 특화된 few-shot 예제들을 사용하여 성능을 향상시킬 수 있습니다.
추출된 엔티티 노드와 함께, 연구진은 추가적인 공변량(covariate)을 추출하기 위한 보조 프롬프트도 지원합니다. 기본 공변량 프롬프트는 검출된 엔티티와 연결된 주장(claim)들을 추출하는 것을 목표로 하며, 여기에는 주체(subject), 객체(object), 유형(type), 설명(description), 소스 텍스트 범위(source text span), 그리고 시작 및 종료 날짜가 포함됩니다.

효율성과 품질의 균형을 맞추기 위해, 연구진은 "gleaning"이라는 다중 라운드 추출 방식을 도입했습니다. 이는 지정된 최대 횟수까지 반복적으로 LLM이 이전 추출 라운드에서 놓쳤을 수 있는 추가 엔티티들을 검출하도록 유도하는 과정입니다. 이 다단계 프로세스는 다음과 같이 작동합니다.

1. LLM에게 모든 엔티티가 추출되었는지 평가하도록 요청합니다. 이때 yes/no 결정을 강제하기 위해 100의 logit bias를 사용합니다.

2. LLM이 엔티티들이 누락되었다고 응답하면, "이전 추출에서 많은 엔티티들이 누락되었습니다"라는 연속 프롬프트를 통해 LLM이 이러한 누락된 엔티티들을 추가로 찾아내도록 유도합니다.

이러한 gleaning 접근 방식의 효과는 앞서 보여진 그래프에서 명확하게 확인할 수 있습니다. gleaning을 적용함으로써 더 큰 청크 크기를 사용하더라도 품질 저하 없이 엔티티를 추출할 수 있으며, 불필요한 노이즈 도입도 방지할 수 있습니다. 특히 2400 토큰 크기의 청크를 사용하는 경우에도 gleaning을 통해 600 토큰 청크와 비슷한 수준의 엔티티 참조를 추출할 수 있게 되었습니다.

이러한 gleaning 메커니즘은 LLM의 컨텍스트 윈도우 한계를 효과적으로 극복하면서도, 처리 효율성을 유지할 수 있게 해주는 혁신적인 접근 방식입니다. 또한 이는 도메인 특화된 엔티티 추출에서도 유용하게 활용될 수 있으며, 특히 복잡한 전문 분야의 문서에서 중요한 정보를 누락 없이 추출하는 데 큰 도움이 됩니다.

### 요소 인스턴스에서 요소 요약으로의 변환

대규모 언어 모델(LLM)을 활용하여 소스 텍스트에서 엔티티, 관계, 그리고 주장들을 "추출"하는 과정은 그 자체로 추상적 요약(abstractive summarization)의 한 형태입니다. 이는 텍스트에 명시적으로 언급되지 않았지만 암시된 개념들(예: 암묵적 관계)에 대해서도 LLM이 독립적으로 의미 있는 요약을 생성할 수 있다는 점에서 주목할 만합니다.

이러한 인스턴스 수준의 요약들을 그래프의 각 요소(엔티티 노드, 관계 엣지, 주장 공변량)에 대한 단일 설명 텍스트 블록으로 변환하기 위해서는 LLM을 통한 추가적인 요약 단계가 필요합니다. 이 과정에서 발생할 수 있는 잠재적 문제점은 LLM이 동일한 엔티티를 항상 일관된 텍스트 형식으로 추출하지 않을 수 있다는 것입니다. 이로 인해 엔티티 그래프에 중복된 엔티티 요소와 노드가 생성될 수 있습니다.

하지만 연구진의 접근 방식은 이러한 변동성에 대해 탄력적으로 대응할 수 있습니다. 그 이유는 다음 단계에서 밀접하게 연관된 엔티티들의 모든 "커뮤니티"가 탐지되고 요약되며, LLM이 여러 이름 변형 뒤에 있는 공통 엔티티를 이해할 수 있기 때문입니다. 이는 모든 변형들이 밀접하게 연관된 엔티티들의 공유 집합과 충분한 연결성을 가지고 있다는 전제 하에 가능합니다.

이러한 접근 방식의 특징은 잠재적으로 노이즈가 있는 그래프 구조에서 균일한 노드들에 대해 풍부한 설명 텍스트를 사용한다는 점입니다. 이는 LLM의 능력과 글로벌 수준의 질의 중심 요약의 요구사항에 모두 부합합니다. 또한 이러한 특성은 연구진의 그래프 인덱스를 전통적인 지식 그래프와 차별화합니다. 기존의 지식 그래프는 주로 간결하고 일관된 지식 트리플(주어, 술어, 목적어)을 사용하여 다운스트림 추론 작업을 수행하는 데 중점을 둡니다.

![Leiden 알고리즘을 사용하여 MultiHop-RAG 데이터셋에서 탐지된 그래프 커뮤니티. (a) 레벨 0의 루트 커뮤니티들 (b) 레벨 1의 서브 커뮤니티들](/html/2404.16130/assets/Level0Multihop.jpg)
![Leiden 알고리즘을 사용하여 MultiHop-RAG 데이터셋에서 탐지된 그래프 커뮤니티. (a) 레벨 0의 루트 커뮤니티들 (b) 레벨 1의 서브 커뮤니티들](/html/2404.16130/assets/Level1Multihop.jpg)

위 그림은 [Traag와 연구진](https://arxiv.org/pdf/1810.08473v3)이 개발한 Leiden 알고리즘을 사용하여 [Tang과 Yang](https://arxiv.org/pdf/2401.15391)의 MultiHop-RAG 데이터셋에서 탐지된 커뮤니티 구조를 보여줍니다. 원의 크기는 해당 엔티티 노드의 차수(degree)에 비례합니다. 노드 레이아웃은 [Martin과 연구진](https://arxiv.org/pdf/0906.0612v2)의 OpenORD와 [Jacomy와 연구진](https://arxiv.org/pdf/0906.0612v2)의 Force Atlas 2 알고리즘을 사용하여 생성되었습니다. 노드의 색상은 엔티티 커뮤니티를 나타내며, 두 가지 수준의 계층적 클러스터링을 보여줍니다. (a) 최대 모듈성을 가진 계층적 분할에 해당하는 레벨 0과 (b) 이러한 루트 레벨 커뮤니티 내의 내부 구조를 보여주는 레벨 1입니다.

### 그래프 커뮤니티 구조 분석과 Leiden 알고리즘

이전 단계에서 생성된 인덱스는 동종 무방향 가중 그래프(homogeneous undirected weighted graph)로 모델링됩니다. 이 그래프에서 엔티티들은 노드로 표현되고, 이들 간의 관계는 엣지로 연결됩니다. 각 엣지의 가중치는 탐지된 관계 인스턴스의 정규화된 빈도수를 나타냅니다. 이러한 그래프 구조에서 커뮤니티를 탐지하기 위해 연구진은 Leiden 알고리즘을 채택했습니다.

Leiden 알고리즘은 [Traag와 연구진](https://arxiv.org/pdf/1810.08473v3)이 개발한 커뮤니티 탐지 알고리즘으로, 대규모 그래프의 계층적 커뮤니티 구조를 효율적으로 식별할 수 있습니다. 이 알고리즘의 핵심은 모듈성(modularity) 최적화에 있으며, 다음과 같은 수식으로 표현됩니다.

$$ Q = \frac{1}{2m} \sum_{ij} \left(A_{ij} - \frac{k_i k_j}{2m}\right) \delta(c_i, c_j) $$

여기서:
- \\(A_{ij}\\)는 노드 \\(i\\)와 \\(j\\) 사이의 엣지 가중치
- \\(k_i\\)는 노드 \\(i\\)의 차수(degree)
- \\(m\\)은 전체 엣지의 가중치 합
- \\(\delta(c_i, c_j)\\)는 노드 \\(i\\)와 \\(j\\)가 같은 커뮤니티에 속하면 1, 아니면 0을 반환하는 크로네커 델타 함수

Leiden 알고리즘은 이 모듈성 값을 최적화하면서 그래프를 여러 계층의 커뮤니티로 분할합니다. 각 계층에서 커뮤니티 분할은 상호 배타적(mutually-exclusive)이면서 전체를 포괄(collective-exhaustive)하는 특성을 가집니다. 이는 분할 정복(divide-and-conquer) 방식의 글로벌 요약을 가능하게 하는 중요한 특성입니다.

![Leiden 알고리즘을 사용하여 MultiHop-RAG 데이터셋에서 탐지된 그래프 커뮤니티. (a) 레벨 0의 루트 커뮤니티들 (b) 레벨 1의 서브 커뮤니티들](/html/2404.16130/assets/Level0Multihop.jpg)
![Leiden 알고리즘을 사용하여 MultiHop-RAG 데이터셋에서 탐지된 그래프 커뮤니티. (a) 레벨 0의 루트 커뮤니티들 (b) 레벨 1의 서브 커뮤니티들](/html/2404.16130/assets/Level1Multihop.jpg)

위 그림은 MultiHop-RAG 데이터셋에 Leiden 알고리즘을 적용한 결과를 보여줍니다. 각 원의 크기는 해당 엔티티 노드의 차수에 비례하며, 노드의 색상은 서로 다른 커뮤니티를 나타냅니다. 레벨 0은 최상위 커뮤니티 구조를, 레벨 1은 각 상위 커뮤니티 내의 세부 구조를 보여줍니다. 이러한 계층적 구조는 문서의 의미적 구조를 다양한 추상화 수준에서 파악할 수 있게 해줍니다.

### 그래프 커뮤니티에서 커뮤니티 요약으로

그래프 RAG 시스템의 다음 단계는 Leiden 계층 구조의 각 커뮤니티에 대해 보고서 형식의 요약을 생성하는 것입니다. 이 과정은 대규모 데이터셋을 효율적으로 처리할 수 있도록 설계되었습니다. 이러한 커뮤니티 요약은 데이터셋의 전체적인 구조와 의미를 이해하는 데 독립적으로도 유용하며, 특정 질문 없이도 말뭉치를 이해하는 데 활용될 수 있습니다.

예를 들어, 사용자는 상위 수준에서 커뮤니티 요약을 훑어보며 관심 있는 일반적인 주제를 찾고, 각 하위 주제에 대한 더 자세한 정보를 제공하는 하위 수준의 보고서로 연결되는 링크를 따라갈 수 있습니다. 이 연구에서는 특히 글로벌 질의에 답변하기 위한 그래프 기반 인덱스의 일부로서 이러한 커뮤니티 요약의 유용성에 초점을 맞추고 있습니다.

커뮤니티 요약은 다음과 같은 방식으로 생성됩니다.

리프 레벨 커뮤니티의 경우, 요소 요약(노드, 엣지, 공변량)은 우선순위가 매겨진 후 LLM 컨텍스트 윈도우의 토큰 제한에 도달할 때까지 반복적으로 추가됩니다. 우선순위는 소스 노드와 타겟 노드의 결합 차수(전체적인 중요도)를 기준으로 내림차순으로 정렬되며, 각 커뮤니티 엣지에 대해 소스 노드, 타겟 노드, 연결된 공변량, 그리고 엣지 자체에 대한 설명이 추가됩니다.

상위 레벨 커뮤니티의 경우, 모든 요소 요약이 컨텍스트 윈도우의 토큰 제한 내에 들어간다면 리프 레벨 커뮤니티와 동일한 방식으로 커뮤니티 내의 모든 요소 요약을 처리합니다. 그러나 제한을 초과하는 경우, 서브 커뮤니티들을 요소 요약 토큰 수를 기준으로 내림차순으로 정렬하고, 컨텍스트 윈도우 내에 맞출 수 있을 때까지 서브 커뮤니티 요약(더 짧은)을 해당 요소 요약(더 긴)으로 반복적으로 대체합니다.

이러한 접근 방식은 [Liu와 연구진](https://arxiv.org/pdf/2307.03172)이 지적한 "중간 손실(lost in the middle)" 현상을 효과적으로 해결합니다. 긴 컨텍스트의 중간 부분에서 정보가 손실되는 문제를 피하기 위해, 계층적 구조를 활용하여 정보를 더 효율적으로 조직화하고 처리합니다.

### 커뮤니티 요약에서 글로벌 답변으로의 변환

커뮤니티 요약을 활용한 최종 답변 생성은 다단계 프로세스를 통해 이루어집니다. 이 과정에서는 커뮤니티의 계층적 구조를 활용하여 다양한 수준의 요약 정보를 효과적으로 통합합니다. 특히 사용자의 질의에 대해 가장 적절한 수준의 커뮤니티 요약을 선택하고 활용하는 것이 중요한데, 이는 3장에서 자세히 평가됩니다.

주어진 커뮤니티 수준에서 글로벌 답변을 생성하는 과정은 다음과 같은 단계로 구성됩니다.

첫째, 커뮤니티 요약 준비 단계에서는 커뮤니티 요약들을 무작위로 섞은 후 사전에 지정된 토큰 크기의 청크로 분할합니다. 이는 관련 정보가 단일 컨텍스트 윈도우에 집중되어 손실되는 것을 방지하고, 정보가 청크들 사이에 고르게 분산되도록 하기 위함입니다. 이 과정은 다음과 같은 수식으로 표현될 수 있습니다.

$$ C = \{c_1, c_2, ..., c_n\} \rightarrow \{S_1, S_2, ..., S_k\} $$

여기서 \\(C\\)는 전체 커뮤니티 요약의 집합이고, \\(S_i\\)는 각각의 토큰 크기 제한을 만족하는 요약 청크입니다.

둘째, 커뮤니티 답변 매핑 단계에서는 각 청크에 대해 병렬적으로 중간 답변을 생성합니다. 이때 대규모 언어 모델은 각 답변에 대해 0-100 사이의 유용성 점수도 함께 생성하도록 지시받습니다. 이 점수는 해당 답변이 목표 질문에 얼마나 도움이 되는지를 나타내며, 점수가 0인 답변은 필터링됩니다. 이 과정은 다음과 같이 표현됩니다.

$$ R_i = f_{LLM}(S_i, q), \quad s_i = g_{LLM}(R_i, q) $$

여기서 \\(f_{LLM}\\)은 답변 생성 함수, \\(g_{LLM}\\)은 점수 산출 함수, \\(q\\)는 사용자 질의, \\(R_i\\)는 생성된 중간 답변, \\(s_i\\)는 해당 답변의 유용성 점수입니다.

마지막으로, 글로벌 답변으로의 축소 단계에서는 중간 답변들을 유용성 점수에 따라 내림차순으로 정렬한 후, 토큰 제한에 도달할 때까지 순차적으로 새로운 컨텍스트 윈도우에 추가합니다. 이렇게 구성된 최종 컨텍스트를 바탕으로 사용자에게 반환될 글로벌 답변이 생성됩니다.

$$ A_{global} = h_{LLM}(\{R_i | s_i > 0\}, q) $$

여기서 \\(h_{LLM}\\)은 최종 답변 생성 함수이며, \\(\{R_i | s_i > 0\}\\)는 점수가 0보다 큰 모든 중간 답변의 집합입니다.

이러한 다단계 접근 방식은 [Liu와 연구진](https://arxiv.org/pdf/2307.03172)이 지적한 "중간 손실(lost in the middle)" 현상을 효과적으로 해결합니다. 특히 정보의 계층적 구조화와 병렬 처리를 통해 대규모 언어 모델의 컨텍스트 윈도우 제한을 우회하면서도, 전체 문서 컬렉션의 맥락을 고려한 포괄적인 답변을 생성할 수 있게 됩니다.

### 평가 방법론과 실험 결과

이 연구에서는 두 가지 주요 데이터셋을 사용하여 Graph RAG 시스템의 성능을 평가했습니다. 첫 번째 데이터셋은 Microsoft CTO인 Kevin Scott와 다른 기술 리더들 간의 팟캐스트 대화 전사본으로, 약 100만 토큰 규모(약 10권의 소설에 해당)의 텍스트로 구성되어 있습니다. 이 데이터셋은 1,669개의 600토큰 크기 텍스트 청크로 분할되었으며, 각 청크 간에는 100토큰의 중첩이 있습니다.

두 번째 데이터셋은 2013년 9월부터 2023년 12월까지 발행된 뉴스 기사들로 구성된 벤치마크 데이터셋입니다. 엔터테인먼트, 비즈니스, 스포츠, 기술, 건강, 과학 등 다양한 카테고리의 기사를 포함하고 있으며, 3,197개의 600토큰 크기 텍스트 청크(약 170만 토큰)로 구성되어 있습니다.

연구진은 기존의 오픈 도메인 질의응답 벤치마크 데이터셋들(HotPotQA, MultiHop-RAG, MT-Bench 등)이 단순한 사실 검색에 초점을 맞추고 있어, 데이터 센스메이킹을 위한 요약 작업을 평가하기에는 적합하지 않다고 판단했습니다. 이에 활동 중심의 접근 방식을 통해 자동으로 질문을 생성하는 방법을 개발했습니다.

구체적으로, 대규모 언어 모델(LLM)을 활용하여 다음과 같은 단계로 평가용 질문을 생성했습니다.

1. 데이터셋에 대한 간단한 설명을 기반으로 N명의 잠재적 사용자를 식별
2. 각 사용자별로 N개의 작업을 생성
3. 각 (사용자, 작업) 조합에 대해 N개의 질문을 생성

이때 N=5로 설정하여 각 데이터셋당 125개의 테스트 질문을 생성했습니다. 생성된 질문들은 특정 텍스트의 세부 사항이 아닌 전체 말뭉치에 대한 이해를 요구하도록 설계되었습니다.

실험에서는 총 6가지 조건을 비교했습니다.

1. C0: 루트 레벨 커뮤니티 요약(가장 적은 수의 요약)을 사용
2. C1: 상위 레벨 커뮤니티 요약(C0의 하위 커뮤니티)을 사용
3. C2: 중간 레벨 커뮤니티 요약을 사용
4. C3: 하위 레벨 커뮤니티 요약(가장 많은 수의 요약)을 사용
5. TS: 맵-리듀스 접근법을 소스 텍스트에 직접 적용
6. SS: 기본적인 "시맨틱 검색" RAG 접근법

모든 조건에서 컨텍스트 윈도우의 크기와 답변 생성을 위한 프롬프트는 동일하게 유지되었으며, 조건들 간의 유일한 차이점은 컨텍스트 윈도우의 내용을 어떻게 구성하는지에 있었습니다.
평가 지표와 실험 구성에 있어서, 연구진은 대규모 언어 모델(LLM)을 평가자로 활용하는 혁신적인 접근 방식을 채택했습니다. [Wang과 연구진](https://arxiv.org/pdf/2307.03172)과 [Zheng과 연구진](https://arxiv.org/pdf/2402.10790v2)의 연구에서 입증된 바와 같이, LLM은 자연어 생성 평가에서 인간 평가자와 비슷하거나 더 나은 수준의 성능을 보여줄 수 있습니다.

연구진은 네 가지 핵심 평가 지표를 설정했습니다.

1. 포괄성(Comprehensiveness): 답변이 질문의 모든 측면과 세부사항을 얼마나 잘 다루는지 평가합니다.
2. 다양성(Diversity): 답변이 질문에 대한 다양한 관점과 통찰을 얼마나 풍부하게 제공하는지 측정합니다.
3. 임파워먼트(Empowerment): 답변이 독자의 이해를 돕고 정보에 기반한 판단을 내리는 데 얼마나 도움이 되는지 평가합니다.
4. 직접성(Directness): 답변이 질문에 얼마나 구체적이고 명확하게 대응하는지 측정합니다.

이러한 평가는 LLM이 질문, 평가 지표, 그리고 한 쌍의 답변을 받아 어떤 답변이 해당 지표에서 더 우수한지 판단하는 방식으로 진행됩니다. 평가의 신뢰성을 높이기 위해 각 비교는 5회 반복되어 평균 점수를 산출했습니다.

실험 구성에서 주목할 만한 점은 컨텍스트 윈도우 크기의 영향에 대한 분석입니다. gpt-4-turbo와 같이 128k 토큰의 큰 컨텍스트 크기를 가진 모델에서도, [Liu와 연구진](https://arxiv.org/pdf/2307.03172)이 지적한 "중간 손실(lost in the middle)" 현상을 고려하여 다양한 컨텍스트 윈도우 크기(8k, 16k, 32k, 64k)를 테스트했습니다.

흥미롭게도, 가장 작은 컨텍스트 윈도우 크기인 8k가 포괄성 측면에서 모든 비교에서 가장 우수한 성능(평균 승률 58.1%)을 보였으며, 다양성(평균 승률 52.4%)과 임파워먼트(평균 승률 51.3%) 측면에서도 더 큰 컨텍스트 크기와 비슷한 성능을 보였습니다. 이러한 결과를 바탕으로 최종 평가에서는 8k 토큰의 고정된 컨텍스트 윈도우 크기를 사용했습니다.

실험 결과를 살펴보면, 팟캐스트 데이터셋에서는 8,564개의 노드와 20,691개의 엣지로 구성된 그래프가 생성되었고, 뉴스 데이터셋에서는 15,754개의 노드와 19,520개의 엣지를 가진 더 큰 그래프가 생성되었습니다. 이러한 그래프 구조는 각각 다른 레벨의 커뮤니티 요약을 생성하는 기반이 되었습니다.
실험 결과를 자세히 살펴보면, Graph RAG의 성능이 기존의 접근 방식들과 비교하여 여러 측면에서 우수함을 확인할 수 있습니다. 특히 글로벌 접근법(Graph RAG의 모든 조건과 텍스트 요약 방식)이 단순 RAG 방식보다 포괄성과 다양성 측면에서 일관되게 더 나은 성능을 보여주었습니다.

팟캐스트 전사본 데이터셋에서는 글로벌 접근법이 포괄성 측면에서 72-83%의 승률을, 다양성 측면에서는 75-82%의 승률을 기록했습니다. 뉴스 기사 데이터셋에서도 비슷한 패턴이 관찰되어, 포괄성에서 72-80%, 다양성에서 62-71%의 승률을 보였습니다. 이는 Graph RAG가 문서의 전체적인 맥락을 더 잘 파악하고 통합할 수 있음을 보여줍니다.

연구진이 직접성(Directness)을 검증 지표로 활용한 것은 매우 흥미로운 전략적 선택이었습니다. 예상대로 단순 RAG 방식이 모든 비교에서 가장 직접적인 응답을 생성했는데, 이는 이 지표가 의도한 대로 작동하고 있음을 입증합니다. 즉, 단순 RAG는 질문에 가장 직접적으로 관련된 정보만을 검색하여 응답하는 반면, Graph RAG는 더 넓은 맥락을 고려하여 포괄적이고 다양한 관점을 제공할 수 있다는 것을 보여줍니다.

커뮤니티 요약과 소스 텍스트 비교에서도 주목할 만한 결과가 나타났습니다. 루트 레벨 요약을 제외한 모든 커뮤니티 요약 수준에서 텍스트 요약 방식보다 일관되게 더 나은 성능을 보였습니다. 특히 팟캐스트 데이터셋에서는 중간 레벨 요약이 57%의 포괄성 승률을, 뉴스 데이터셋에서는 하위 레벨 커뮤니티 요약이 64%의 포괄성 승률을 달성했습니다. 다양성 측면에서도 팟캐스트 데이터셋의 중간 레벨 요약이 57%, 뉴스 데이터셋의 하위 레벨 커뮤니티 요약이 60%의 승률을 기록했습니다.

Graph RAG의 가장 큰 장점 중 하나는 확장성입니다. 소스 텍스트 요약 방식과 비교했을 때, 하위 레벨 커뮤니티 요약(C3)은 26-33% 더 적은 컨텍스트 토큰을 필요로 했으며, 루트 레벨 커뮤니티 요약(C0)의 경우 97% 이상 적은 토큰으로도 작동이 가능했습니다. 특히 루트 레벨 Graph RAG는 다른 글로벌 방법들에 비해 성능이 다소 떨어지더라도, 단순 RAG에 비해 여전히 포괄성(72% 승률)과 다양성(62% 승률)에서 우위를 보이면서도 매우 효율적인 반복적 질의응답을 가능하게 합니다.

임파워먼트 측면에서의 결과는 다소 혼재된 양상을 보였습니다. 단순 RAG와의 비교에서나 소스 텍스트 요약과의 비교에서도 명확한 우위를 보이지는 않았습니다. LLM의 평가 근거를 분석한 결과, 구체적인 예시, 인용구, 그리고 출처 표시가 사용자의 정보에 기반한 이해를 돕는 데 핵심적인 요소로 판단되었습니다. 이는 Graph RAG의 엔티티 추출 프롬프트를 조정하여 이러한 세부 정보들을 더 잘 보존하도록 개선할 여지가 있음을 시사합니다.

### 관련 연구

#### RAG 접근법과 시스템

검색 증강 생성(Retrieval-Augmented Generation, RAG)은 대규모 언어 모델(LLM)을 사용할 때 외부 데이터 소스에서 관련 정보를 먼저 검색한 후, 이를 원래 질의와 함께 LLM의 컨텍스트 윈도우에 추가하는 방식입니다. [Ram과 연구진](https://arxiv.org/abs/2312.10997)이 설명한 기본적인 RAG 접근법은 문서를 텍스트로 변환하고, 이를 청크로 분할한 뒤, 이러한 청크들을 의미적으로 유사한 위치가 유사한 벡터를 갖는 벡터 공간에 임베딩하는 방식으로 작동합니다. 질의는 동일한 벡터 공간에 임베딩되며, 가장 가까운 k개의 벡터에 해당하는 텍스트 청크들이 컨텍스트로 사용됩니다.

더 발전된 RAG 시스템들은 기본 RAG의 한계를 극복하기 위해 사전 검색, 검색, 사후 검색 전략을 포함하며, 모듈형 RAG 시스템은 반복적이고 동적인 검색-생성 사이클을 위한 패턴을 포함합니다. 본 연구의 Graph RAG 구현은 다른 시스템들과 관련된 여러 개념을 통합했습니다. 예를 들어, 커뮤니티 요약은 [Cheng과 연구진](https://arxiv.org/abs/2401.18059)이 제안한 자체 메모리(self-memory)의 일종으로, [Mao와 연구진](https://arxiv.org/abs/2009.08553)이 개발한 생성 증강 검색(Generation-Augmented Retrieval, GAR)을 통해 향후 생성 사이클을 용이하게 합니다.

또한 이러한 요약으로부터 커뮤니티 답변을 병렬적으로 생성하는 방식은 [Shao와 연구진](https://arxiv.org/abs/2305.15294)의 반복적 검색-생성(Iter-RetGen) 또는 [Wang과 연구진](https://arxiv.org/abs/2402.11891)의 연합 검색(FeB4RAG) 전략과 유사합니다. 다른 시스템들도 이러한 개념들을 다중 문서 요약([Su와 연구진](https://arxiv.org/abs/2004.03074)의 CAiRE-COVID)이나 다중 홉 질의응답([Feng과 연구진](https://arxiv.org/abs/2305.15294)의 ITRG, [Trivedi와 연구진](https://arxiv.org/abs/2112.10509)의 IR-CoT, [Khattab과 연구진](https://arxiv.org/abs/2212.14024)의 DSP)에 활용했습니다.

본 연구의 계층적 인덱스와 요약 방식은 텍스트 임베딩의 벡터를 클러스터링하여 계층적 인덱스를 생성하는 [Sarthi와 연구진](https://arxiv.org/abs/2401.18059)의 RAPTOR나, 모호한 질문의 다양한 해석을 답변하기 위해 "명확화 트리"를 생성하는 [Kim과 연구진](https://arxiv.org/abs/2310.14696)의 접근법과 유사점을 가집니다. 하지만 이러한 반복적이거나 계층적인 접근법들 중 어느 것도 Graph RAG가 활용하는 것과 같은 자체 생성 그래프 인덱스는 사용하지 않습니다.

#### 그래프와 LLM

LLM과 RAG와 관련된 그래프 활용은 여러 방향으로 발전하고 있는 연구 분야입니다. 여기에는 [Trajanoska와 연구진](https://arxiv.org/abs/2305.15294)의 지식 그래프 생성과 [Yao와 연구진](https://arxiv.org/abs/2305.15294)의 지식 그래프 완성, 그리고 [Ban과 연구진](https://arxiv.org/abs/2305.15294)과 [Zhang과 연구진](https://arxiv.org/abs/2305.15294)의 인과 그래프 추출 연구가 포함됩니다.

또한 [Gao와 연구진](https://arxiv.org/abs/2312.10997)이 제시한 고급 RAG의 형태로, [Baek과 연구진](https://arxiv.org/abs/2306.04136)의 KAPING처럼 인덱스가 지식 그래프인 경우, [He와 연구진](https://arxiv.org/abs/2402.07630)의 G-Retriever처럼 그래프 구조의 부분집합을 조회하는 경우, [Zhang](https://arxiv.org/abs/2305.15294)의 Graph-ToolFormer처럼 그래프 메트릭을 활용하는 경우 등이 있습니다.

[Kang과 연구진](https://arxiv.org/abs/2305.18846)의 SURGE는 검색된 서브그래프의 사실에 강하게 근거한 내러티브 출력을 생성하며, [Ranade와 Joshi](https://arxiv.org/abs/2305.15294)의 FABULA는 검색된 이벤트-플롯 서브그래프를 내러티브 템플릿을 사용해 직렬화합니다. [Wang과 연구진](https://arxiv.org/abs/2305.15294)은 다중 홉 질의응답을 위한 텍스트-관계 그래프의 생성과 탐색을 모두 지원하는 시스템을 개발했습니다.

오픈소스 소프트웨어 측면에서는 [LangChain](https://github.com/langchain-ai/langchain)과 [LlamaIndex](https://github.com/run-llama/llama_index) 라이브러리가 다양한 그래프 데이터베이스를 지원하며, Neo4J의 [NaLLM](https://github.com/neo4j/NaLLM)과 NebulaGraph의 [GraphRAG](https://github.com/nebula-contrib/GraphRAG)를 포함한 더 일반적인 그래프 기반 RAG 애플리케이션도 등장하고 있습니다. 하지만 이러한 시스템들 중 어느 것도 본 연구의 Graph RAG처럼 그래프의 자연스러운 모듈성을 활용하여 전역 요약을 위한 데이터 분할을 수행하지는 않습니다.

### 관련 연구 분석

이 연구의 관련 연구는 크게 두 가지 주요 영역으로 나눌 수 있습니다. 첫째는 검색 증강 생성(RAG) 접근법과 시스템에 관한 연구이고, 둘째는 대규모 언어 모델(LLM)과 그래프를 결합한 연구입니다.

검색 증강 생성 분야에서는 [Gao와 연구진](https://arxiv.org/pdf/2312.10997)이 제시한 포괄적인 RAG 프레임워크가 주목할 만합니다. 이들은 RAG 시스템의 세 가지 핵심 구성 요소인 검색, 생성, 증강 기술을 체계적으로 분석했습니다. 특히 검색 컴포넌트에서는 밀집 패시지 검색(dense passage retrieval)과 대조적 번역 메모리(contrastive translation memories)와 같은 최신 기술들을 다루었고, 생성 컴포넌트에서는 낭독 증강 언어 모델(recitation-augmented language models)과 검색-생성 시너지에 대해 논의했습니다.

[Cheng과 연구진](https://arxiv.org/abs/2401.18059)이 개발한 RAPTOR 시스템은 자체 메모리(self-memory) 개념을 도입하여 RAG의 성능을 향상시켰습니다. 이는 본 연구의 커뮤니티 요약과 유사한 접근 방식을 보여줍니다. [Mao와 연구진](https://arxiv.org/abs/2009.08553)의 생성 증강 검색(Generation-Augmented Retrieval, GAR)은 향후 생성 사이클을 더욱 효율적으로 만드는 방법을 제시했습니다.

그래프와 LLM을 결합한 연구 분야에서는 다양한 혁신적인 접근법들이 제시되었습니다. [Trajanoska와 연구진](https://arxiv.org/abs/2305.15294)은 LLM을 사용하여 지식 그래프를 자동으로 생성하는 방법을 개발했으며, [Yao와 연구진](https://arxiv.org/abs/2305.15294)은 불완전한 지식 그래프를 LLM을 통해 보완하는 방법을 제안했습니다. [Ban과 연구진](https://arxiv.org/abs/2305.15294)과 [Zhang과 연구진](https://arxiv.org/abs/2305.15294)은 텍스트에서 인과 관계 그래프를 추출하는 연구를 수행했습니다.

특히 주목할 만한 것은 [Kang과 연구진](https://arxiv.org/abs/2305.18846)의 SURGE 시스템입니다. 이 시스템은 검색된 서브그래프의 사실들을 바탕으로 일관성 있는 내러티브를 생성하는 능력을 보여주었습니다. [Ranade와 Joshi](https://arxiv.org/abs/2305.15294)의 FABULA는 이벤트-플롯 서브그래프를 내러티브 템플릿을 사용해 효과적으로 직렬화하는 방법을 제시했습니다.

오픈소스 생태계에서는 [LangChain](https://github.com/langchain-ai/langchain)과 [LlamaIndex](https://github.com/run-llama/llama_index)가 다양한 그래프 데이터베이스를 지원하며, Neo4J의 [NaLLM](https://github.com/neo4j/NaLLM)과 NebulaGraph의 [GraphRAG](https://github.com/nebula-contrib/GraphRAG)와 같은 특화된 그래프 기반 RAG 애플리케이션들이 개발되고 있습니다.

이러한 기존 연구들과 비교할 때, 본 연구의 Graph RAG는 그래프의 자연스러운 모듈성을 활용하여 전역 요약을 위한 데이터 분할을 수행한다는 점에서 독창성을 가집니다. 특히 [Blondel과 연구진](https://arxiv.org/pdf/0803.0476v2)이 개발한 커뮤니티 탐지 알고리즘을 효과적으로 활용하여 문서의 의미적 구조를 계층적으로 파악하고 이를 요약에 활용한다는 점이 주목할 만합니다.

## 결론: 그래프 RAG를 통한 전체 텍스트 말뭉치의 이해

이 연구는 지식 그래프 생성, 검색 증강 생성(RAG), 그리고 질의 중심 요약(QFS)을 통합하여 전체 텍스트 말뭉치에 대한 인간의 이해를 지원하는 글로벌 접근법인 그래프 RAG를 제시했습니다. 초기 평가 결과는 단순 RAG 기준선과 비교하여 답변의 포괄성과 다양성 측면에서 상당한 개선을 보여주었으며, 맵-리듀스 방식의 소스 텍스트 요약을 사용하는 그래프가 없는 글로벌 접근법과 비교해도 긍정적인 결과를 나타냈습니다.

특히 동일한 데이터셋에 대해 여러 번의 글로벌 질의가 필요한 상황에서, 엔티티 기반 그래프 인덱스의 루트 레벨 커뮤니티 요약은 단순 RAG보다 우수하면서도 다른 글로벌 방법들과 비교하여 훨씬 적은 토큰 비용으로 경쟁력 있는 성능을 달성할 수 있었습니다.

[Gao와 연구진](https://arxiv.org/pdf/2312.10997)이 제시한 RAG 프레임워크의 발전 방향을 따라, 이 연구는 그래프 기반의 검색 메커니즘을 도입하여 기존 RAG 시스템의 한계를 극복했습니다. 특히 [RAPTOR](https://arxiv.org/pdf/2401.18059)에서 제시된 계층적 문서 구조화 방식에서 영감을 받아, 문서의 구조적 특성을 보다 효과적으로 활용할 수 있는 방법을 개발했습니다.

이 연구의 성과는 https://aka.ms/graphrag에서 공개될 예정이며, 글로벌 및 로컬 그래프 RAG 접근법을 모두 구현한 파이썬 기반 오픈소스 코드를 통해 연구 커뮤니티와 공유될 것입니다. 이는 향후 그래프 기반 RAG 시스템의 발전과 응용 연구에 중요한 기여를 할 것으로 기대됩니다.

연구진은 이 작업에 기여한 여러 연구자들의 공헌을 인정하며, 특히 Alonso Guevara Fernández, Amber Hoak, Andrés Morales Esquivel을 비롯한 많은 동료들의 도움에 감사를 표했습니다. 이러한 협력적 연구는 그래프 RAG라는 혁신적인 접근법의 개발을 가능하게 했으며, 이는 대규모 텍스트 말뭉치의 효과적인 이해와 활용을 위한 새로운 지평을 열었다고 할 수 있습니다.

- - -
### References
* [From Local to Global: A Graph RAG Approach to Query-Focused Summarization](http://arxiv.org/pdf/2404.16130v1)