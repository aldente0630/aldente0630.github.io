---
layout: post
title: "LLaMA: Open and Efficient Foundation Language Models"
date: 2023-02-27 17:11:15
author: "Meta AI"
categories: "Natural-Language-Processing"
tags: ["Transformer Architecture", "AdamW Optimizer", "Byte-Pair Encoding", "Causal Multi-Head Attention", "Checkpointing", "Cosine Learning Rate Schedule", "Gradient Clipping", "Rotary Positional Embeddings", "SwiGLU Activation Function", "Toxicity and Bias Evaluation"]
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

대규모 언어 모델의 발전은 주로 비공개 데이터셋과 막대한 컴퓨팅 자원에 의존해왔습니다. GPT-3, PaLM, Chinchilla와 같은 최첨단 모델들은 뛰어난 성능을 보여주었지만, 그들의 독점적 특성으로 인해 연구 커뮤니티의 접근과 검증이 제한되었습니다. 이러한 상황에서 Meta AI 연구진은 공개 데이터만을 활용하여 최고 수준의 성능을 달성할 수 있는 효율적인 언어 모델을 개발하고자 했습니다. 특히 Hoffmann의 스케일링 법칙이 제시한 모델 크기와 학습 데이터 규모 사이의 최적 균형점을 찾는 것이 주요 연구 동기였습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

LLaMA는 전적으로 공개 데이터셋만을 활용하면서도 효율적인 모델 구조를 통해 성능을 극대화하는 접근법을 제시했습니다. 연구팀은 7B부터 65B에 이르는 다양한 규모의 모델을 개발하여, 모델 크기와 성능 사이의 관계를 체계적으로 분석했습니다. 특히 사전 정규화, SwiGLU 활성화 함수, 로터리 위치 임베딩과 같은 최신 기술들을 효과적으로 통합하여 모델의 학습 안정성과 성능을 향상시켰습니다. 또한 약 1.4T 토큰 규모의 고품질 공개 데이터셋을 신중하게 구성하여 학습에 활용했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?

LLaMA의 구현은 효율적인 분산 학습과 메모리 최적화에 중점을 두었습니다. 연구팀은 인과적 다중 헤드 어텐션의 효율적인 구현, 체크포인팅을 통한 역전파 과정 개선, GPU 간 통신과 계산의 효과적인 오버랩 등을 통해 학습 효율성을 극대화했습니다. 특히 2048개의 A100-80GB GPU를 활용한 5개월간의 학습 과정에서, 모델 병렬화와 시퀀스 병렬화를 통해 대규모 모델의 효율적인 학습을 가능하게 했습니다. 또한 CommonCrawl, Wikipedia, GitHub 등 다양한 공개 데이터 소스를 엄격한 품질 관리 과정을 거쳐 학습 데이터로 활용했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

이 연구는 공개 데이터만으로도 최고 수준의 언어 모델을 개발할 수 있다는 것을 실증적으로 입증했습니다. 특히 LLaMA-13B가 GPT-3(175B)를 능가하고, LLaMA-65B가 PaLM-540B와 대등한 성능을 보여준 것은 매우 중요한 의미를 갖습니다. 이는 모델의 규모보다 학습 데이터의 품질과 모델 아키텍처의 효율성이 더 중요할 수 있다는 것을 시사합니다. 또한 이 연구는 언어 모델 개발의 민주화에 기여하며, 향후 연구 커뮤니티가 더욱 효율적이고 접근 가능한 언어 모델을 개발하는 데 중요한 이정표를 제시했습니다. 다만 편향성, 유해성, 허위정보 생성과 같은 문제들이 여전히 존재하며, 이는 향후 연구에서 해결해야 할 중요한 과제로 남아있습니다.
- - -
## LLaMA: 공개 데이터로 구현한 효율적인 기초 언어 모델

Meta AI 연구진이 개발한 LLaMA는 인공지능 연구 커뮤니티에 중요한 이정표를 제시했습니다. LLaMA는 7B부터 65B에 이르는 다양한 규모의 매개변수를 가진 기초 언어 모델 시리즈로, 특히 주목할 만한 점은 이 모델들이 전적으로 공개 데이터셋만을 활용하여 학습되었다는 것입니다.

LLaMA의 가장 큰 기술적 성과는 비공개 데이터셋에 의존하지 않고도 최고 수준의 성능을 달성했다는 점입니다. 특히 13B 매개변수를 가진 LLaMA-13B 모델은 175B 매개변수의 GPT-3를 대부분의 벤치마크에서 능가했으며, 가장 큰 모델인 LLaMA-65B는 Chinchilla-70B와 PaLM-540B와 같은 최고 성능의 모델들과 대등한 성능을 보여주었습니다.

이러한 성과는 대규모 언어 모델 개발에 있어 중요한 시사점을 제공합니다. [Kaplan과 연구진](https://arxiv.org/pdf/2001.08361)이 제시한 스케일링 법칙에 따르면, 모델의 성능은 단순히 매개변수의 수에만 의존하는 것이 아니라 학습 데이터의 품질과 규모에도 크게 영향을 받습니다. LLaMA는 이러한 이론을 실증적으로 입증하며, 수조 개의 토큰으로 구성된 고품질 공개 데이터셋을 효과적으로 활용하여 더 적은 매개변수로도 우수한 성능을 달성할 수 있음을 보여주었습니다.

연구팀은 이 모델들을 연구 커뮤니티에 공개함으로써, 언어 모델 연구의 민주화에 기여하고 있습니다. 이는 [Hoffmann과 연구진](https://arxiv.org/pdf/2203.15556)이 주장한 바와 같이, 고성능 언어 모델 개발에 있어 공개 데이터셋의 중요성과 가능성을 재확인하는 결과입니다. LLaMA의 성공은 향후 언어 모델 연구에서 공개 데이터셋의 효과적인 활용과 모델 효율성 개선이 핵심 연구 방향이 될 것임을 시사합니다.
### 대규모 언어 모델의 새로운 패러다임: 효율성과 접근성의 균형

대규모 언어 모델(Large Language Models, LLMs)은 방대한 텍스트 데이터를 학습하여 텍스트 지시나 퓨샷 샘플만으로도 새로운 과제를 수행할 수 있는 능력을 보여주었습니다. [Brown과 연구진](https://arxiv.org/abs/2005.14165)이 제시한 이러한 퓨샷 샘플 학습 특성은 모델의 규모가 충분히 커졌을 때 처음 관찰되었으며, 이는 더 큰 모델을 개발하려는 연구 흐름으로 이어졌습니다.

하지만 최근 [Hoffmann과 연구진](https://arxiv.org/pdf/2203.15556)의 연구는 주어진 연산 비용 내에서 최고의 성능은 가장 큰 모델이 아닌, 더 많은 데이터로 학습된 상대적으로 작은 모델에서 달성된다는 것을 보여주었습니다. 이러한 발견은 모델 크기와 학습 데이터 규모 사이의 최적 균형점을 찾는 새로운 관점을 제시했습니다.

특히 주목할 점은 기존의 스케일링 법칙이 학습 시의 연산 비용만을 고려했다는 것입니다. 실제 서비스 환경에서는 추론 비용이 더욱 중요한 요소가 됩니다. 예를 들어, Hoffmann의 연구는 10B 매개변수 모델을 200B 토큰으로 학습하는 것을 권장했지만, 우리의 연구에서는 7B 매개변수 모델이 1T 토큰 이상의 학습에서도 지속적인 성능 향상을 보이는 것을 발견했습니다.

이러한 통찰을 바탕으로 개발된 LLaMA 모델 시리즈는 7B부터 65B 매개변수에 이르는 다양한 규모로 구성되어 있으며, 기존의 최고 성능 모델들과 견줄 만한 성능을 보여줍니다. 특히 주목할 만한 점은 LLaMA-13B가 GPT-3보다 10배 작은 규모임에도 대부분의 벤치마크에서 더 우수한 성능을 보인다는 것입니다. 이는 단일 GPU에서도 구동 가능한 수준으로, 언어 모델 연구의 민주화에 크게 기여할 것으로 기대됩니다.

더욱이 LLaMA는 Chinchilla, PaLM, GPT-3와 달리 전적으로 공개 데이터만을 사용하여 학습되었습니다. 이는 기존의 주요 모델들이 "Books-2TB"나 "Social media conversations"와 같은 비공개 또는 문서화되지 않은 데이터에 의존했던 것과 대조됩니다. OPT, GPT-NeoX, BLOOM, GLM과 같은 공개 모델들도 존재하지만, 이들은 PaLM-62B나 Chinchilla와 같은 최고 성능 모델들과 경쟁하기 어려웠습니다.

이러한 LLaMA의 성과는 대규모 언어 모델 개발에 있어 새로운 패러다임을 제시합니다. 단순히 모델의 크기를 키우는 것이 아니라, 효율적인 추론과 접근성을 고려한 균형 잡힌 접근이 필요하다는 것을 보여줍니다. 이어지는 장에서는 LLaMA가 도입한 트랜스포머 아키텍처의 수정사항과 학습 방법론을 상세히 살펴보고, 다양한 벤치마크에서의 성능을 분석하며, 책임 있는 AI 개발을 위한 편향성과 유해성 평가 결과를 논의하겠습니다.
### 학습 접근 방식: 효율적인 트랜스포머 아키텍처의 구현

LLaMA의 학습 접근 방식은 [Brown과 연구진](https://arxiv.org/abs/2005.14165)과 [Chowdhery와 연구진](https://arxiv.org/abs/2204.02311)이 제시한 방법론을 기반으로 하며, 특히 [Hoffmann과 연구진](https://arxiv.org/abs/2203.15556)이 제안한 Chinchilla 스케일링 법칙의 통찰을 적극적으로 반영했습니다. 이 접근 방식의 핵심은 대규모 텍스트 데이터에 대해 표준 최적화 기법을 사용하여 트랜스포머 모델을 학습시키는 것입니다.

구체적인 구현에 있어서, LLaMA는 트랜스포머 아키텍처의 효율성을 극대화하기 위한 여러 기술적 혁신을 도입했습니다. 코드에서 확인할 수 있듯이, 모델은 다중 헤드 어텐션 메커니즘을 중심으로 구성되어 있으며, 특히 모델 병렬화를 위한 최적화된 구조를 채택했습니다.

```python
class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
        self.n_local_heads = args.n_heads // model_parallel_size
        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size
```

이러한 구현은 효율적인 분산 학습을 가능하게 하며, 특히 주목할 만한 점은 로터리 임베딩(Rotary Embeddings)의 도입입니다. 이는 위치 정보를 더욱 효과적으로 인코딩할 수 있게 해주며, 다음과 같이 구현되었습니다.

```python
def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)
    return torch.view_as_real(xq_ * freqs_cis).flatten(3).type_as(xq)
```

모델의 학습 과정에서는 Chinchilla 스케일링 법칙에 따라 모델 크기와 학습 데이터의 규모를 최적화했습니다. 이는 \\( C \\)의 계산 예산이 주어졌을 때, 모델 크기 \\( N \\)과 학습 토큰 수 \\( D \\)를 다음과 같이 조정하는 것을 의미합니다.

$$ N_{opt}(C) \propto C^a $$
$$ D_{opt}(C) \propto C^b $$

여기서 \\( a \\)와 \\( b \\)는 실험적으로 결정된 스케일링 지수입니다. 이러한 접근은 단순히 모델의 크기를 키우는 것이 아니라, 주어진 계산 자원을 가장 효율적으로 활용할 수 있게 해줍니다.

이러한 기술적 선택들은 LLaMA가 상대적으로 작은 모델 크기로도 우수한 성능을 달성할 수 있게 한 핵심 요인이 되었습니다. 특히 모델 병렬화와 효율적인 어텐션 메커니즘의 구현은 학습 과정의 확장성을 크게 향상시켰으며, 이는 대규모 언어 모델의 개발에 있어 새로운 표준을 제시했다고 할 수 있습니다.
### 사전학습 데이터셋: 다양성과 품질의 균형

LLaMA의 사전학습 데이터셋은 공개적으로 이용 가능한 다양한 도메인의 텍스트 데이터를 포함하고 있습니다. 연구팀은 기존 대규모 언어 모델들이 사용한 데이터 소스들 중에서 공개적으로 접근 가능하고 오픈 소스 라이선스와 호환되는 데이터만을 선별적으로 활용했습니다. 전체 데이터셋은 약 1.4T 토큰 규모로 구성되었으며, 각 데이터 소스의 비중은 신중하게 조정되었습니다.

가장 큰 비중을 차지하는 것은 CommonCrawl 데이터로, 전체의 67%를 차지합니다. 연구팀은 2017년부터 2020년까지의 다섯 개 CommonCrawl 덤프를 CCNet 파이프라인을 통해 처리했습니다. 이 과정에서 행 단위 중복제거, fastText 선형 분류기를 이용한 영어 텍스트 식별, n-gram 언어 모델을 통한 품질 필터링이 수행되었습니다. 특히 주목할 만한 점은 Wikipedia 참조 페이지와 무작위 샘플링된 페이지를 구분하는 선형 모델을 학습시켜, 참조 품질의 콘텐츠만을 선별했다는 것입니다.

두 번째로 큰 비중을 차지하는 C4 데이터셋(15%)은 다양한 전처리된 CommonCrawl 데이터셋의 활용이 성능 향상에 도움이 된다는 실험적 관찰에 기반하여 포함되었습니다. C4의 전처리 과정은 CCNet과 유사하게 중복제거와 언어 식별 단계를 포함하지만, 품질 필터링에 있어 문장부호의 존재나 단어 및 문장 수와 같은 휴리스틱을 주로 활용한다는 차이가 있습니다.

GitHub 데이터(4.5%)는 Apache, BSD, MIT 라이선스하의 프로젝트들만을 선별적으로 포함했습니다. 데이터 품질 관리를 위해 행 길이나 영숫자 문자 비율 등의 휴리스틱을 적용했으며, 정규 표현식을 활용하여 헤더와 같은 상용구를 제거했습니다. 최종적으로 파일 수준에서 정확한 매칭을 통한 중복제거를 수행했습니다.

Wikipedia 데이터(4.5%)는 2022년 6월에서 8월 사이의 덤프를 활용했으며, 라틴 문자나 키릴 문자를 사용하는 20개 언어를 포함합니다. 전처리 과정에서는 하이퍼링크, 주석, 서식 관련 요소들이 제거되었습니다.

도서 데이터(4.5%)는 공공 도메인의 Gutenberg Project와 ThePile의 Books3 섹션을 결합하여 구성했습니다. 도서 수준에서 90% 이상의 내용 중복이 있는 경우 제거하는 방식으로 중복제거를 수행했습니다.

학술 데이터로는 ArXiv(2.5%)와 Stack Exchange(2%)가 포함되었습니다. ArXiv 데이터는 LaTeX 파일에서 첫 섹션 이전 내용과 참고문헌을 제거하고, 주석을 제거하며, 사용자 정의 매크로를 인라인으로 확장하는 등의 전처리를 거쳤습니다. Stack Exchange는 28개의 주요 웹사이트에서 추출한 고품질 질문과 답변을 포함하며, HTML 태그를 제거하고 점수에 따라 답변을 정렬했습니다.

토크나이저는 SentencePiece 구현체를 사용한 바이트 페어 인코딩(BPE) 알고리즘을 채택했습니다. 특히 모든 숫자를 개별 자릿수로 분할하고, 알 수 없는 UTF-8 문자는 바이트 단위로 분해하는 방식을 적용했습니다. 이러한 토크나이저 구현은 다음과 같은 코드로 구현되었습니다.

```python
def encode(self, s: str, bos: bool, eos: bool) -> List[int]:
    assert type(s) is str
    t = self.sp_model.encode(s)
    if bos:
        t = [self.bos_id] + t
    if eos:
        t = t + [self.eos_id]
    return t
```

전체 학습 데이터셋은 토크나이저 처리 후 약 1.4T 토큰 규모이며, Wikipedia와 도서 도메인을 제외한 대부분의 데이터는 학습 과정에서 한 번만 사용되었습니다. Wikipedia와 도서 데이터는 약 2회의 에포크를 거쳤는데, 이는 이들 도메인의 상대적으로 높은 품질과 중요성을 고려한 선택입니다.
### 트랜스포머 아키텍처의 혁신적 개선

LLaMA는 [Vaswani와 연구진](https://arxiv.org/pdf/1706.03762)이 제안한 기본 트랜스포머 아키텍처를 기반으로 하되, 최신 연구 결과들을 반영한 여러 가지 중요한 개선사항을 도입했습니다. 이러한 수정사항들은 모델의 학습 안정성과 성능을 크게 향상시켰으며, 특히 대규모 언어 모델에서 효과적으로 작동하는 것으로 입증되었습니다.

첫 번째 주요 개선사항은 사전 정규화(Pre-normalization) 방식의 도입입니다. GPT-3에서 영감을 받은 이 접근법은 각 트랜스포머 부층의 출력이 아닌 입력을 정규화합니다. 구체적으로, [Zhang과 Sennrich](https://arxiv.org/pdf/1910.07467)가 제안한 RMSNorm을 정규화 함수로 사용합니다. RMSNorm은 다음과 같이 구현됩니다.

```python
def _norm(self, x):
    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
```

이 정규화 방식은 계산 효율성이 높으며, 특히 대규모 모델의 학습 안정성을 크게 향상시킵니다.

두 번째 개선사항은 [Shazeer](https://arxiv.org/pdf/2002.05202)가 도입한 SwiGLU 활성화 함수의 채택입니다. 기존 ReLU 비선형성을 대체하는 이 함수는 모델의 성능을 향상시키는 것으로 나타났습니다. PaLM 모델과 달리, LLaMA는 은닉층 차원을 \\( \frac{2}{3}4d \\) 로 설정했는데, 이는 \\( 4d \\)를 사용한 PaLM보다 계산 효율성을 높이면서도 성능을 유지할 수 있게 해줍니다.

세 번째 주요 개선사항은 [Su와 연구진](https://arxiv.org/pdf/2104.09864)이 제안한 로터리 위치 임베딩(Rotary Position Embeddings, RoPE)의 도입입니다. 절대 위치 임베딩을 제거하고 대신 각 네트워크 층에 RoPE를 추가함으로써, 모델은 시퀀스 내 토큰들의 상대적 위치 정보를 더욱 효과적으로 처리할 수 있게 되었습니다.

이러한 아키텍처 개선사항들의 효과는 학습 과정에서 명확하게 드러납니다. 아래 그래프에서 볼 수 있듯이, 다양한 크기의 LLaMA 모델들(7B, 13B, 33B, 65B)은 학습이 진행됨에 따라 안정적인 성능 향상을 보여줍니다.

![학습 손실 곡선](https://ar5iv.org//html/2302.13971/assets/x1.png)

특히 주목할 만한 점은 더 큰 모델들(33B, 65B)이 1.4T 토큰으로 학습되었을 때, 작은 모델들(7B, 13B)이 1.0T 토큰으로 학습되었을 때보다 더 낮은 학습 손실을 달성한다는 것입니다. 이는 모델 크기와 학습 데이터 규모 사이의 최적 균형점을 찾는 것의 중요성을 보여줍니다.

이러한 아키텍처 개선사항들의 조합은 LLaMA가 상대적으로 작은 매개변수 수로도 최고 수준의 성능을 달성할 수 있게 한 핵심 요인이 되었습니다. 특히 사전 정규화와 로터리 임베딩의 도입은 모델의 학습 안정성과 시퀀스 처리 능력을 크게 향상시켰으며, SwiGLU 활성화 함수의 효율적인 구현은 계산 리소스의 효과적인 활용을 가능하게 했습니다.
### 옵티마이저 구성: 효율적인 학습을 위한 세부 설정

LLaMA의 학습 과정에서는 [Loshchilov와 Hutter](https://arxiv.org/pdf/1711.05101)가 제안한 AdamW 옵티마이저를 채택했습니다. AdamW는 기존 Adam 옵티마이저의 개선된 버전으로, 가중치 감쇠(weight decay)를 그래디언트 업데이트와 분리하여 처리함으로써 더 효과적인 정규화 효과를 얻을 수 있습니다. 연구팀은 AdamW의 주요 하이퍼파라미터를 \\( \beta_1 = 0.9 \\)와 \\( \beta_2 = 0.95 \\)로 설정했는데, 이는 대규모 언어 모델의 학습에 특화된 값들입니다.

학습률 조정을 위해서는 코사인 스케줄링 방식을 채택했습니다. 이 방식에서는 학습률이 초기값에서 시작하여 코사인 함수를 따라 점진적으로 감소하며, 최종 학습률은 최대 학습률의 10%가 되도록 설정되었습니다. 이러한 점진적 감소는 학습 과정 후반부에서도 적절한 수준의 파라미터 업데이트를 가능하게 합니다.

모델의 안정적인 학습을 위해 0.1의 가중치 감쇠와 1.0의 그래디언트 클리핑을 적용했습니다. 가중치 감쇠는 모델의 과적합을 방지하고 일반화 성능을 향상시키는 데 기여하며, 그래디언트 클리핑은 학습 과정에서 발생할 수 있는 그래디언트 폭발 문제를 효과적으로 제어합니다.

특히 주목할 만한 점은 2,000 스텝의 웜업 기간을 도입한 것입니다. 이 기간 동안 학습률이 점진적으로 증가하면서 모델이 초기에 안정적으로 학습될 수 있도록 합니다. 더불어 모델의 규모에 따라 학습률과 배치 크기를 적응적으로 조정했는데, 이는 각 모델 크기에 최적화된 학습 조건을 제공하기 위함입니다.

이러한 옵티마이저 설정은 LLaMA의 효율적인 학습에 핵심적인 역할을 했습니다. 특히 AdamW의 분리된 가중치 감쇠 메커니즘은 모델의 일반화 성능 향상에 크게 기여했으며, 코사인 학습률 스케줄링과 웜업 전략의 조합은 안정적이면서도 효과적인 학습 과정을 가능하게 했습니다. 이러한 세부적인 최적화 전략들은 LLaMA가 상대적으로 적은 계산 리소스로도 우수한 성능을 달성할 수 있게 한 중요한 요인이 되었습니다.
### 효율적인 구현: 최적화와 병렬화 전략

LLaMA 모델의 효율적인 구현을 위해 연구팀은 여러 가지 혁신적인 최적화 기법을 도입했습니다. 이러한 최적화는 크게 메모리 사용량 감소, 실행 시간 단축, 그리고 분산 학습 효율성 향상이라는 세 가지 측면에 초점을 맞추고 있습니다.

첫 번째 주요 최적화는 인과적 다중 헤드 어텐션(causal multi-head attention)의 효율적인 구현입니다. 연구팀은 [Rabe와 Staats](https://arxiv.org/pdf/2112.05682)가 제안한 방식을 기반으로 하되, [Dao와 연구진](https://arxiv.org/pdf/2205.14135)이 개발한 역전파 알고리즘을 통합했습니다. 이 구현의 핵심은 어텐션 가중치를 저장하지 않고, 언어 모델링 작업의 인과적 특성으로 인해 마스킹되는 키/쿼리 점수를 계산하지 않는 것입니다. 이는 xformers 라이브러리를 통해 구현되었으며, 메모리 사용량과 계산 시간을 크게 줄일 수 있었습니다.

두 번째 최적화는 체크포인팅을 통한 역전파 과정의 개선입니다. 연구팀은 역전파 과정에서 재계산되는 활성화의 양을 줄이는 방식을 채택했습니다. 특히 선형 층의 출력과 같이 계산 비용이 높은 활성화들을 선택적으로 저장함으로써, PyTorch의 자동 미분 대신 트랜스포머 층에 대한 역전파 함수를 수동으로 구현했습니다. 이러한 최적화의 효과를 극대화하기 위해, [Korthikanti와 연구진](https://arxiv.org/pdf/2205.05198)이 제안한 모델 병렬화와 시퀀스 병렬화를 도입하여 모델의 메모리 사용량을 더욱 줄였습니다.

세 번째 주요 최적화는 GPU 간 통신과 계산의 효율적인 오버랩입니다. 연구팀은 활성화 계산과 all-reduce 연산으로 인한 GPU 간 네트워크 통신을 최대한 겹치도록 구현했습니다. 이러한 최적화의 결과, 65B 매개변수 모델의 경우 80GB RAM을 탑재한 2048개의 A100 GPU에서 GPU당 초당 약 380개의 토큰을 처리할 수 있게 되었습니다. 이는 1.4T 토큰으로 구성된 전체 데이터셋의 학습을 약 21일 만에 완료할 수 있게 해주었습니다.

이러한 최적화 기법들의 효과는 다양한 상식 추론 작업에서의 성능을 통해 입증되었습니다. 예를 들어, LLaMA-65B 모델은 BoolQ에서 85.3%, PIQA에서 82.8%, WinoGrande에서 77.0%의 정확도를 달성했으며, 이는 GPT-3 175B나 PaLM 540B와 같은 더 큰 모델들과 비교해도 경쟁력 있는 수준입니다.

이러한 구현상의 혁신은 대규모 언어 모델의 학습에 있어 새로운 효율성 기준을 제시했습니다. 특히 메모리 사용량 최적화와 계산 효율성 향상을 통해, 더 적은 컴퓨팅 자원으로도 고성능 언어 모델을 학습할 수 있게 되었다는 점에서 큰 의의를 가집니다.
## LLaMA의 주요 실험 결과: 포괄적 성능 평가

LLaMA 모델의 성능 평가는 [Brown과 연구진](https://arxiv.org/abs/2005.14165)이 제시한 평가 프레임워크를 따라 제로샷(zero-shot)과 퓨샷(few-shot) 학습 상황에서 진행되었습니다. 총 20개의 다양한 벤치마크에서 평가가 이루어졌으며, 각각의 평가 방식은 특정한 목적을 가지고 설계되었습니다. 제로샷 평가에서는 모델에게 과제에 대한 텍스트 설명과 테스트 예제만을 제공하고, 모델은 자유 형식의 생성이나 제안된 답변의 순위 매기기를 수행합니다. 퓨샷 평가에서는 1개에서 64개 사이의 과제 예시와 테스트 예제를 함께 제공하여 모델의 적응 능력을 평가합니다.

연구팀은 LLaMA를 GPT-3, Gopher, Chinchilla, PaLM과 같은 비공개 대규모 언어 모델들과 비교했으며, 동시에 OPT, GPT-J, GPT-Neo와 같은 공개 모델들과도 비교 평가를 진행했습니다. 객관식 과제에서는 주어진 맥락에 가장 적합한 답변을 선택하는 방식을 채택했으며, 답변 선택 시에는 Gao와 연구진이 제안한 문자 수로 정규화된 우도를 사용했습니다. 단, OpenBookQA와 BoolQ와 같은 특정 데이터셋에서는 [Brown과 연구진](https://arxiv.org/abs/2005.14165)의 방식을 따라 다음과 같은 정규화된 우도를 사용했습니다.

$$ P(\text{completion}|\text{context})/P(\text{completion}|\text{"Answer:"}) $$

상식 추론 능력 평가에서 LLaMA는 주목할 만한 성과를 보여주었습니다. BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, OpenBookQA 등 8개의 표준 벤치마크에서 평가가 이루어졌으며, LLaMA-65B는 BoolQ를 제외한 모든 벤치마크에서 Chinchilla-70B의 성능을 능가했습니다. 더욱 주목할 만한 점은 LLaMA-13B가 GPT-3보다 10배 작은 규모임에도 불구하고 대부분의 벤치마크에서 더 우수한 성능을 보였다는 것입니다.

질문 답변 과제에서도 LLaMA는 뛰어난 성능을 보여주었습니다. Natural Questions와 TriviaQA 벤치마크에서 문서 접근 없이 수행되는 폐쇄형 질문 답변 평가에서 LLaMA-65B는 제로샷과 퓨샷 설정 모두에서 최고 수준의 성능을 달성했습니다. 특히 LLaMA-13B는 단일 V100 GPU에서 실행 가능함에도 불구하고 GPT-3 및 Chinchilla와 비교했을 때 경쟁력 있는 성능을 보여주었습니다.

수학적 추론 능력 평가에서는 MATH와 GSM8k 벤치마크를 통해 모델의 성능을 검증했습니다. 특히 주목할 만한 점은 LLaMA-65B가 수학 데이터로 추가 학습을 받지 않았음에도 불구하고 수학 데이터로 미세조정된 Minerva-62B의 성능을 능가했다는 것입니다. 이러한 결과는 LLaMA의 기초적인 추론 능력이 매우 견고하게 학습되었음을 시사합니다.

![학습 과정에서의 성능 진화](https://ar5iv.org//html/2302.13971/assets/x2.png)

학습 과정에서의 성능 변화를 추적한 결과는 매우 흥미로운 패턴을 보여줍니다. 대부분의 벤치마크에서 성능은 학습이 진행됨에 따라 꾸준히 향상되었으며, 이는 모델의 학습 손실(perplexity) 감소와 높은 상관관계를 보였습니다. 다만 SIQA와 WinoGrande에서는 예외적인 패턴이 관찰되었는데, 특히 SIQA에서는 성능의 높은 변동성이 관찰되어 이 벤치마크의 신뢰성에 대한 의문을 제기하게 되었습니다.

이러한 포괄적인 평가 결과들은 LLaMA가 상대적으로 작은 모델 크기에도 불구하고 다양한 자연어 처리 과제에서 최고 수준의 성능을 달성할 수 있다는 것을 입증합니다. 특히 추가적인 미세조정 없이도 이러한 성능을 달성했다는 점은 LLaMA의 기초적인 언어 이해 및 추론 능력이 매우 견고하게 학습되었음을 시사합니다.
### 지시어 미세조정을 통한 LLaMA의 성능 향상

지시어 미세조정(Instruction Finetuning)은 언어 모델의 성능을 효과적으로 향상시키는 중요한 기법으로 주목받고 있습니다. LLaMA 연구팀은 기본 LLaMA-65B 모델이 이미 기초적인 지시사항을 따를 수 있는 능력을 보유하고 있음을 확인했으나, 소량의 지시어 데이터를 활용한 미세조정을 통해 MMLU(Massive Multitask Language Understanding) 벤치마크에서 상당한 성능 향상을 달성할 수 있었습니다.

연구팀은 [Chung과 연구진](https://arxiv.org/pdf/2210.11416)이 제시한 프로토콜을 따라 단일 실험을 진행했습니다. 이 과정에서 개발된 LLaMA-I 모델은 MMLU 벤치마크에서 68.9%의 정확도를 달성했는데, 이는 기존 LLaMA-65B 모델의 63.4%에서 상당한 향상을 보여주는 결과입니다. 특히 주목할 만한 점은 이러한 성능 향상이 매우 적은 양의 지시어 데이터만으로도 달성되었다는 것입니다.

LLaMA-I의 성능은 기존의 중간 규모 지시어 미세조정 모델들과 비교했을 때 더욱 두드러집니다. OPT-IML-Max-30B(43.2%), Flan-T5-XXL-11B(55.1%), Flan-PaLM-62B(59.6%) 등과 비교하여 LLaMA-I는 현저히 높은 성능을 보여주었습니다. 다만, GPT code-davinci-002가 달성한 77.4%의 최고 성능과는 아직 격차가 있음을 확인할 수 있습니다.

이러한 결과는 지시어 미세조정의 효율성과 잠재력을 잘 보여줍니다. [Iyer와 연구진](https://arxiv.org/pdf/2212.12017)의 연구에서도 언급된 바와 같이, 적은 양의 지시어 데이터로도 모델의 전반적인 성능을 크게 향상시킬 수 있다는 점은 매우 고무적입니다. 특히 LLaMA-I가 보여준 성능은 모델의 규모나 학습 데이터의 양을 크게 늘리지 않고도 효과적인 성능 향상이 가능하다는 것을 입증합니다.

연구팀은 부록의 Table 16에서 57개의 개별 MMLU 과제에 대한 상세한 성능 분석을 제공하고 있습니다. 이러한 세부적인 분석은 모델의 강점과 약점을 이해하고, 향후 개선 방향을 설정하는 데 중요한 통찰을 제공합니다. 지시어 미세조정의 단순성과 효과성을 고려할 때, 이는 향후 언어 모델 개발에 있어 매우 유망한 접근 방식임이 분명해 보입니다.
### 편향성, 유해성, 허위정보 생성에 대한 포괄적 평가

대규모 언어 모델의 발전과 함께 이러한 모델들이 가진 잠재적 위험성에 대한 평가의 중요성이 더욱 부각되고 있습니다. LLaMA 연구팀은 모델의 편향성, 유해 콘텐츠 생성 가능성, 그리고 허위정보 생성 경향을 체계적으로 평가했습니다. 이는 웹 데이터를 광범위하게 활용한 학습 과정에서 발생할 수 있는 잠재적 문제들을 이해하고 대응하기 위한 중요한 과정이었습니다.

유해 콘텐츠 생성 평가를 위해 연구팀은 RealToxicityPrompts 벤치마크를 활용했습니다. 이 평가는 약 10만 개의 프롬프트에 대한 모델의 응답을 분석하며, PerspectiveAPI를 통해 유해성 점수를 측정합니다. 평가는 기본 프롬프트와 "공손하고, 존중하며, 편향되지 않은 방식으로 다음 문장을 완성하시오"라는 지시가 포함된 '존중' 프롬프트 두 가지 방식으로 진행되었습니다. 실험 결과, 모델의 크기가 증가할수록 유해성 점수도 증가하는 경향이 관찰되었으며, 특히 '존중' 프롬프트에서 이러한 경향이 더욱 두드러졌습니다.

편향성 평가를 위해서는 CrowS-Pairs 데이터셋을 활용했습니다. 이 데이터셋은 성별, 종교, 인종, 성적 지향성 등 9개 범주에서의 편향성을 측정합니다. 각 예시는 고정관념적 문장과 반고정관념적 문장 쌍으로 구성되어 있으며, 모델이 고정관념적 문장을 선호하는 정도를 퍼플렉시티를 통해 측정합니다. LLaMA-65B는 평균적으로 GPT-3와 OPT-175B보다 약간 낮은 편향성을 보였으나, 종교 범주에서는 상대적으로 높은 편향성(+10%)을 나타냈습니다.

성별 편향성에 대한 더 깊은 분석을 위해 WinoGender 벤치마크를 활용했습니다. 이 평가는 직업과 관련된 대명사 참조 해결 능력을 테스트합니다. 연구 결과, LLaMA는 "their/them/someone"과 같은 중성적 대명사에서 더 높은 성능을 보였으며, "her/her/she"와 "his/him/he"와 같은 성별 특정적 대명사에서는 상대적으로 낮은 성능을 보였습니다. 특히 "gotcha" 케이스(직업의 다수 성별과 대명사가 일치하지 않는 경우)에서 성능이 현저히 저하되는 것이 관찰되었습니다.

모델의 진실성 평가를 위해 TruthfulQA 벤치마크가 활용되었습니다. 이 평가는 모델이 실제 세계에 대한 사실적 진술을 생성할 수 있는 능력을 측정합니다. LLaMA-65B는 GPT-3보다 높은 성능을 보였으나, 여전히 정답률이 낮아 허위정보 생성의 위험성이 존재함을 확인했습니다.

이러한 포괄적인 평가 결과들은 LLaMA가 가진 잠재적 위험성과 한계를 명확히 보여줍니다. 특히 모델의 규모가 커질수록 유해 콘텐츠 생성 가능성이 증가하는 점, 특정 범주에서의 높은 편향성, 그리고 허위정보 생성의 위험성은 향후 개선이 필요한 중요한 과제로 확인되었습니다. 이러한 평가 결과는 대규모 언어 모델의 책임 있는 개발과 배포를 위한 중요한 지표를 제공합니다.
### 탄소 발자국: LLaMA 모델 학습의 환경적 영향 평가

대규모 언어 모델의 학습 과정에서 발생하는 환경적 영향, 특히 탄소 발자국에 대한 이해와 평가는 AI 연구의 지속가능성을 위해 매우 중요한 요소입니다. LLaMA 연구팀은 Wu와 연구진이 제시한 방법론을 기반으로 모델 학습 과정의 에너지 소비량과 탄소 배출량을 체계적으로 분석했습니다.

에너지 소비량 산정을 위해 연구팀은 다음과 같은 수식을 활용했습니다.

$$ \textrm{Wh} = \textrm{GPU-h} \times (\textrm{GPU power consumption}) \times \textrm{PUE} $$

여기서 전력사용효율(PUE)은 1.1로 설정되었으며, 이는 현대적인 데이터 센터의 효율성을 반영하는 값입니다. 이러한 계산 방식은 GPU 사용 시간과 전력 소비를 직접적으로 고려함으로써 정확한 에너지 소비량 추정을 가능하게 합니다.

탄소 배출량 환산을 위해서는 다음 수식이 사용되었습니다.

$$ \textrm{tCO}_2\textrm{eq} = \textrm{MWh} \times 0.385 $$

이 계산에서는 미국의 평균 전력망 탄소 집약도인 0.385 kg CO2eq/KWh를 기준으로 사용했습니다. 이는 다른 모델들과의 공정한 비교를 위한 선택이었습니다. 예를 들어, BLOOM 모델이 사용한 전력망의 탄소 집약도는 0.057 kg CO2eq/KWh로 27 tCO2eq의 배출량을 기록했으며, OPT 모델의 경우 0.231 kg CO2eq/KWh의 전력망에서 82 tCO2eq를 배출했습니다.

LLaMA 모델 시리즈의 개발 과정에서는 2048개의 A100-80GB GPU가 약 5개월 동안 사용되었습니다. 이는 연구팀의 추정에 따르면 약 2,638 MWh의 에너지를 소비했으며, 총 1,015 tCO2eq의 탄소를 배출한 것으로 계산됩니다. OPT 모델의 경우, 공개된 학습 로그에 따르면 992개의 A100-80B GPU를 34일 동안 사용한 것으로 확인됩니다.

연구팀은 이러한 상당한 환경적 영향에도 불구하고, 학습된 모델을 공개함으로써 향후 발생할 수 있는 추가적인 탄소 배출을 줄일 수 있을 것으로 기대합니다. 특히 일부 작은 규모의 모델들은 단일 GPU에서도 실행이 가능하도록 설계되어, 연구 커뮤니티에서 보다 효율적으로 활용될 수 있습니다. 이는 대규모 언어 모델 연구에서 환경적 지속가능성과 접근성을 동시에 고려한 중요한 진전이라고 할 수 있습니다.
### 언어 모델의 발전 역사: n-gram에서 대규모 트랜스포머까지

언어 모델은 단어, 토큰, 또는 문자 시퀀스에 대한 확률 분포를 학습하는 시스템으로, Shannon이 1948년에 처음 제안한 이후 자연어 처리의 핵심 과제로 자리잡았습니다. 특히 다음 토큰을 예측하는 과제는 Turing이 제안한 "모방 게임"을 통한 기계 지능 측정의 중요한 벤치마크가 되었습니다.

초기 언어 모델은 n-그램 통계를 기반으로 했습니다. Bahl과 연구진은 n-그램 카운트를 활용한 기본적인 언어 모델을 제안했으며, 이후 Katz와 Kneser와 Ney는 희소 사건(rare events)의 추정을 개선하기 위한 다양한 스무딩 기법을 개발했습니다.

신경망의 도입은 언어 모델링에 혁신적인 변화를 가져왔습니다. Bengio와 연구진이 제안한 피드포워드 신경망을 시작으로, Mikolov와 연구진의 순환 신경망(RNN), Hochreiter와 Schmidhuber의 LSTM 등이 차례로 등장했습니다. 그러나 가장 큰 혁신은 [Vaswani와 연구진](https://arxiv.org/pdf/1706.03762)이 2017년에 발표한 트랜스포머 네트워크였습니다. 셀프 어텐션(self-attention) 메커니즘을 기반으로 한 트랜스포머는 특히 장거리 의존성 포착에서 탁월한 성능을 보여주었습니다.

언어 모델의 스케일링은 지속적으로 발전해왔습니다. Brants와 연구진은 2조 토큰으로 학습된 언어 모델이 기계 번역의 품질을 크게 향상시킬 수 있음을 보여주었습니다. 이후 [Brown과 연구진](https://arxiv.org/pdf/2005.14165)이 개발한 GPT-3(175B 매개변수)는 대규모 언어 모델의 새로운 시대를 열었으며, 이는 Jurassic-1, Gopher, Chinchilla, PaLM 등 더욱 발전된 모델들의 등장으로 이어졌습니다.

[Kaplan과 연구진](https://arxiv.org/pdf/2001.08361)은 트랜스포머 기반 언어 모델에 대한 스케일링 법칙을 도출했으며, 이는 후에 [Hoffmann과 연구진](https://arxiv.org/pdf/2203.15556)에 의해 더욱 정교화되었습니다. 이러한 스케일링 법칙은 모델 크기, 데이터셋 크기, 그리고 계산 자원 사이의 최적 균형점을 찾는 데 중요한 지침을 제공합니다.

이러한 발전 과정은 언어 모델이 단순한 통계적 도구에서 복잡한 언어 이해와 생성이 가능한 시스템으로 진화해왔음을 보여줍니다. 특히 최근의 대규모 언어 모델들은 다양한 자연어 처리 과제에서 인상적인 성능을 보여주며, 인공지능 연구의 새로운 지평을 열어가고 있습니다.
## 결론: 공개 데이터로 달성한 효율적 언어 모델의 새 지평

본 연구는 공개적으로 접근 가능한 데이터만을 활용하여 최고 수준의 성능을 달성할 수 있다는 것을 입증했습니다. 특히 주목할 만한 점은 LLaMA-13B가 GPT-3보다 10배 이상 작은 규모임에도 불구하고 더 우수한 성능을 보여주었으며, LLaMA-65B는 Chinchilla-70B와 PaLM-540B와 같은 최첨단 기초 모델들과 대등한 성능을 달성했다는 것입니다.

이전의 연구들과 달리, 본 연구는 독점적인 데이터셋에 의존하지 않고도 최고 수준의 성능을 달성할 수 있다는 것을 실증적으로 보여주었습니다. 이는 [Hoffmann과 연구진](https://arxiv.org/pdf/2203.15556)이 제시한 스케일링 법칙의 실천적 검증이며, 동시에 언어 모델 연구의 민주화에 중요한 이정표를 제시합니다.

연구팀은 이러한 모델들을 연구 커뮤니티에 공개함으로써 대규모 언어 모델의 발전을 가속화하고, 유해성과 편향성과 같은 알려진 문제점들을 개선하기 위한 노력을 지원하고자 합니다. [Chung과 연구진](https://arxiv.org/pdf/2210.11416)의 연구에서 관찰된 바와 같이, 지시어에 대한 미세조정이 유망한 결과를 보여주었으며, 연구팀은 향후 연구에서 이를 더욱 심도 있게 탐구할 계획입니다.

마지막으로, 연구팀은 모델의 규모를 확장함에 따라 지속적인 성능 향상이 관찰되었다는 점을 고려하여, 향후 더 큰 규모의 사전학습 데이터를 활용한 대규모 모델을 공개할 계획을 가지고 있습니다. 이는 [Kaplan과 연구진](https://arxiv.org/pdf/2001.08361)이 제시한 스케일링 법칙의 실용적 적용을 더욱 확장하는 의미 있는 시도가 될 것입니다.

이러한 연구 성과는 대규모 언어 모델 분야에서 공개 데이터와 효율적인 아키텍처의 중요성을 재확인하며, 향후 연구 방향에 대한 중요한 통찰을 제공합니다. 특히 모델의 규모와 효율성 사이의 최적 균형점을 찾는 노력은 계속될 것이며, 이는 더욱 강력하고 접근 가능한 언어 모델의 개발로 이어질 것으로 기대됩니다.
- - -
### References
* [LLaMA: Open and Efficient Foundation Language Models](http://arxiv.org/pdf/2302.13971v1)
