---
layout: post
title: "Llama 2: Open Foundation and Fine-Tuned Chat Models"
date: 2023-07-18 14:31:57
author: "Meta AI"
categories: "Natural-Language-Processing"
tags: ["Transformer-Architecture", "Reinforcement-Learning-with-Human-Feedback", "Supervised-Fine-Tuning", "Grouped-Query-Attention", "Rotary-Positional-Embeddings", "SwiGLU-Activation-Function", "AdamW-Optimizer", "Cosine-Learning-Rate-Schedule", "Gradient-Clipping", "Toxicity-and-Bias-Evaluation"]
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

대규모 언어 모델(LLM)의 발전이 가속화되는 가운데, 연구 커뮤니티의 접근성과 모델의 안전성이라는 두 가지 중요한 과제가 대두되었습니다. 기존의 강력한 언어 모델들은 대부분 독점적으로 운영되어 연구 발전에 제약이 있었고, 공개된 모델들은 성능과 안전성 측면에서 한계를 보였습니다. Meta는 이러한 간극을 해소하고 AI 기술의 민주화를 촉진하기 위해 Llama 2 프로젝트를 시작했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

Llama 2는 세 가지 핵심적인 혁신을 제시했습니다. 첫째, 사전 학습 데이터의 40% 증가와 문맥 길이의 2배 확장을 통해 기본 성능을 크게 향상시켰습니다. 둘째, 그룹 쿼리 어텐션(GQA) 메커니즘을 도입하여 계산 효율성을 개선했습니다. 셋째, 지도 학습 기반 미세조정(SFT)과 인간 피드백 강화학습(RLHF)을 결합한 포괄적인 안전성 강화 프레임워크를 구축했습니다. 특히 Ghost Attention(GAtt) 메커니즘의 도입으로 다중 턴 대화의 일관성 문제를 효과적으로 해결했습니다.

#### 제안된 방법은 어떻게 구현되었습니까?

구현은 크게 세 단계로 진행되었습니다. 먼저, 2조 개의 토큰으로 구성된 고품질 데이터셋으로 사전 학습을 수행했습니다. 다음으로, 27,540개의 신중하게 선별된 지시 사항으로 SFT를 진행했습니다. 마지막으로, PPO 알고리즘을 활용한 RLHF를 통해 모델의 응답을 인간의 선호도에 맞게 최적화했습니다. 안전성 강화를 위해 350명 이상의 전문가로 구성된 레드팀의 평가와 피드백이 지속적으로 반영되었으며, 특별히 설계된 안전성 데이터 주석 작업을 통해 모델의 잠재적 위험을 최소화했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

Llama 2의 성공은 개방형 AI 모델의 새로운 기준을 제시했습니다. 특히 7B에서 70B에 이르는 다양한 규모의 모델이 공개됨으로써, 연구자들은 각자의 필요에 맞는 모델을 선택하여 활용할 수 있게 되었습니다. 더욱 중요한 것은, 이 모델이 안전성과 성능 사이의 균형을 성공적으로 달성했다는 점입니다. ToxiGen 벤치마크에서 0.01%의 낮은 독성 생성률을 보이면서도, TruthfulQA에서 64.14%의 높은 정확도를 달성한 것은 책임있는 AI 개발의 모범 사례를 제시했습니다. 이는 AI 기술의 민주화와 안전한 발전이 양립 가능하다는 것을 입증하는 중요한 이정표가 되었습니다.

- - -
## Llama 2: 개방형 기반 및 미세조정된 대화 모델

대규모 언어 모델의 발전이 가속화되는 가운데, Meta에서 공개한 Llama 2는 개방형 기반 모델과 대화에 최적화된 모델을 동시에 제공하는 획기적인 진전을 이루었습니다. 이 모델은 [Touvron과 연구진](https://arxiv.org/pdf/2302.13971)이 개발한 원래의 LLaMA를 기반으로 하여, 성능과 접근성 측면에서 상당한 개선을 이루어냈습니다.

Llama 2의 핵심 아키텍처는 [Vaswani과 연구진](https://arxiv.org/pdf/1706.03762v7)이 제안한 Transformer 구조를 기반으로 하되, 여러 가지 중요한 개선사항을 도입했습니다. 특히 주목할 만한 것은 사전 정규화(pre-normalization) 기법의 도입으로, 각 Transformer 하위 계층의 입력을 정규화함으로써 학습 안정성을 크게 향상시켰습니다. 이는 다음과 같은 수식으로 표현됩니다.

$$ h' = \text{LayerNorm}(h) $$
$$ h_{\text{out}} = h + \text{SubLayer}(h') $$

여기서 \\(h\\)는 입력 표현이며, \\(\text{SubLayer}\\)는 자기 주의 메커니즘이나 피드포워드 네트워크와 같은 하위 계층을 나타냅니다.

[Hoffmann과 연구진](https://arxiv.org/pdf/2203.15556)의 연구 결과를 반영하여, Llama 2는 모델 크기와 학습 데이터 규모 사이의 최적 균형을 찾는데 주력했습니다. 이는 계산 효율성을 극대화하면서도 성능을 최적화하는 방향으로 설계되었습니다. 특히 주목할 만한 점은 모델 크기를 두 배로 늘릴 때마다 학습 토큰의 수도 비례적으로 증가시키는 접근 방식을 채택했다는 것입니다.

[Chung과 연구진](https://arxiv.org/pdf/2210.11416)의 연구에서 제시된 지시사항 미세조정(instruction finetuning) 기법을 확장 적용하여, Llama 2는 다양한 작업에서 뛰어난 성능을 보여줍니다. 이 과정에서 PPO(Proximal Policy Optimization) 알고리즘을 활용하여 모델의 응답을 개선했으며, 이는 [Schulman과 연구진](https://arxiv.org/pdf/1707.06347)이 제안한 방법을 기반으로 합니다. PPO의 핵심 목적 함수는 다음과 같습니다.

$$ L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right) \right] $$

이러한 기술적 혁신을 통해 Llama 2는 공개된 데이터만을 사용하면서도 최첨단 성능을 달성했으며, 연구 커뮤니티에 개방함으로써 AI 발전의 민주화에 기여하고 있습니다. 특히 대화 모델 버전은 안전성과 유용성을 크게 향상시켰으며, 이는 광범위한 실제 응용 분야에서 활용될 수 있는 가능성을 보여줍니다.

### Llama 2의 혁신적 발전과 기술적 특징

Meta가 공개한 Llama 2는 대규모 언어 모델(LLM) 분야에서 중요한 전환점을 마련했습니다. 7B에서 70B 매개변수에 이르는 다양한 규모의 모델을 제공하며, 특히 대화 시스템에 최적화된 Llama 2-Chat 모델은 기존 공개 모델들의 한계를 뛰어넘는 성능을 보여주고 있습니다.

Llama 2의 가장 주목할 만한 기술적 혁신은 사전 학습 데이터의 40% 증가와 문맥 길이(context length)의 2배 확장입니다. 또한 [Ainslie과 연구진]이 제안한 그룹 쿼리 어텐션(grouped-query attention) 메커니즘을 도입하여 계산 효율성을 크게 향상시켰습니다. 이러한 개선사항들은 모델의 기본적인 이해력과 추론 능력을 한층 강화했습니다.

![Llama 2-Chat의 유용성 평가 결과](https://ar5iv.org//html/2307.09288/assets/x1.png)  
모델의 실제 성능을 검증하기 위해 수행된 인간 평가에서, Llama 2-Chat은 약 4,000개의 단일 및 다중 턴 프롬프트에 대해 우수한 성과를 보여주었습니다. 특히 주목할 만한 점은 95% 신뢰구간이 1-2% 사이로 매우 안정적인 결과를 보여주었다는 것입니다.

안전성 측면에서도 Llama 2는 획기적인 진전을 이루었습니다. 특별히 설계된 안전성 데이터 주석 작업과 레드팀(red teaming) 평가를 통해 모델의 잠재적 위험을 최소화했습니다. 이는 [Bai와 연구진]이 제안한 인간 피드백 강화학습(RLHF) 방법론을 확장 적용한 결과입니다. RLHF 과정은 다음과 같은 수식으로 표현됩니다.

$$ L_{RLHF}(\theta) = \mathbb{E}_{(x,y)\sim D} [\min(r_\theta(x,y)A(x,y), \text{clip}(r_\theta(x,y), 1-\epsilon, 1+\epsilon)A(x,y))] $$

여기서 \\(r_\theta\\)는 정책 비율을, \\(A(x,y)\\)는 이점 함수를 나타냅니다.

Llama 2의 학습 과정은 크게 세 단계로 구성됩니다. 먼저 공개 데이터를 활용한 사전 학습을 진행하고, 이어서 지도 학습 기반의 미세조정(SFT)을 수행합니다. 마지막으로 RLHF를 통해 모델의 응답을 인간의 선호도에 맞게 최적화합니다. 이러한 단계적 접근은 모델의 기본 능력을 보존하면서도 실용적인 대화 시스템으로서의 성능을 극대화하는데 기여했습니다.

특히 주목할 만한 점은 Llama 2가 도구 사용과 시간적 지식 구조화와 같은 새로운 능력을 자발적으로 발현했다는 것입니다. 이는 대규모 언어 모델이 단순한 텍스트 생성을 넘어 보다 복잡한 인지 능력을 개발할 수 있다는 가능성을 보여줍니다.

### Llama 2의 사전학습 방법론과 기술적 혁신

#### 사전학습 데이터 처리와 아키텍처 개선

Llama 2는 [Touvron과 연구진](https://arxiv.org/pdf/2302.13971)이 제시한 최적화된 자기회귀 트랜스포머 모델을 기반으로 하되, 여러 핵심적인 개선사항을 도입했습니다. 특히 데이터 품질 관리 측면에서 더욱 엄격한 정제 과정을 거쳤으며, 문맥 길이를 2배로 확장하고 대규모 모델의 추론 확장성을 개선하기 위해 그룹 쿼리 어텐션(GQA)을 도입했습니다.

사전학습 데이터는 공개적으로 이용 가능한 소스에서 수집되었으며, 개인정보가 포함된 웹사이트의 데이터는 특별히 제외되었습니다. 총 2조 개의 토큰으로 학습을 진행했는데, 이는 성능과 비용 사이의 최적의 균형점을 찾은 결과입니다. 특히 사실에 기반한 소스의 비중을 높여 모델의 지식을 강화하고 환각 현상을 줄이는데 주력했습니다.

#### 학습 세부 사항과 최적화 전략

학습 과정에서는 AdamW 최적화기를 사용했으며, 다음과 같은 하이퍼파라미터를 적용했습니다.

$$ \beta_1 = 0.9, \beta_2 = 0.95, \epsilon = 10^{-5} $$

코사인 학습률 스케줄을 채택했으며, 2000 스텝의 웜업 기간을 거친 후 최종 학습률을 피크 학습률의 10%까지 감소시켰습니다. 가중치 감쇠는 \\(0.1\\)을, 그래디언트 클리핑은 \\(1.0\\)을 적용했습니다.

![Llama 2 모델들의 학습 손실](https://ar5iv.org//html/2307.09288/assets/x4.png)  
이 그래프는 Llama 2 모델 계열의 학습 손실을 보여줍니다. 2조 토큰으로 사전학습을 진행한 후에도 모델들이 포화 상태에 도달하지 않았다는 점이 주목할 만합니다.

#### 토크나이저 구현과 하드웨어 구성

토크나이저는 Llama 1과 동일하게 바이트페어 인코딩(BPE) 알고리즘을 사용했으며, SentencePiece 구현체를 활용했습니다. 모든 숫자는 개별 자릿수로 분할되고, 알 수 없는 UTF-8 문자는 바이트 단위로 분해됩니다. 전체 어휘 크기는 32k 토큰으로 설정되었습니다.

학습은 Meta의 Research Super Cluster(RSC)와 내부 프로덕션 클러스터에서 진행되었으며, 두 환경 모두 NVIDIA A100 GPU를 사용했습니다. 주목할 만한 차이점은 인터커넥트 방식과 GPU 전력 소비 제한에 있습니다. RSC는 NVIDIA Quantum InfiniBand를, 프로덕션 클러스터는 RoCE를 사용했으며, GPU 전력 소비 제한은 각각 400W와 350W로 설정되었습니다.

#### 환경적 영향과 지속가능성

사전학습 과정의 탄소 배출량은 총 539 tCO₂eq로 추정되며, 이는 Meta의 지속가능성 프로그램을 통해 100% 상쇄되었습니다. 특히 주목할 만한 점은, 모델을 공개함으로써 다른 기업들이 사전학습 비용을 중복해서 지출할 필요가 없게 되었다는 것입니다. 이는 글로벌 자원의 효율적 활용이라는 측면에서 중요한 의미를 갖습니다.

### Llama 2-Chat의 미세조정 기술과 구현

#### 미세조정 전략의 혁신적 접근

Llama 2-Chat의 미세조정 과정은 수개월에 걸친 연구와 반복적인 정렬 기법의 적용을 통해 완성되었습니다. 이 과정은 지도 학습 기반 미세조정(Supervised Fine-Tuning, SFT)과 인간 피드백 강화학습(Reinforcement Learning from Human Feedback, RLHF)의 두 가지 주요 단계로 구성되었으며, 각 단계는 상당한 계산 자원과 주석 작업을 필요로 했습니다.

#### 지도 학습 기반 미세조정의 혁신

SFT 단계에서 가장 주목할 만한 발견은 "품질이 곧 전부"라는 원칙입니다. Zhou와 연구진의 연구 결과와 유사하게, 소수의 고품질 지시 학습 데이터만으로도 우수한 성능을 달성할 수 있다는 것을 확인했습니다. 실제로 27,540개의 주석만으로 높은 품질의 결과를 얻을 수 있었습니다.

미세조정의 기술적 구현에서는 다음과 같은 하이퍼파라미터를 사용했습니다.

$$ \text{learning_rate} = 2 \times 10^{-5} $$,
$$ \text{weight_decay} = 0.1 $$,
$$ \text{batch_size} = 64 $$,
$$ \text{sequence_length} = 4096 $$

### RLHF와 보상 모델링의 혁신

RLHF 구현에서는 두 가지 주요 알고리즘을 탐구했습니다. PPO(Proximal Policy Optimization)와 Rejection Sampling입니다. 보상 모델링을 위한 이진 순위 손실 함수는 다음과 같이 정의됩니다.

$$ \mathcal{L}_{\text{ranking}} = -\text{log}(\sigma(r_{\theta}(x,y_{c})-r_{\theta}(x,y_{r})-m(r))) $$

여기서 \\(r_{\theta}(x,y)\\)는 프롬프트 \\(x\\)와 응답 \\(y\\)에 대한 보상 모델의 스칼라 출력을, \\(m(r)\\)은 선호도 등급에 따른 마진을 나타냅니다.

![보상 모델의 정확도 스케일링 트렌드](https://ar5iv.org//html/2307.09288/assets/x5.png)  
이 그래프는 데이터와 모델 크기에 따른 보상 모델의 정확도 향상을 보여줍니다. 더 큰 모델이 동일한 데이터 양에 대해 더 높은 성능을 달성하며, 현재 데이터 양으로도 성능이 포화되지 않았음을 알 수 있습니다.

#### Ghost Attention 메커니즘

다중 턴 대화의 일관성 문제를 해결하기 위해 도입된 Ghost Attention(GAtt)은 주목할 만한 혁신입니다. 이 메커니즘은 시스템 메시지를 효과적으로 활용하여 대화 전반에 걸쳐 일관된 제약 조건을 유지합니다.

![GAtt 적용 전후의 어텐션 시각화](https://ar5iv.org//html/2307.09288/assets/img/data_toxicity.png)  
이 시각화는 GAtt의 효과를 명확히 보여줍니다. 오른쪽의 GAtt 적용 모델이 시스템 메시지에 대해 더 강한 어텐션 활성화를 유지하는 것을 확인할 수 있습니다.

#### 성능 평가 및 검증

모델의 성능은 약 4,000개의 프롬프트에 대한 인간 평가를 통해 검증되었습니다. Llama 2-Chat은 오픈소스 모델들을 큰 차이로 능가했으며, 특히 7B 모델은 MPT-7B-chat 대비 60%의 승률을, 34B 모델은 Vicuna-33B와 Falcon 40B 모델 대비 75% 이상의 승률을 기록했습니다.

이러한 포괄적인 미세조정 접근법을 통해 Llama 2-Chat은 안전성과 유용성 면에서 균형 잡힌 성능을 달성했으며, 특히 다중 턴 대화에서의 일관성 문제를 효과적으로 해결했습니다.

### Llama 2의 안전성 강화 기술과 평가

#### 사전학습 데이터의 안전성 분석

Llama 2의 개발 과정에서 가장 중요하게 고려된 요소 중 하나는 모델의 안전성입니다. [Weidinger과 연구진](https://arxiv.org/pdf/2112.04359)이 제시한 프레임워크를 기반으로, Meta 연구진은 사전학습 데이터의 투명성과 잠재적 위험 요소를 철저히 분석했습니다.

데이터 안전성 분석은 크게 세 가지 차원에서 이루어졌습니다. 첫째, 인구통계학적 대표성 분석에서는 영어 말뭉치에서 대명사 사용 빈도를 조사했습니다. 분석 결과, 'He' 계열 대명사가 'She' 계열 대명사보다 더 높은 빈도로 등장하는 것으로 나타났습니다. 이는 모델이 'She' 관련 문맥에 대해 상대적으로 적은 학습을 할 수 있다는 것을 시사합니다.

![데이터 독성 분포](https://ar5iv.org//html/2307.09288/assets/x16.png)  
데이터의 독성 수준은 ToxiGen 데이터셋에서 미세조정된 HateBERT 분류기를 사용하여 측정되었습니다. 분석 결과, 전체 문서의 약 0.2%만이 0.5 이상의 독성 점수를 기록했습니다. 이는 사전학습 데이터에 유해 콘텐츠가 상대적으로 적게 포함되어 있음을 보여줍니다.

#### 안전성 미세조정 기술

Llama 2의 안전성 강화를 위해 세 가지 핵심 기술이 도입되었습니다. 첫째, 지도 학습 기반 안전성 미세조정에서는 적대적 프롬프트와 안전한 응답 예시를 수집하여 모델을 학습시켰습니다. 이 과정은 다음과 같은 손실 함수를 통해 최적화되었습니다.

$$ L_{safety} = -\sum_{i=1}^{N} y_i \log(p_i) + (1-y_i)\log(1-p_i) $$

여기서 \\(y_i\\)는 안전한 응답의 레이블을, \\(p_i\\)는 모델의 예측 확률을 나타냅니다.

둘째, 안전성 RLHF에서는 별도의 보상 모델을 학습시켜 모델의 응답을 안전성 기준에 맞게 조정했습니다. 보상 모델의 스코어는 다음과 같이 계산됩니다.

$$ R_{safety}(x,y) = w_1R_{toxicity}(y) + w_2R_{bias}(y) + w_3R_{truthfulness}(y) $$

마지막으로, 문맥 증류 기법을 통해 안전성 관련 프롬프트의 효과를 모델에 내재화했습니다. 이 과정에서는 안전성 보상 모델을 활용하여 각 샘플별로 문맥 증류의 적용 여부를 결정했습니다.

#### 레드팀 평가와 안전성 검증

350명 이상의 전문가로 구성된 레드팀은 다양한 위험 카테고리에 대해 모델을 평가했습니다. 평가 결과는 모델의 견고성(robustness) 지표 \\(\gamma\\)로 정량화되었으며, 이는 시간당 발견되는 위반 사례의 평균 수를 나타냅니다. 7B 모델의 경우, 여러 차례의 개선을 통해 \\(\gamma\\) 값이 1.8에서 0.45로 감소했습니다.

![위반율 분석](https://ar5iv.org//html/2307.09288/assets/x18.png)  
안전성 평가 결과, Llama 2-Chat은 모든 모델 크기에서 낮은 위반율을 보였습니다. 특히 70B 모델의 경우 ToxiGen 벤치마크에서 0.01%라는 매우 낮은 독성 생성률을 기록했으며, TruthfulQA에서는 64.14%의 높은 정확도를 달성했습니다.

이러한 포괄적인 안전성 강화 노력을 통해 Llama 2는 높은 수준의 안전성과 신뢰성을 확보했으며, 이는 실제 응용 환경에서의 안정적인 활용을 가능하게 합니다.

### Llama 2의 주요 발견과 기술적 통찰

#### RLHF의 혁신적 성과와 인간 지도 학습의 한계 극복

Llama 2 개발 과정에서 가장 주목할 만한 발견은 강화학습 기반 인간 피드백(RLHF)의 놀라운 효과성입니다. 초기에는 많은 연구자들이 더 밀도 높은 신호를 제공하는 지도 학습 방식을 선호했으나, RLHF가 비용과 시간 효율성 측면에서 탁월한 성과를 보여주었습니다. 

RLHF의 성공은 인간과 LLM 사이의 시너지에 기인합니다. 지도 학습에서는 각 주석 작성자의 개인적 변동성이 모델에 그대로 반영되어, 때로는 낮은 품질의 주석까지도 학습되는 문제가 있었습니다. 반면 RLHF에서는 인간이 두 출력을 비교하는 과정에서 더 일관된 판단을 제공할 수 있었습니다. 이는 다음과 같은 수식으로 표현되는 보상 메커니즘을 통해 구현됩니다.

$$ R(x, y) = \mathbb{E}_{h \sim H}[P_h(y_1 > y_2 | x)] $$

여기서 \\(H\\)는 인간 평가자 집단을, \\(P_h\\)는 평가자 \\(h\\)의 선호도 판단을 나타냅니다.

![분포 이동 분석](https://ar5iv.org//html/2307.09288/assets/x25.png)  
이 그래프는 RLHF를 통해 모델의 출력 분포가 점진적으로 개선되는 과정을 보여줍니다. 특히 낮은 품질의 응답들이 점차 제거되면서 전체적인 분포가 우측으로 이동하는 것을 확인할 수 있습니다.

#### 문맥 기반 온도 조절 메커니즘

Llama 2는 프롬프트의 특성에 따라 자동으로 온도 파라미터를 조절하는 독특한 능력을 보여주었습니다. 이는 Self-BLEU 메트릭을 통해 정량적으로 분석되었으며, 다음과 같이 계산됩니다.

$$ \text{Self-BLEU}(Y) = \frac{1}{|Y|}\sum_{y_i \in Y}\max_{y_j \in Y\setminus\{y_i\}}\text{BLEU}(y_i, y_j) $$

창의적 프롬프트(예: 시 작성)에 대해서는 높은 온도에서도 다양성이 유지되는 반면, 사실 기반 프롬프트(예: 수도 질문)에 대해서는 온도와 관계없이 일관된 응답을 생성하는 것으로 나타났습니다.

#### 시간 인식과 도구 활용 능력

단 1,000개의 시간 관련 SFT 데이터만으로도 Llama 2는 놀라운 시간적 추론 능력을 보여주었습니다. 더욱 주목할 만한 점은 도구 사용 능력의 자발적 출현입니다. 특히 수학 문제 해결에서 Llama 2-Chat은 67.1%의 정확도를 달성하여, GPT-3(14.0%)나 Toolformer(40.4%)를 크게 앞섰습니다.

이러한 발견들은 대규모 언어 모델이 단순한 텍스트 생성을 넘어 더 복잡한 인지 능력을 개발할 수 있다는 가능성을 보여줍니다. 특히 RLHF를 통한 학습이 이러한 능력의 출현에 핵심적인 역할을 한다는 점은 향후 AI 발전 방향에 중요한 시사점을 제공합니다.

### 대규모 언어 모델의 발전과 관련 연구

#### 대규모 언어 모델의 진화

대규모 언어 모델(LLM)의 발전은 [Kaplan과 연구진](https://arxiv.org/pdf/2001.08361)이 제시한 스케일링 법칙을 기반으로 급속히 진행되어 왔습니다. 이들의 연구는 모델 성능이 매개변수 수 \\(N\\), 데이터셋 크기 \\(D\\), 총 학습 계산량 \\(C\\)와의 관계에서 다음과 같은 멱법칙을 따른다는 것을 밝혀냈습니다.

$$ L(N,D) = [(N_c/N)^{\alpha_N/\alpha_D} + D_c/D]^{\alpha_D} $$

여기서 \\(\alpha_N \approx 0.076\\), \\(\alpha_D \approx 0.095\\)는 실험적으로 도출된 지수입니다. 이러한 이론적 기반을 토대로 GPT-3에서 Gopher에 이르기까지 100B 이상의 매개변수를 가진 여러 모델들이 개발되었습니다.

#### 효율성과 개방성의 균형

[Hoffmann과 연구진](https://arxiv.org/pdf/2203.15556)의 Chinchilla 연구는 기존의 스케일링 법칙을 재정의했습니다. 그들은 계산 효율성 최적화를 위해 다음과 같은 관계식을 제시했습니다.

$$ N_{opt} \propto C^{0.5}, D_{opt} \propto C^{0.5} $$

이러한 발견을 바탕으로 [Touvron과 연구진](https://arxiv.org/pdf/2302.13971)이 개발한 Llama는 추론 시의 계산 효율성에 중점을 두어 설계되었으며, 이는 개방형 모델의 새로운 기준을 제시했습니다.

#### 지시 학습과 인간 피드백 강화학습

지시 학습의 효과성은 [Wei과 연구진](https://arxiv.org/pdf/2109.01652)의 연구에서 처음 입증되었습니다. 이들은 다양한 작업에 대한 제로샷 성능을 향상시키는 방법을 제시했으며, 이는 다음과 같은 목적 함수를 통해 최적화됩니다.

$$ L_{instruction} = -\mathbb{E}_{(x,y)\sim D}[\log p_\theta(y|x)] $$

RLHF는 이러한 지시 학습을 더욱 발전시킨 형태로, [Christiano과 연구진](https://arxiv.org/pdf/1706.03741)이 제안한 방법론을 기반으로 합니다. 특히 [Ouyang과 연구진](https://arxiv.org/pdf/2203.02155)은 지시 학습과 RLHF를 결합하여 사실성, 유해성, 유용성 측면에서의 개선을 이루어냈습니다.

#### 안전성 과제와 해결 방안

LLM의 안전성 문제는 [Bender과 연구진](https://arxiv.org/pdf/2112.04359)과 [Weidinger과 연구진](https://arxiv.org/pdf/2112.04359)이 체계적으로 분석했습니다. 이들은 편향성, 유해성, 개인정보 유출 등의 위험을 다음과 같은 메트릭으로 정량화했습니다.

$$ R_{safety} = w_1R_{bias} + w_2R_{toxicity} + w_3R_{privacy} $$

이러한 위험을 완화하기 위해 [Roller과 연구진](https://arxiv.org/pdf/2004.13637)은 대화형 LLM에 특화된 안전성 프레임워크를 제안했으며, Deng과 연구진은 이를 분류 체계로 체계화했습니다. 특히 레드팀 평가를 통한 안전성 검증은 모델 배포 전 필수적인 단계로 자리잡았습니다.

### Llama 2의 결론과 미래 전망

#### 기술적 성과와 혁신

Llama 2는 7B에서 70B에 이르는 다양한 규모의 매개변수를 가진 모델 제품군으로서, 기존 오픈소스 대화 모델들과의 경쟁에서 우위를 보여주었을 뿐만 아니라, 일부 평가 지표에서는 독점 모델들과 대등한 수준의 성능을 달성했습니다. 특히 주목할 만한 점은 GPT-4와 같은 최상위 모델과는 아직 격차가 있음에도 불구하고, 공개 데이터만을 사용하여 이러한 성과를 이루어냈다는 것입니다.

성능 평가는 다음과 같은 메트릭을 통해 정량화되었습니다.

$$ P_{comparative} = \frac{1}{N}\sum_{i=1}^N \mathbb{I}(s_{\text{Llama2}}^i > s_{\text{baseline}}^i) $$

여기서 \\(s_{\text{Llama2}}^i\\)와 \\(s_{\text{baseline}}^i\\)는 각각 Llama 2와 비교 모델의 i번째 평가 점수를 나타냅니다.

#### 안전성과 유용성의 균형

Llama 2의 개발 과정에서 가장 중요하게 고려된 것은 유용성과 안전성의 균형입니다. [Weidinger과 연구진](https://arxiv.org/pdf/2112.04359)이 제시한 프레임워크를 기반으로, 다음과 같은 통합 평가 지표를 도입했습니다.

$$ R_{total} = \alpha R_{utility} + (1-\alpha)R_{safety} $$

여기서 \\(\alpha\\)는 두 요소 간의 균형을 조절하는 가중치입니다. 이러한 접근은 모델의 실용적 가치를 극대화하면서도 잠재적 위험을 최소화하는데 기여했습니다.

#### 연구 커뮤니티에 대한 기여

Llama 2와 Llama 2-Chat의 책임있는 공개는 AI 연구의 민주화에 중요한 이정표를 제시했습니다. 특히 모델의 학습 방법론과 기술적 세부사항을 상세히 공개함으로써, 연구 커뮤니티가 이를 기반으로 더 나은 모델을 개발할 수 있는 토대를 마련했습니다.

#### 향후 연구 방향

Meta는 Llama 2-Chat의 지속적인 개선을 약속하며, 특히 다음과 같은 영역에 초점을 맞출 계획입니다.

$$ \Delta_{improvement} = f(R_{feedback}, R_{safety}, R_{capability}) $$

이 수식은 사용자 피드백(\\(R_{feedback}\\)), 안전성 평가(\\(R_{safety}\\)), 그리고 모델 능력(\\(R_{capability}\\))을 통합적으로 고려한 개선 방향을 나타냅니다.

투명성과 안전성에 대한 Meta의 지속적인 노력은 AI 기술의 책임있는 발전을 위한 모범 사례를 제시하고 있으며, 이는 향후 대규모 언어 모델 연구의 중요한 지침이 될 것으로 기대됩니다.

### 부록

#### 사전학습 아키텍처의 혁신

Llama 2는 Llama 1 대비 여러 가지 중요한 아키텍처 개선을 도입했습니다. 가장 주목할 만한 변화는 문맥 길이(context length)의 확장으로, 2048 토큰에서 4096 토큰으로 두 배 증가했습니다. 이러한 확장의 효과는 SCROLLS와 같은 긴 문맥 벤치마크에서 명확히 드러났습니다. 예를 들어, NarrativeQA에서 2k 문맥 길이 모델이 0.21의 F1 점수를 기록한 반면, 4k 모델은 17.26으로 큰 폭의 성능 향상을 보였습니다.

문맥 길이 확장의 효과는 다음과 같은 수식으로 정량화할 수 있습니다.

$$ \Delta_{performance} = \frac{Score_{4k} - Score_{2k}}{Score_{2k}} \times 100\% $$

#### Grouped-Query Attention의 구현

Grouped-Query Attention(GQA)은 멀티헤드 어텐션(MHA)의 메모리 효율적인 변형으로 구현되었습니다. GQA의 핵심 수식은 다음과 같습니다.

$$ Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

여기서 \\(Q \in \mathbb{R}^{B \times H \times L \times d_k}\\), \\(K \in \mathbb{R}^{B \times G \times d_k \times d_v}\\), \\(V \in \mathbb{R}^{B \times G \times d_v \times L}\\)이며, \\(G\\)는 그룹 수를 나타냅니다.

GQA의 구현은 다음과 같은 코드로 이루어집니다.

```python
def grouped_query_attention(q, k, v, num_groups):
    batch_size, num_heads, seq_len, head_dim = q.shape
    # 키와 값을 그룹으로 나누어 공유
    k = k.view(batch_size, num_groups, -1, seq_len, head_dim)
    v = v.view(batch_size, num_groups, -1, seq_len, head_dim)
    
    # 어텐션 계산
    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(head_dim)
    attn = torch.softmax(scores, dim=-1)
    output = torch.matmul(attn, v)
    return output
```

![GQA 성능 비교](https://ar5iv.org//html/2307.09288/assets/x35.png)  
이 그래프는 GQA가 큰 배치 크기에서 더 높은 처리량을 달성하면서도 작은 배치에서는 MHA와 유사한 지연 시간을 보여준다는 것을 보여줍니다. 특히 256 토큰 문맥에서 MHA는 배치 크기 1024에서 메모리 부족 오류가 발생하는 반면, GQA는 이러한 제한 없이 계속 확장할 수 있었습니다.

#### 벤치마크 성능 평가

Llama 2는 다양한 벤치마크에서 평가되었습니다. MMLU(Massive Multitask Language Understanding) 평가에서 70B 모델은 인문학 분야에서 65.0%, STEM 분야에서 58.0%의 정확도를 달성했습니다. 특히 주목할 만한 점은 수학적 추론 능력으로, GSM8k 벤치마크에서 70B 모델이 56.8%의 정확도를 기록했습니다. 이는 다음과 같은 평가 메트릭을 통해 측정되었습니다.

$$ Accuracy_{GSM8k} = \frac{\text{Correct Solutions}}{\text{Total Problems}} \times 100\% $$

이러한 포괄적인 평가 결과는 Llama 2가 다양한 작업에서 강건한 성능을 보여주며, 특히 긴 문맥이 필요한 작업에서 문맥 길이 확장과 GQA의 도입이 효과적이었음을 입증합니다.

- - -
#### References
* [Llama 2: Open Foundation and Fine-Tuned Chat Models](http://arxiv.org/pdf/2307.09288v2)
