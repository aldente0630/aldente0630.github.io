---
layout: post
title: "LLaMA: Open and Efficient Foundation Language Models"
date: 2023-02-27 17:11:15
author: "Meta AI"
categories: "Natural-Language-Processing"
tags: ["Transformer-Architecture", "AdamW-Optimizer", "Byte-Pair-Encoding", "Causal-Multi-Head-Attention", "Checkpointing", "Cosine-Learning-Rate-Schedule", "Gradient-Clipping", "Rotary-Positional-Embeddings", "SwiGLU-Activation-Function", "Toxicity-and-Bias-Evaluation"]
use_math: true
---
### TL;DR
#### 이 연구를 시작하게 된 배경과 동기는 무엇입니까?

대규모 언어 모델(LLM) 연구는 그동안 주로 독점적이거나 접근이 제한된 데이터셋을 활용하여 진행되어 왔습니다. GPT-3, PaLM, Chinchilla와 같은 최첨단 모델들은 뛰어난 성능을 보여주었지만, 이들의 학습 데이터와 방법론에 대한 투명성이 부족했습니다. 이러한 상황은 언어 모델 연구의 재현성과 접근성을 제한하는 요인이 되었습니다. Meta AI 연구진은 이러한 한계를 극복하고, 공개 데이터만으로도 최고 수준의 성능을 달성할 수 있는 언어 모델을 개발하고자 이 연구를 시작했습니다.

#### 이 연구에서 제시하는 새로운 해결 방법은 무엇입니까?

LLaMA는 7B에서 65B 매개변수에 이르는 다양한 규모의 모델 시리즈를 통해 효율적인 학습 방법을 제시합니다. 특히 Chinchilla 스케일링 법칙을 적극적으로 활용하여, 모델 크기와 학습 데이터 양 사이의 최적 균형을 찾았습니다. 또한 Pre-normalization, SwiGLU 활성화 함수, Rotary Position Embeddings(RoPE)와 같은 최신 아키텍처 혁신을 도입하여 모델의 성능을 향상시켰습니다. 이러한 접근은 The Pile, C4, GitHub 코드 등 완전히 공개된 데이터셋만을 사용하면서도 최고 수준의 성능을 달성할 수 있음을 보여줍니다.

#### 제안된 방법은 어떻게 구현되었습니까?

LLaMA의 구현은 효율적인 학습을 위한 여러 기술적 최적화를 포함합니다. AdamW 최적화 알고리즘을 사용하여 β₁=0.9, β₂=0.95의 파라미터로 설정하였으며, 코사인 스케줄을 통한 학습률 조정을 적용했습니다. 특히 효율적인 인과적 다중 헤드 어텐션 구현을 통해 메모리 사용량을 최적화했으며, 활성화 재계산과 모델 병렬화를 통해 학습 효율성을 높였습니다. 65B 파라미터 모델의 경우, 2048개의 A100 GPU를 사용하여 GPU당 초당 약 380개의 토큰을 처리할 수 있는 성능을 달성했습니다.

#### 이 연구의 결과가 가지는 의미는 무엇입니까?

이 연구는 대규모 언어 모델 개발에 있어 중요한 전환점을 제시합니다. 특히 13B 매개변수 모델이 175B 매개변수의 GPT-3를 능가하고, 65B 모델이 PaLM-540B와 대등한 성능을 보여준 것은 모델 효율성 측면에서 큰 진전을 의미합니다. 더욱 중요한 것은 이러한 성과가 완전히 공개된 데이터만으로 달성되었다는 점입니다. 이는 언어 모델 연구의 민주화와 투명성 향상에 크게 기여할 것으로 기대됩니다. 또한 모델의 편향성, 유해성, 환경적 영향에 대한 체계적인 평가를 제공함으로써, 향후 언어 모델 개발에서 고려해야 할 중요한 지침을 제시했습니다.
- - -
## LLaMA: 공개 데이터로 훈련된 효율적인 기초 언어 모델

Meta AI 연구진이 개발한 LLaMA는 7B에서 65B 매개변수에 이르는 기초 언어 모델 시리즈로, 인공지능 연구 커뮤니티에 중요한 전환점을 제시합니다. 이 연구의 가장 주목할 만한 특징은 독점적이거나 접근이 제한된 데이터셋을 사용하지 않고, 오직 공개된 데이터셋만을 활용하여 최첨단 성능을 달성했다는 점입니다.

LLaMA의 기술적 혁신은 모델 규모와 성능 간의 효율적인 균형에서 찾을 수 있습니다. 특히 13B 매개변수를 가진 LLaMA-13B 모델이 175B 매개변수의 GPT-3를 대부분의 벤치마크에서 능가했다는 점은 주목할 만합니다. 더 나아가 LLaMA-65B는 Chinchilla-70B와 PaLM-540B와 같은 최고 성능의 모델들과 대등한 성능을 보여주었습니다.

이러한 성과는 Kaplan과 연구진이 제시한 언어 모델의 스케일링 법칙을 실증적으로 뒷받침합니다. 모델 학습에는 \\(2 \times 10^{-4}\\)에서 \\(1.25 \times 10^{-4}\\)까지의 학습률이 적용되었으며, 코사인 스케줄을 통해 학습 과정에서 10배씩 감소되도록 설계되었습니다. 이는 Loshchilov와 Hutter가 제안한 AdamW 최적화 기법의 효과적인 적용을 보여줍니다.

LLaMA의 아키텍처는 Vaswani와 연구진이 제안한 Transformer 모델을 기반으로 하되, 공개 데이터셋에 최적화된 형태로 수정되었습니다. 이 모델은 The Pile, C4, GitHub 코드 등 다양한 공개 데이터셋에서 수조 개의 토큰으로 학습되었으며, 이는 Hoffmann과 연구진의 연구에서 제시된 최적 학습 방법론을 따릅니다.

연구팀은 이 모델의 소스 코드를 GitHub를 통해 연구 커뮤니티에 공개함으로써, 언어 모델 개발의 민주화에 기여하고 있습니다. 이는 고성능 언어 모델 개발이 더 이상 대규모 독점 데이터셋에 의존할 필요가 없다는 것을 입증하며, 인공지능 연구의 새로운 지평을 열었다고 할 수 있습니다.

### LLaMA: 대규모 언어 모델의 새로운 패러다임

대규모 언어 모델(Large Language Models, LLMs)의 발전은 인공지능 분야에서 괄목할 만한 성과를 이루어왔습니다. [Brown과 연구진](https://arxiv.org/abs/2005.14165)이 제시한 것처럼, 방대한 텍스트 데이터로 학습된 LLM들은 텍스트 지시사항이나 퓨샷 샘플만으로도 새로운 작업을 수행할 수 있는 능력을 보여주었습니다. 이러한 성과는 Kaplan과 연구진이 연구한 모델 규모의 확장에 따른 것이었으며, 이는 PaLM과 같은 더욱 큰 규모의 모델 개발로 이어졌습니다.

하지만 최근 Hoffmann과 연구진의 연구는 기존의 "더 큰 모델이 더 나은 성능을 보인다"는 통념에 도전장을 던졌습니다. 그들의 연구는 주어진 계산 자원 내에서 최고의 성능은 가장 큰 모델이 아닌, 더 많은 데이터로 학습된 상대적으로 작은 모델에서 달성될 수 있다는 것을 보여주었습니다. 이는 특히 실제 서비스 환경에서 중요한 의미를 갖습니다. 추론(inference) 단계에서의 효율성을 고려할 때, 동일한 성능 목표를 달성하는데 있어 더 작은 모델을 더 오래 학습시키는 것이 더 큰 모델을 사용하는 것보다 비용 효율적일 수 있기 때문입니다.

이러한 맥락에서 LLaMA는 주목할 만한 혁신을 보여줍니다. 7B에서 65B 매개변수에 이르는 다양한 규모의 모델들이 기존의 최고 성능 LLM들과 견줄 만한 성능을 달성했습니다. 특히 주목할 만한 점은 LLaMA-13B가 GPT-3보다 10배 작은 규모임에도 불구하고 대부분의 벤치마크에서 더 우수한 성능을 보였다는 것입니다. 이는 단일 GPU에서도 실행 가능한 수준의 모델이 고성능 언어 모델 연구를 더욱 접근 가능하게 만들었음을 의미합니다.

더욱 중요한 것은 LLaMA가 Chinchilla나 PaLM과 달리 전적으로 공개 데이터만을 사용했다는 점입니다. 이는 기존의 "Books-2TB"나 "소셜 미디어 대화" 등 문서화되지 않거나 비공개 데이터에 의존하던 관행에서 벗어나, 연구의 재현성과 투명성을 높였습니다. OPT, GPT-NeoX, BLOOM, GLM과 같은 오픈소스 모델들도 있었지만, PaLM-62B나 Chinchilla와 경쟁할 만한 성능을 보여주지는 못했었습니다.

이러한 LLaMA의 성과는 대규모 언어 모델 개발에 있어 새로운 지평을 열었으며, 특히 공개 데이터만으로도 최고 수준의 성능을 달성할 수 있다는 것을 입증했습니다. 이는 인공지능 연구의 민주화와 접근성 향상에 큰 기여를 할 것으로 기대됩니다.

### LLaMA의 학습 방법론

LLaMA의 학습 접근 방식은 Brown과 연구진과 Chowdhery와 연구진이 제시한 방법론을 기반으로 하되, Hoffmann과 연구진의 Chinchilla 스케일링 법칙을 적극적으로 반영했습니다. 이 접근 방식의 핵심은 표준 최적화 기법을 사용하여 대규모 트랜스포머 모델을 방대한 텍스트 데이터로 학습시키는 것입니다.

트랜스포머 아키텍처의 핵심인 어텐션 메커니즘은 다음과 같은 수학적 형태로 구현됩니다.

$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

여기서 \\(Q\\), \\(K\\), \\(V\\)는 각각 쿼리, 키, 밸류 행렬을 나타내며, \\(d_k\\)는 키 벡터의 차원을 의미합니다. 이는 코드에서 다음과 같이 구현됩니다.

```python
scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
scores = F.softmax(scores.float(), dim=-1).type_as(xq)
output = torch.matmul(scores, values)
```

모델의 학습은 Chinchilla 스케일링 법칙에 따라 최적화되었습니다. 이 법칙은 계산 예산이 주어졌을 때 모델 크기와 학습 데이터의 양 사이의 최적 균형을 제시합니다. 구체적으로, 모델 크기를 두 배로 늘릴 때마다 학습 토큰의 수도 두 배로 늘려야 한다는 것을 수식으로 표현하면 다음과 같습니다.

$$ N_{optimal} \propto D_{optimal} $$

여기서 \\(N_{optimal}\\)은 최적의 모델 파라미터 수이고, \\(D_{optimal}\\)은 최적의 학습 토큰 수입니다.

모델의 구현에서는 특히 효율적인 학습을 위한 여러 기술적 최적화가 적용되었습니다. 예를 들어, RMSNorm을 사용한 정규화는 다음과 같이 구현됩니다.

```python
def _norm(self, x):
    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
```

이러한 구현은 학습 과정의 안정성을 높이면서도 계산 효율성을 유지합니다. 또한 모델 병렬화를 위해 ColumnParallelLinear와 RowParallelLinear 레이어를 사용하여 대규모 분산 학습을 가능하게 했습니다.

이러한 접근 방식은 기존의 대규모 언어 모델들과 비교했을 때 더 효율적인 학습을 가능하게 했습니다. 특히 주목할 만한 점은 더 작은 모델 크기로도 GPT-3와 같은 대형 모델들과 견줄 만한 성능을 달성했다는 것입니다. 이는 Chinchilla 스케일링 법칙의 실효성을 입증하는 동시에, 효율적인 모델 설계와 학습 방법의 중요성을 보여줍니다.

### LLaMA의 사전 학습 데이터셋 구성과 처리 방법

LLaMA 모델의 사전 학습 데이터셋은 다양한 공개 데이터 소스를 활용하여 구성되었습니다. 이 데이터셋의 핵심은 CommonCrawl이 전체의 67%를 차지하며, 이는 CCNet 파이프라인을 통해 처리되었습니다. Wenzek과 연구진이 개발한 이 파이프라인은 줄 단위 중복 제거, 언어 식별, 그리고 n-gram 언어 모델을 통한 품질 필터링을 수행합니다. 특히 주목할 만한 점은 Wikipedia 참조로 사용된 페이지와 무작위 샘플링된 페이지를 구분하기 위해 선형 분류 모델을 훈련시켜 적용했다는 것입니다.

데이터셋의 두 번째로 큰 부분을 차지하는 것은 C4로, 전체의 15%를 차지합니다. Raffel과 연구진이 개발한 C4는 CCNet과 유사한 중복 제거와 언어 식별 단계를 포함하지만, 품질 필터링에 있어 문장부호의 존재나 단어 및 문장 수와 같은 휴리스틱을 주로 활용한다는 점에서 차이가 있습니다.

GitHub 데이터(4.5%)는 Apache, BSD, MIT 라이선스로 배포된 프로젝트만을 선별적으로 포함했습니다. 데이터 품질을 보장하기 위해 줄 길이나 영숫자 문자 비율을 기반으로 한 휴리스틱 필터링을 적용했으며, 정규 표현식을 사용하여 헤더와 같은 상용구를 제거했습니다. 파일 수준에서의 정확한 일치를 기반으로 한 중복 제거도 수행되었습니다.

Wikipedia 데이터(4.5%)는 2022년 6월에서 8월 사이의 덤프를 사용했으며, 라틴 문자나 키릴 문자를 사용하는 20개 언어를 포함합니다. 데이터 처리 과정에서는 하이퍼링크, 주석, 서식 관련 상용구를 제거하는 작업이 수행되었습니다.

토큰화 과정에서는 Kudo와 Richardson이 개발한 SentencePiece 구현체를 사용하여 byte-pair encoding(BPE) 알고리즘을 적용했습니다. 특히 모든 숫자를 개별 자릿수로 분할하고, 알 수 없는 UTF-8 문자는 바이트 단위로 분해하는 방식을 채택했습니다. 이러한 토큰화 과정을 거친 후 전체 학습 데이터셋은 약 1.4T 토큰을 포함하게 되었습니다.

토큰화 구현은 다음과 같은 코드로 이루어졌습니다.

```python
class Tokenizer:
    def __init__(self, model_path: str):
        self.sp_model = SentencePieceProcessor(model_file=model_path)
        self.n_words = self.sp_model.vocab_size()
        self.bos_id = self.sp_model.bos_id()
        self.eos_id = self.sp_model.eos_id()
        self.pad_id = self.sp_model.pad_id()
```

이러한 데이터 처리와 토큰화 전략은 모델의 학습 효율성과 성능에 직접적인 영향을 미치는 중요한 요소입니다. 특히 Wikipedia와 Books 도메인의 데이터에 대해서는 약 2회의 에포크를 수행했으며, 나머지 데이터에 대해서는 각 토큰을 한 번씩만 사용하여 학습을 진행했습니다.

### LLaMA의 혁신적 아키텍처 설계

LLaMA는 Vaswani와 연구진이 제안한 Transformer 아키텍처를 기반으로 하되, 최신 연구 결과들을 통합하여 성능과 학습 안정성을 크게 개선했습니다. 주요 아키텍처 혁신은 세 가지 핵심 영역에서 이루어졌습니다.

첫째, Pre-normalization 기법을 도입했습니다. GPT-3에서 영감을 받은 이 접근법은 각 Transformer 서브레이어의 출력이 아닌 입력을 정규화합니다. Zhang과 Sennrich가 제안한 RMSNorm을 사용하여 다음과 같이 구현됩니다.

```python
def _norm(self, x):
    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
```

이는 수학적으로 다음과 같이 표현됩니다.

$$ \text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^n x_i^2 + \epsilon}} \cdot \gamma $$

여기서 \\(\gamma\\)는 학습 가능한 스케일 파라미터이고, \\(\epsilon\\)은 수치 안정성을 위한 작은 상수입니다.

둘째, Shazeer가 제안한 SwiGLU 활성화 함수를 도입했습니다. PaLM 모델에서 사용된 이 함수는 기존의 ReLU를 대체하며, 다음과 같이 정의됩니다.

$$ \text{SwiGLU}(x) = x \cdot \text{swish}_{\beta}(W_1x) \cdot W_2x $$

여기서 \\(\text{swish}_{\beta}(x) = x \cdot \sigma(\beta x)\\)이며, \\(\sigma\\)는 시그모이드 함수입니다. PaLM과 달리 LLaMA는 은닉층 차원을 \\(4d\\) 대신 \\(\frac{2}{3}4d\\)로 설정하여 계산 효율성을 개선했습니다.

셋째, Su와 연구진이 제안한 Rotary Position Embeddings(RoPE)를 각 레이어에 도입했습니다. 절대 위치 임베딩을 제거하고 RoPE를 사용함으로써, 모델이 시퀀스 내의 상대적 위치 정보를 더 효과적으로 학습할 수 있게 되었습니다. RoPE는 다음과 같이 구현됩니다.

```python
def apply_rotary_emb(xq, xk, freqs_cis):
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xk)
```

이러한 아키텍처 혁신의 효과는 학습 과정에서 명확하게 드러납니다. 아래 그림은 다양한 크기의 LLaMA 모델들의 학습 손실 곡선을 보여줍니다.

![Figure 1](https://ar5iv.org/html/2302.13971/assets/x1.png)

그래프에서 볼 수 있듯이, 모델 크기가 증가할수록 학습 손실이 더 낮아지는 경향을 보이며, 특히 33B와 65B 모델은 뛰어난 수렴 특성을 보여줍니다. 모든 모델은 4M 토큰의 배치 크기로 학습되었으며, 33B와 65B 모델은 1.4T 토큰으로, 더 작은 모델들은 1.0T 토큰으로 학습되었습니다.

이러한 아키텍처 설계는 모델의 효율성과 성능을 최적화하는 데 중요한 역할을 했으며, 특히 계산 자원이 제한된 환경에서도 높은 성능을 달성할 수 있게 했습니다. 이는 LLaMA가 기존의 대형 언어 모델들과 비교하여 더 적은 계산 자원으로도 경쟁력 있는 성능을 달성할 수 있었던 핵심 요인입니다.

### LLaMA의 최적화 전략과 구현

LLaMA 모델의 학습에는 Loshchilov와 Hutter가 제안한 AdamW 최적화 알고리즘이 사용되었습니다. AdamW는 기존 Adam 최적화 알고리즘의 변형으로, 가중치 감쇠(weight decay)를 적응적 학습률 업데이트와 분리하여 더 효과적인 정규화를 가능하게 합니다.

AdamW 최적화기의 핵심 파라미터는 다음과 같이 설정되었습니다.

$$ \beta_1 = 0.9, \beta_2 = 0.95 $$

여기서 \\(\beta_1\\)은 1차 모멘트 추정치의 지수 감소율을, \\(\beta_2\\)는 2차 모멘트 추정치의 지수 감소율을 나타냅니다. 이러한 값들은 경험적 연구를 통해 대규모 언어 모델 학습에 최적화된 것으로 알려져 있습니다.

학습률 스케줄링에는 코사인 스케줄이 적용되었으며, 최종 학습률은 최대 학습률의 10%가 되도록 설정되었습니다. 이는 다음과 같은 수식으로 표현됩니다.

$$ \eta_t = \eta_{final} + \frac{1}{2}(\eta_{max} - \eta_{final})(1 + \cos(\frac{t\pi}{T})) $$

여기서 \\(\eta_t\\)는 시간 \\(t\\)에서의 학습률, \\(\eta_{max}\\)는 최대 학습률, \\(\eta_{final}\\)은 최종 학습률, \\(T\\)는 총 학습 스텝 수입니다.

가중치 감쇠는 0.1로 설정되었으며, 이는 모델의 과적합을 방지하고 일반화 성능을 향상시키는 데 중요한 역할을 합니다. 또한 기울기 폭발을 방지하기 위해 1.0의 기울기 클리핑이 적용되었습니다. 이러한 설정은 다음과 같이 구현될 수 있습니다.

```python
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=max_lr,
    betas=(0.9, 0.95),
    weight_decay=0.1
)
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

모델 학습의 초기 단계에서는 2,000 스텝의 웜업 기간이 적용되었습니다. 이 기간 동안 학습률은 0에서 시작하여 점진적으로 최대값까지 증가하며, 이는 학습의 안정성을 높이는 데 기여합니다. 배치 크기는 모델의 크기에 따라 조정되었으며, 이는 메모리 효율성과 학습 안정성 사이의 균형을 맞추기 위한 것입니다.

이러한 최적화 전략은 LLaMA 모델의 효율적인 학습을 가능하게 했으며, 특히 대규모 모델에서도 안정적인 수렴을 달성할 수 있게 했습니다. AdamW 최적화기의 사용과 함께 적용된 다양한 정규화 기법들은 모델이 과적합 없이 높은 성능을 달성하는 데 핵심적인 역할을 했습니다.

### LLaMA의 효율적인 구현 전략

LLaMA 모델의 구현에서는 학습 속도를 향상시키기 위한 여러 가지 최적화 기법이 적용되었습니다. 이러한 최적화의 핵심은 Rabe와 Staats가 제안한 효율적인 인과적 다중 헤드 어텐션(causal multi-head attention) 구현에 있습니다. 이 구현은 xformers 라이브러리를 통해 제공되며, Dao와 연구진이 제안한 역전파 방식을 채택했습니다.

효율적인 어텐션 구현의 핵심 아이디어는 어텐션 가중치를 명시적으로 저장하지 않고, 인과적 언어 모델링 작업의 특성상 마스킹되는 키/쿼리 점수를 계산하지 않는 것입니다. 이는 수학적으로 다음과 같이 표현됩니다.

$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

이 수식은 다음과 같이 효율적으로 구현됩니다.

```python
def forward(self, x, start_pos, freqs_cis, mask):
    bsz, seqlen, _ = x.shape
    xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
    
    scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
    if mask is not None:
        scores = scores + mask
    scores = F.softmax(scores.float(), dim=-1).type_as(xq)
    output = torch.matmul(scores, values)
```

활성화 재계산(activation recomputation)을 통한 최적화도 중요한 역할을 합니다. 역전파 과정에서 재계산되는 활성화의 양을 줄이기 위해, 선형 레이어의 출력과 같이 계산 비용이 높은 활성화들을 저장합니다. 이는 PyTorch의 자동 미분 대신 트랜스포머 레이어에 대한 역전파 함수를 수동으로 구현함으로써 달성됩니다.

Korthikanti와 연구진이 제안한 모델 및 시퀀스 병렬화를 통해 이러한 최적화의 이점을 최대한 활용합니다. 또한 GPU 간의 네트워크 통신(all_reduce 연산으로 인한)과 활성화 계산을 최대한 중첩시켜 처리합니다.

이러한 최적화 기법들의 효과는 실제 성능 지표에서 명확하게 드러납니다. 65B 파라미터 모델을 학습할 때, 80GB RAM을 갖춘 2048개의 A100 GPU에서 GPU당 초당 약 380개의 토큰을 처리할 수 있습니다. 이는 1.4T 토큰을 포함하는 전체 데이터셋에 대한 학습이 약 21일 정도 소요됨을 의미합니다.

이러한 구현 최적화는 대규모 언어 모델의 학습을 현실적으로 가능하게 만드는 핵심 요소입니다. 특히 메모리 사용량과 계산 효율성의 균형을 맞추는 것이 중요한데, LLaMA의 구현은 이러한 균형을 효과적으로 달성했습니다. 결과적으로 이러한 최적화는 모델의 규모를 확장하면서도 실용적인 학습 시간을 유지할 수 있게 했습니다.

### LLaMA의 주요 성능 평가 결과

LLaMA 모델의 성능 평가는 [Brown과 연구진](https://arxiv.org/abs/2005.14165)이 제시한 방법론을 따라 제로샷(zero-shot)과 퓨샷(few-shot) 학습 환경에서 진행되었습니다. 총 20개의 벤치마크에서 평가가 이루어졌으며, 각각의 평가 방식은 다음과 같은 특징을 가집니다.

제로샷 평가에서는 모델에게 과제에 대한 텍스트 설명과 테스트 예제만을 제공하고, 모델은 자유 형식의 생성이나 제안된 답변의 순위 매기기를 수행합니다. 퓨샷 평가에서는 1개에서 64개 사이의 과제 예시와 테스트 예제를 함께 제공하여 모델의 성능을 측정합니다.

다중 선택 과제에서는 주어진 맥락에 대해 가장 적절한 답변을 선택하는 방식으로 평가가 진행됩니다. Gao와 연구진의 방법론을 따라, 대부분의 데이터셋에서는 답변의 문자 수로 정규화된 우도를 사용합니다. 그러나 OpenBookQA와 BoolQ와 같은 특정 데이터셋에서는 [Brown과 연구진](https://arxiv.org/abs/2005.14165)의 방법을 따라 다음과 같은 정규화된 우도를 사용합니다.

$$ P(\text{completion}|\text{context}) / P(\text{completion}|\text{"Answer:"}) $$

이러한 평가 방식을 통해 LLaMA는 GPT-3, Gopher, Chinchilla, PaLM과 같은 비공개 언어 모델들과 OPT, GPT-J, GPT-Neo와 같은 오픈소스 모델들과 비교 평가되었습니다. 특히 주목할 만한 점은 LLaMA-65B가 대부분의 벤치마크에서 Chinchilla-70B를 능가했으며, PaLM-540B와도 대등한 성능을 보여주었다는 것입니다.

![Figure 2](https://ar5iv.org/html/2302.13971/assets/x2.png)

모델의 학습 과정에서 성능 변화를 추적한 결과는 그림에서 확인할 수 있습니다. 대부분의 벤치마크에서 성능이 꾸준히 향상되는 것을 관찰할 수 있으며, 이는 모델의 학습 손실(perplexity) 감소와 상관관계를 보입니다. 다만 SIQA와 WinoGrande와 같은 일부 벤치마크에서는 예외적인 패턴이 관찰되었는데, 특히 SIQA에서는 성능의 변동성이 크게 나타나 해당 벤치마크의 신뢰성에 대한 의문을 제기하게 되었습니다.

이러한 종합적인 평가 결과는 LLaMA가 기존의 대규모 언어 모델들과 비교하여 더 적은 계산 자원으로도 경쟁력 있는 성능을 달성할 수 있다는 것을 입증합니다. 특히 LLaMA-13B 모델이 GPT-3보다 10배 작은 규모임에도 불구하고 대부분의 벤치마크에서 더 우수한 성능을 보여준 것은 주목할 만한 성과입니다.

### LLaMA의 명령어 미세조정과 성능 향상

LLaMA 모델의 명령어 미세조정(Instruction Finetuning) 연구는 기초 언어 모델의 성능을 효과적으로 향상시키는 방법을 보여줍니다. Chung과 연구진이 제시한 프로토콜을 따라 수행된 이 실험에서는, 소량의 명령어 데이터를 사용한 미세조정만으로도 MMLU(Massive Multitask Language Understanding) 벤치마크에서 상당한 성능 향상을 달성할 수 있음을 입증했습니다.

LLaMA-65B 모델은 미세조정 이전에도 기본적인 명령어를 따를 수 있는 능력을 보여주었지만, 명령어 데이터를 통한 간단한 미세조정 과정을 거친 후 MMLU 성능이 크게 향상되었습니다. 이렇게 미세조정된 모델인 LLaMA-I는 5-shot 설정에서 68.9%의 정확도를 달성했는데, 이는 OPT-IML-Max(30B)의 43.2%와 Flan-PaLM(62B)의 59.6%를 크게 상회하는 결과입니다.

특히 주목할 만한 점은 LLaMA-I가 비슷한 규모의 다른 모델들과 비교했을 때 우수한 성능을 보여준다는 것입니다. 예를 들어, Chinchilla(70B)의 67.5%나 PaLM-cont(62B)의 62.8%보다 높은 성능을 달성했습니다. 이는 명령어 미세조정이 모델의 기본 성능을 효과적으로 향상시킬 수 있다는 것을 보여줍니다.

하지만 Iyer와 연구진의 연구에서 보고된 GPT code-davinci-002의 77.4% 성능과 비교하면, LLaMA-I는 아직 개선의 여지가 있음을 알 수 있습니다. 이러한 격차는 모델 규모의 차이에서 기인할 수 있으며, 더 큰 규모의 모델이나 더 정교한 미세조정 기법을 통해 줄일 수 있을 것으로 예상됩니다.

LLaMA-I의 성능은 57개의 개별 MMLU 작업에 걸쳐 상세히 분석되었으며, 이는 부록의 Table 16에서 확인할 수 있습니다. 이러한 상세 분석은 모델의 강점과 약점을 이해하고, 향후 개선 방향을 설정하는 데 중요한 통찰을 제공합니다. 특히 명령어 미세조정이 모델의 전반적인 성능을 향상시키면서도, 특정 작업 유형에서는 더 큰 개선이 필요함을 보여줍니다.

## LLaMA의 편향성, 유해성 및 허위정보 평가

대규모 언어 모델의 발전과 함께 이러한 모델들이 가진 잠재적 위험성에 대한 평가의 중요성이 더욱 부각되고 있습니다. Sheng과 연구진의 연구에서 지적된 바와 같이, 언어 모델들은 학습 데이터에 존재하는 편향성을 재생산하고 증폭시킬 수 있으며, Gehman과 연구진이 보고한 것처럼 유해하거나 공격적인 콘텐츠를 생성할 수 있습니다.

### RealToxicityPrompts 평가

LLaMA-65B의 잠재적 위험성을 평가하기 위해 먼저 RealToxicityPrompts 벤치마크를 통한 유해성 평가가 수행되었습니다. 이 벤치마크는 약 10만 개의 프롬프트로 구성되어 있으며, 각 프롬프트에 대한 모델의 응답은 PerspectiveAPI를 통해 독성 점수를 측정받습니다. 독성 점수는 0(무해)에서 1(유해) 사이의 값을 가지며, 모델의 크기가 증가할수록 독성 점수도 증가하는 경향을 보였습니다.

특히 주목할 만한 점은 "예의 바르고, 존중하며, 편향되지 않은 방식으로 다음 문장을 완성하시오"라는 지시문을 추가했을 때의 결과입니다. LLaMA-65B 모델은 기본 프롬프트에서 0.128, 존중하는 프롬프트에서 0.141의 독성 점수를 기록했는데, 이는 모델의 크기가 커질수록 오히려 유해한 콘텐츠 생성 가능성이 높아질 수 있음을 시사합니다.

### CrowS-Pairs 편향성 평가

Nangia와 연구진이 개발한 CrowS-Pairs 데이터셋을 통해 9개 범주에서의 편향성이 평가되었습니다. 성별, 종교, 인종/피부색, 성적 지향성, 연령, 국적, 장애, 외모, 사회경제적 지위에 대한 편향성을 측정한 결과, LLaMA는 특히 종교(79.0%), 성적 지향성(81.0%), 외모(77.8%) 범주에서 높은 편향성을 보였습니다.

GPT-3, OPT-175B와의 비교에서 LLaMA는 평균적으로 약간 더 나은 성능을 보였으나(66.6% vs 67.2%, 69.5%), 종교 범주에서는 다른 모델들보다 10% 이상 높은 편향성을 나타냈습니다. 이는 CommonCrawl 데이터의 필터링 과정에도 불구하고 여전히 존재하는 편향성을 반영합니다.

### LLaMA의 탄소 발자국 분석

대규모 언어 모델의 학습 과정에서 발생하는 환경적 영향을 정량적으로 평가하는 것은 AI 연구의 지속가능성을 위해 매우 중요합니다. Wu와 연구진이 제안한 방법론을 기반으로, LLaMA 모델의 에너지 소비량과 탄소 배출량을 체계적으로 분석했습니다.

에너지 소비량 계산을 위해 다음과 같은 수식이 사용되었습니다.

$$ \text{Wh} = \text{GPU-h} \times (\text{GPU power consumption}) \times \text{PUE} $$

여기서 GPU-h는 총 GPU 사용 시간, GPU power consumption은 GPU의 전력 소비량을 나타내며, PUE(Power Usage Effectiveness)는 데이터 센터의 전력 사용 효율성을 나타내는 지표로 1.1로 설정되었습니다. 이는 데이터 센터의 냉각 시스템과 기타 인프라에 의한 추가 전력 소비를 고려한 것입니다.

탄소 배출량은 다음 수식을 통해 계산됩니다.

$$ \text{tCO}_2\text{eq} = \text{MWh} \times 0.385 $$

여기서 0.385는 미국의 평균 탄소 집약도 계수(kg CO₂eq/KWh)를 나타냅니다. 이 계수를 사용함으로써 서로 다른 데이터 센터에서 학습된 모델들의 탄소 배출량을 공정하게 비교할 수 있습니다. 예를 들어, BLOOM은 0.057 kg CO₂eq/KWh의 그리드를 사용하여 27 tCO₂eq를 배출했고, OPT는 0.231 kg CO₂eq/KWh의 그리드에서 82 tCO₂eq를 배출했습니다.

LLaMA 모델 시리즈의 개발에는 2048개의 A100-80GB GPU가 약 5개월 동안 사용되었습니다. 이러한 학습 인프라를 통해 총 2,638 MWh의 전력이 소비되었으며, 이는 미국 평균 탄소 집약도 기준으로 1,015 tCO₂eq의 탄소 배출량에 해당합니다. 비교를 위해, OPT 모델은 992개의 A100-80B GPU를 34일 동안 사용하여 학습되었습니다.

이러한 상당한 환경적 영향에도 불구하고, 연구팀은 이미 학습된 모델을 공개함으로써 향후 발생할 수 있는 추가적인 탄소 배출을 줄일 수 있을 것으로 기대합니다. 특히 일부 작은 규모의 모델들은 단일 GPU에서도 실행이 가능하므로, 개별 연구자들이 새로운 모델을 처음부터 학습시키는 대신 이미 학습된 모델을 활용할 수 있습니다. 이는 AI 연구의 환경적 지속가능성을 높이는 데 기여할 것으로 예상됩니다.

### 언어 모델의 발전과 스케일링에 관한 연구

언어 모델의 역사는 Shannon의 1948년 연구에서 시작되어, 단어, 토큰, 또는 문자 시퀀스에 대한 확률 분포를 모델링하는 것을 목표로 발전해왔습니다. 초기에는 n-gram 통계에 기반한 접근 방식이 주를 이루었는데, 이는 다음과 같은 조건부 확률로 표현됩니다.

$$ P(w_n|w_1^{n-1}) = \frac{\text{count}(w_1^n)}{\text{count}(w_1^{n-1})} $$

여기서 \\(w_1^n\\)은 n개 단어의 시퀀스를 나타냅니다. 이러한 n-gram 모델의 한계를 극복하기 위해 Katz와 Kneser와 Ney는 희소 사건(rare events)의 추정을 개선하기 위한 다양한 스무딩 기법을 제안했습니다.

신경망 기반 언어 모델의 시대는 Bengio와 연구진의 피드포워드 모델로 시작되었습니다. 이후 Mikolov와 연구진의 순환 신경망(RNN)과 Hochreiter와 Schmidhuber의 LSTM을 거쳐, [Vaswani와 연구진](https://arxiv.org/abs/1706.03762)이 제안한 트랜스포머 아키텍처에 이르렀습니다. 트랜스포머의 셀프 어텐션(self-attention) 메커니즘은 다음과 같이 정의됩니다.

$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

모델 스케일링의 관점에서, Brants와 연구진은 2조 토큰으로 학습된 언어 모델이 기계 번역 품질을 향상시킬 수 있음을 보여주었습니다. 이후 [Brown과 연구진](https://arxiv.org/abs/2005.14165)의 GPT-3(175B 매개변수)를 시작으로, Jurassic-1, Megatron-Turing NLG, Gopher, Chinchilla, PaLM 등 대규모 언어 모델들이 등장했습니다.

[Kaplan과 연구진](https://arxiv.org/abs/2001.08361)은 트랜스포머 기반 언어 모델의 스케일링 법칙을 도출했으며, 이는 [Hoffmann과 연구진](https://arxiv.org/abs/2203.15556)에 의해 더욱 정교화되었습니다. 특히 Hoffmann의 연구는 데이터셋 크기를 확장할 때 학습률 스케줄을 조정하는 것의 중요성을 강조했습니다.

이러한 발전 과정에서 주목할 만한 점은 모델 크기와 데이터셋 크기가 성능에 미치는 영향이 멱법칙(power law)을 따른다는 것입니다. Wei와 연구진의 연구는 이러한 스케일링이 대규모 언어 모델의 능력에 미치는 영향을 체계적으로 분석했습니다.

### LLaMA의 결론과 향후 전망

본 연구는 최신 기초 언어 모델들과 경쟁력 있는 성능을 보이는 공개 언어 모델 시리즈인 LLaMA를 소개했습니다. 특히 주목할 만한 성과는 LLaMA-13B가 GPT-3보다 10배 이상 작은 규모임에도 불구하고 더 우수한 성능을 보였다는 점이며, LLaMA-65B는 Chinchilla-70B 및 PaLM-540B와 대등한 수준의 성능을 달성했다는 것입니다. 이전 연구들과 달리, 본 연구는 독점적인 데이터셋에 의존하지 않고 오직 공개된 데이터만을 사용하여 최고 수준의 성능을 달성할 수 있다는 것을 입증했습니다.

연구팀은 이러한 모델들을 연구 커뮤니티에 공개함으로써 대규모 언어 모델의 발전을 가속화하고, 유해성과 편향성과 같은 알려진 문제점들을 개선하기 위한 노력을 지원할 수 있기를 기대합니다. Chung과 연구진의 연구에서 보여진 것처럼, 명령어에 대한 미세조정이 유망한 결과를 보여주었으며, 이는 향후 연구에서 더욱 심도 있게 탐구될 예정입니다.

마지막으로, 연구팀은 모델의 규모를 확장함에 따라 지속적인 성능 향상이 관찰되었기 때문에, 향후 더 큰 규모의 사전 학습 데이터셋으로 학습된 더 큰 모델을 공개할 계획을 가지고 있습니다. 이는 대규모 언어 모델의 성능 한계가 아직 도달하지 않았음을 시사하며, 향후 연구를 통해 더 높은 성능과 더 넓은 응용 가능성을 탐구할 수 있을 것으로 기대됩니다.

이러한 연구 성과는 인공지능 분야에서 개방성과 협력의 중요성을 강조하며, 대규모 언어 모델 개발의 민주화에 기여할 것으로 기대됩니다. 특히 공개 데이터만으로도 최고 수준의 성능을 달성할 수 있다는 점은, 향후 더 많은 연구자들이 언어 모델 개발에 참여할 수 있는 가능성을 열어주었다는 점에서 큰 의의를 가집니다.

- - -
### References
* [LLaMA: Open and Efficient Foundation Language Models](http://arxiv.org/pdf/2302.13971v1)
